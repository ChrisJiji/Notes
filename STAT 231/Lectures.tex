\documentclass[10pt,letter]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{setspace}
\onehalfspacing
\usepackage{fullpage}
\newtheorem*{remark}{Remark}
\theoremstyle{plain}
\newtheorem*{theorem*}{Theorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem*{lemma*}{Lemma}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem*{definition*}{Definition}
\usepackage{forest}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\section{Introduction To Statistical Sciences}
\subsection{Empirical Studies and Statistical Sciences}
statistics is important and different than probability

\subsection{Data Collection}
\paragraph{Definitions}
A \textbf{population} is a collection of units. \\ 
A \textbf{process} is a system by which units are produced. Time is implied in a process. \\ 
A \textbf{variate} is a characteristic of a unit. Types of variates: 
\begin{itemize}
    \item Continuous Variate: variates that can take any real number, e.g. height and weight
    \item Discrete Variate: variates that take a discrete set of positive values, e.g. number of deaths in a year 
    \item Categorical Variate: variates that are categories instead of numbers, e.g. hair colour, university program 
    \item Ordinal Variate: a subclass of categorical variates in which ordering is implied, e.g. agree, neutral, disagree
    \item Complex Variates: an image or open-ended response
\end{itemize}
An \textbf{attribute} of a population or process is a function of a variate which is defined for all units in the population/process. E.g.: for all people aged 18-25, an attribute might be the proportion of population who owns a smartphone, or mean annual income.\\ 

\paragraph{Approaches to Data Collection}
\begin{enumerate}
    \item Sample Survey - a study in which information is gathered by selecting a representative sample of units. 
    \item Observational Study - a study in which data are collected without any attempt to change any variates.
    \item Experimental Study - a study in which the experimenter intervenes and changes or sets the values of one or more variables for the units in the study.
\end{enumerate}

han roslings 200 countries, 200 years



\section*{Lecture 2}
\subsection{Data Summaries}
Summaries are important to conclude studies. They must be clear and informative. The basic set-up is as follows. Suppose that you perform a study on $n$ units, or a set $\{1,\ldots,n\}$. Then, for every variate you have data for, say $x,y$, denote the data on the $i^{th}$ unit by $x_i,y_i$. We refer to $n$ as the sample size and $\{x_1,\ldots,x_n\},\{y_1,\ldots,y_n\}$ or $\{(x_1,y_1),\ldots,(x_n,y_n)\}$ as data sets. There are two classes of summaries: numerical and graphical.
\paragraph{Numerical Summaries}
These are useful when the variates are either continuous or discrete. Numerical summaries generally fall into three categories: measure of location, measure of variability or dispersion, and measures of shape.
\subparagraph{Measure of Location}
\begin{itemize}
    \item The (sample) mean also called sample average: $\bar{y}=\frac{1}{n}\sum_{i=1}^ny_i$ 
    \item The (sample) median $m$ is the middle value when $n$ is odd and the sample is ordered, and the average of the two middle values when $n$ is even. 
    \item The (sample) mode is the value of $y$ which appears with the highest frequency.
\end{itemize}
Since the median is less affected by a few extreme observations, it is a more robust measure of location. The units for all of these are the same as the original variate. 
\subparagraph{Measure of Dispersion or Variability}
\begin{itemize}
    \item The (sample) variance $s^2=\frac{1}{n-1}\sum_{i=1}^n(y_i-\bar{y})^2=\frac{1}{n-1}\left[\sum_{i=1}^ny_i^2-\frac{1}{n}\left(\sum_{i=1}^ny_i\right)^2\right]=\frac{1}{n-1}\left[\sum_{i=1}^ny_i^2-n\bar{y}^2\right]$ and the (sample) standard deviation: $s=\sqrt{s^2}$
    \item The range $=y_{(n)}-y_{(1)}$ where $y_{(n)}=\text{max}(y_1,\ldots,y_n)$ and $y_{(1)}=\text{min}(y_1,\ldots,y_n)$. 
    \item The interquartile range $IQR$.
\end{itemize}
Since the interquartile range is less affected by a few extreme observations, it is a more robust measure of variability. The units are the same as the original variate.

\paragraph{Quantiles and Percentiles}
For $0<p<1$, the $p$th quantile (or 100$p$th percentile) is a value such that approximately $p$ of the $y$ values are less than $q(p)$ and approximately $1-p$ are greater than $q(p)$. \\ 
\textbf{Definition:} Let $\{y_{(1)},\ldots,y_{(n)}\}$ where $y_{(1)}\leq\cdots\leq y_{(n)}$ be the \textbf{order statistic} for the data set $\{y_1,\ldots,y_n\}$. For $0<p<1$, the $p$th (sample) quantile is a value, $q(p)$, determined as follows:
\begin{itemize}
    \item Let $m=(n+1)p$ where $n$ is the sample size.
    \item If $m$ is an integer and $1\leq m\leq n$, then $q(p)=y_{(m)}$ 
    \item If $m$ is not an integer but $1<m<n$ then determine the closest integer $j$ such that $j<m<j+1$ and then $q(p)=\frac{1}{2}[y_{(j)}+y_{(j+1)}]$
\end{itemize}
The quantiles $q(0.25),q(0.5),q(0.75)$ are often used to summarize data, and are called the lower or first quartile, the median, and the upper or third quartile respectively. The \textbf{interquartile range} (IQR) is $IQR=q(0.75)-q(0.25)$. The \textbf{five number summary} of a data set is $\{y_{(1)},q(0.25),q(0.5),q(0.75),y_{(n)}\}$. 

\section*{Lecture 3}
\subparagraph{Measure of Shape}
\begin{itemize}
    \item The (sample) skewness $g_1=\frac{\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^3}{\left[\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2\right]^\frac{3}{2}}$ 
    \item The (sample) kurtosis $g_2=\frac{\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^4}{\left[\frac{1}{n}\sum_{i=1}^n(y_i-\bar{y})^2\right]^2}$ 
\end{itemize}
Measures of shape generally indicate how the data differ from the Normal bell-shaped curve. Skewness is a measure of the lack of symmetry in data. Positive skewness results in a long right tail, and negative skewness results in a long left tail. Kurtosis measures the heaviness of the tails and the peakedness of the data relative to the Normal bell-shaped curve. The more positive the kurtosis, the more peaked in the centre the curve is. If the data looks normal, then the kurtosis is close to 3. If it is very peaked, then the kurtosis is larger than 3. If the data is uniform, the kurtosis is around $1.2$. Skewness and kurtosis have no units. 

\paragraph{Sample Correlation}
A numerical summary of bivariate data is the sample correlation, defined as $r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$. 
$$S_{xx}=\sum_{i=1}^n(x_i-\bar{x})^2=\sum_{i=1}^nx_i^2-\frac{1}{n}\left(\sum_{i=1}^nx_i\right)^2$$ 
$$S_{xy}=\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^nx_iy_i-\frac{1}{n}\left(\sum_{i=1}^nx_i\right)\left(\sum_{i=1}^ny_i\right)$$
$$S_{yy}=\sum_{i=1}^n(y_i-\bar{x})^2=\sum_{i=1}^ny_i^2-\frac{1}{n}\left(\sum_{i=1}^ny_i\right)^2$$
The sample correlation takes on values between $-1$ and $1$, and measures the linear relationship between $x$ and $y$. If $r$ is close to $-1/1$ then we say the two variates have a strong negative/positive linear relationship. If $r$ is close to 0 then there is no linear relationship. 

\paragraph{Graphical Summaries}All graphs should: 
\begin{itemize}
    \item Displayed at an appropriate size 
    \item Graphics should have clear titles which are fairly self explanatory 
    \item Axes should be labeled and units given where appropriate 
    \item The choice of scales should be made with care 
    \item Graphics should not be used without thought; there may well be better ways of displaying the information
\end{itemize}

The following are the types of graphical summaries: 
\begin{itemize}
    \item Histograms 
    \item Empirical Cumulative Distribution Function (ecdf)
    \item Box plots
    \item Run charts 
    \item Bar graphs 
    \item Scatter plots
\end{itemize}

\subparagraph{Histograms}
The idea is to create a graphical summary that we can use to compare to a pdf for a continuous random variable, or a pf for a discrete random variable. Histograms are helpful to determine what probability model could be used later. \\ 
Let the observed data be represented as $\{y_1,\ldots,y_n\}$. Partition the range of $y$ into $k$ non-overlapping intervals $I_j=[a_j,a_{j-1})$ for $j=1,\ldots,k$. Let $f_j$ be the number of values from $\{y_1,\ldots,y_n\}$ that are in $I_j$. The $f_j$ are called the \textbf{observed frequencies}. Note that $\sum_{j=1}^kf_j=n$. A \textbf{histogram} is a graph in which a rectangle is constructed above each interval, with the height being proportional to $f_j$. There are two main types of histograms: 
\begin{itemize}
    \item In a \textbf{standard histogram} the intervals are of equal width and the heights are equal to the frequencies or the relative frequencies. 
    \item In a \textbf{relative frequency histogram} the intervals may not be of equal width. The height of the rectangle is chosen so that the area of the rectangle equals $\frac{f_j}{n}$, that is $\text{height}=\frac{\frac{f_j}{n}}{(a_j-a_{j-1})}$. In this case the sum of the areas of the rectangles equals one. When comparing to a pdf, only a relative frequency histogram can be used. 
\end{itemize}




\section*{Lecture 4}
\subparagraph{Empirical CDF}
The idea is that we want to use a graphical summary of the data that we could use to compare with a cdf. This is helpful in determining what probability model could be used to model the data. The definition of the empirical cdf is $\hat{F}(y)=\frac{\text{number of values in }\{y_1,\ldots,y_n\}\text{ which are }\leq y}{n}$, $y\in\mathbb{R}$. The area above the ecdf line is the sample mean. This is because the "height" of each horizontal rectangle is $\frac{1}{n}$, and the width is $y_{(j)}$, and so the total area is summing all of the horizontal rectangles, and hence summing all of the $\frac{1}{n}\sum_{j}^ny_{(j)}$, exactly the sample mean.

\subparagraph{Box Plots}
A boxplot gives a graphical summary about the shape of the distribution. How to draw:
\begin{enumerate}
    \item Draw a box that ends at the lower and upper quartiles so the box height is the IQR.
    \item Draw a line in the box at the median.
    \item Draw two lines of "whiskers" outside the box to the minimum and maximum. If the minimum or maximum is more than $1.5*$IQR then add whiskers at $q(0.25)-1.5*$IQR and $q(0.75)+1.5*$IQR.
    \item Plot any additional points beyond $\pm1.5*$IQR individually using a special symbol like "$+$" or "$*$". These points are called "outliers". 
\end{enumerate}

\subparagraph{Run Chart}
A graphical summary of data which are varying over time. 

\subparagraph{Scatterplots}
Used for when the datasets $\{(x_1,y_1),\ldots,(x_n,y_n)\}$, where $x_i$ and $y_i$ are real numbers. You simply plot the points at the coordinates $(x_i,y_i)$. 

\section*{Lecture 5}
First lecture of Dr. Banajee, he just did another introduction to the course talking about difference between STAT230/231, and some examples of interesting statistics (st.petersburg paradox, disappearance of the 400 hitter, correlation vs causation).

\section*{Lecture 6}
\paragraph{What is a Statistical Model}
A \textbf{statistical model} is a specification of the distribution from which your data is drawn, where the attribute of interest is typically a parameter of the distribution. Gave an example of whether or not Canadians are better at Jeopardy than Americans. The data points are the number of shows each Canadian is featured in, and it is easy to see that each data point is the number of trials before the first failure, and so therefore follows a geometric distribution. \\ 
Data has two personalities. $y_1,\ldots,y_n$ are numbers, but ALSO outcomes of some random experiment. Identifying that random experiment is setting up a statistical model. For this course, $y_1$ is actual data (numbers), and $Y_i$ are random variables. $\theta,\pi,\mu$ will all be population parameters (unknown constants). 

\paragraph{Types of Statistical Inference}
\begin{itemize}
\item Estimation Problems: Trying to make an educated guess of the value of some population attribute based on the data.
\item Hypothesis Testing: Based on data, is the hypothesis "reasonable"? 
\item Prediction: How to forecast future observations from data sets? (stat 443)
\end{itemize}


\section*{Lecture 7}
\paragraph{On the tutorial}
\begin{itemize}
    \item R commands that were on the assignment
    \item STAT 230 
    \begin{itemize}
        \item Normal Distribution
        \begin{itemize}
            \item if $Y\sim N(\mu,\sigma^2)$, then $\frac{Y-\mu}{\sigma}=Z\sim N(0,1)$
            \item If $Y_1,\ldots,Y_n\sim(\mu,\sigma^2)$ are independent, then $\bar{Y}\sim N(\mu,\frac{\sigma^2}{n})$
            \item Empirical fact: If the data is normal, then 68\% of the observations lie within $\mu\pm\sigma$. 95\% of the observations lie within $\mu\pm2\sigma$, and 99\% of the observations lie within $\mu\pm3\sigma$. 
        \end{itemize}
    \end{itemize}
    \item Definitions and concepts 
    \begin{itemize}
        \item Linear transformation problems (location stays the same, variance and standard deviation we know from stat230) 
        \item calculate new mean and variance if you remove a data point (mean is easy, for variance use $\frac{1}{n-1}\left[\sum_{i=1}^ny_i^2-n\bar{y}^2\right]$)
    \end{itemize}
    \item Graphical and Numerical data summaries 
    Types of Data:
    \begin{itemize}
        \item Discrete 
        \item Continuous 
        \item Categorical 
        \item Ordinal
    \end{itemize}
    Variates of Interest: 
    \begin{itemize}
        \item Response Variate - variates that are usually the focus of the study
        \item Explanatory Variate - variates used to explain the response 
    \end{itemize}   
    Numerical Measures: 
    \begin{itemize}
        \item Location: sample mean, sample median, sample mode 
        \item Variability: range, IQR, sample variance, sample standard deviation 
        \item Shape: sample skewness, sample kurtosis 
    \end{itemize}
\end{itemize}

\paragraph{The Theory of Estimation}
maximum likelyhood estimation (covered examples this time, the formal definition will be next class)

\section*{Lecture 8}
\paragraph{The Theory of Estimation}
We have an unknown population parameter $\theta$ that we are interested in. We have data: $\{y_1,\ldots,y_n\}$, and $Y_i\sim f(y_i;\theta)$, with $i=1,\ldots,n$. Question: Based on your data, what is the most likely value of $\theta$? 
\paragraph{Definition}
Let $y=(y_1,\ldots,y_n)$ be a vector of the data set. The likelihood function $L(y;\theta)=P(Y_1=y_1,\ldots,Y_n=y_n)$. What is the probability of observing our sample, as a function of $\theta$? Example: Suppose a coin is tossed 200 times and $Y=\text{number of heads}$. The experiment leads to 110 heads. Assume $\theta=P(\text{head})$, then we have $L(y,\theta)={200\choose 110}\theta^{110}(1-\theta)^{90}$. We choose the value of $\theta$ that maximizes the probability of observing what we observed, or the maximum likelihood estimate, denoted $\hat{\theta}$. Instead of taking the derivative of the function to maximize (cause its hard to), we take the log-likelihood function $l(\theta)=\log L(\theta)$, note that $\log$ is always base $e$. Then $l(\theta)=\ln{200\choose 110}+110\ln\theta+90\ln(1-\theta)$, then $\frac{dl}{d\theta}=0\Rightarrow \frac{110}{\theta}-\frac{90}{1-\theta}=0$, and solving we get $\theta=\frac{110}{200}=0.55$. 
\paragraph{Example 2}
Canadian Jeopardy example. Let $\theta=P(\text{Canadian wins})$. Then we have $y=(2,3,1,1,1,2)$, and $L(y,\theta)=P(Y_1=y_1,\ldots,Y_6=y_6)$. Then $L(y,\theta)=\theta(1-\theta)\theta^2(1-\theta)(1-\theta)(1-\theta)(1-\theta)\theta(1-\theta)=\theta^4(1-\theta)^6$. Then $\frac{dl}{d\theta}=0\Rightarrow \frac{4}{\theta}-\frac{6}{1-\theta}=0\Rightarrow \theta=\frac{4}{10}$ 
\paragraph{Example 3}
The number of texts you receive in an hour is assumed to have a Poisson distribution. Data points: $(2,3,1,0,0,0,1,2,1,0)$. $\mu=\text{average amount of texts per hour}$. Based on your sample, what is $\hat{\mu}$? (MLE for $\mu$). $L(\mu;y)=\frac{e^{-\mu}\mu^2}{2!}\cdots\frac{e^{-u}\mu^0}{0!}=L(\mu)=\frac{e^{-10\mu}\mu^{10}}{2!3!1!\ldots0!}$. Then setting the horrible denominator to be $k$, we get $l(\mu)=-10\mu+10\ln\mu-\ln k$, then $\frac{dl}{d\mu}=-10+\frac{10}{\mu}=0\Rightarrow \hat{\mu}=1$. 
\paragraph{General Poisson}
Let $\{y_1,\ldots,y_n\}$ be our data set, and $Y_i\sim \text{Poisson}(\mu)$. Then the MLE for $\mu$ can be found using $L(\mu,y)=\frac{e^{-\mu}\mu^{y_1}}{y_1!}\cdots\frac{e^{-\mu}\mu^{y_n}}{y_n!}=\frac{e^{-n\mu}\mu^{\sum y_i}}{y_1!\ldots y_n!}$. Then $l(\mu)=-n\mu+\sum y_i\ln\mu-\ln k$, and then solving for $\frac{dl}{d\mu}=0$ we get $\hat{\mu}=\bar{y}$. 

\section*{Lecture 9}
\paragraph{Set up}
Say we have some (population) parameter of interest, $\theta(\mu,\pi,\sigma^2,\ldots)$. We collect a sample from the population, $\{y_1,\ldots,y_n\}$, then $Y_i\sim f(y_i;\theta)$ is our model. Based on the model and the data, what is the "most likely" value of $\theta$? The MLE of $\theta=\hat{\theta}(y_1,\ldots,y_n)$\\ 
\paragraph{Definition}
$L(\theta;y_1,\ldots,y_n)=P(Y_1=y_1,\ldots,Y_n=y_n)$ If the data is drawn independently, and the distribution is the same, then $L(\theta;y_1,\ldots,y_n)=\Pi_{i=1}^nf(y_i;\theta)$. $\hat{\theta}$ is called the MLE if $\hat{\theta}$ maximizes $L(\theta;y_1,\ldots,y_n)$. $l(\theta;y_1,\ldots,y_n)=\log_lL(\theta;y_1,\ldots,y_n)$. $\hat{\theta}\text{ maximizes }L\Leftrightarrow \hat{\theta}\text{ maximizes }l$. 
\paragraph{Example}
A sample of 1000 voters are taken. 293 of them say they approve of what the government is doing. $\theta=$approval rating. Find $\hat{\theta}$ \\ 
Solution: Note that $Y\sim\text{Bin}(1000,\theta)$, so $L(\theta;y)={1000\choose293}\theta^{293}(1-\theta)^{707}$. Taking logs, we get $l(\theta)=\ln(k)+293\ln\theta+707\ln(1-\theta)$, with $k={1000\choose293}$. Since we want to maximize $\theta$, we find the $0$ of the derivative, so $\frac{dl}{d\theta}=0\Rightarrow\frac{293}{\theta}-\frac{707}{1-\theta}=0$, or $\hat\theta=0.293$. Note that this is equal to $\frac{y}{n}$, which is true in general for binomial distributions. 

\section*{Lecture 10}
\paragraph{MLE for Normal Distribution}
If $Y_i\sim N(\mu,\sigma^2)$, if $\mu,\sigma^2$ were unknown, what are $\hat\mu,\hat\sigma^2$? \\ 
Multiplying out the definition of $L(\theta)$, taking the log-likelihood function, and taking partial derivatives, we get $\hat\mu=\bar{y}$, and $\hat\sigma^2=\frac{1}{n}\sum(y_i-\bar{y})^2$
\paragraph{Invariance Property of the MLE}
If $\hat\theta$ is the MLE for $\theta$, then $g(\hat\theta)$ is the MLE for $g(\theta)$. 
\paragraph{Using the Invariance Property}
Let $Y_i\sim N(\mu,\sigma^2)$ for $i=1,\ldots,n$. Find the MLE for the $95^{th}$ percentile of $Y$. \\ 
solution: first, what is the $95^{th}$ percentile of $Y$? Let $A=95^{th}$ percentile. Then $P(Y\leq A)=0.95\Rightarrow P\left(\frac{Y-\mu}{\sigma}\leq\frac{A-\mu}{\sigma}\right)=0.95$. Let $Z=\frac{Y-\mu}{\sigma},B=\frac{A-\mu}{\sigma}$. We can find $B$ from the $Z-$table, $B=1.65$, and then $A=\mu+1.65\sigma$. By the invariance property, the MLE for $A$ is $\hat\mu+1.65\hat\sigma$, with $\hat\mu=\bar{y},\hat\sigma=\sqrt{\frac{1}{n}\sum(y_i-\bar{y})^2}$




\section*{Lecture 11}
\paragraph{Recap}
\subparagraph{Model} $Y_i\sim f(y_i;\theta)$, with $i=1,\ldots,n$. \begin{itemize}
    \item $n$ is the sample size 
    \item $\theta$ is the population parameter 
    \item $Y_i$'s are independent and identically distributed
    \item $\{y_1,\ldots,y_n\}$ is our sample 
\end{itemize}
Objective: Find $\hat{\theta}$ where $L(\theta)=\pi_{i=1}^nf(y_i;\theta)$, which is the product of the probability (or density) function, evaluated at each sample point. $\hat{\theta}$ maximizes $L(\theta)$. \\ 
Binomial: When $Y\sim\text{Bin}(n,\theta)$, we have $\hat{\theta}=\frac{y}{n}$\\ 
Poisson: When $Y_1,\ldots,Y_n\sim\text{Poi}(\mu)$, we have $\hat{\mu}=\overline{y}$ \\ 
Uniform: When $Y_1,\ldots,Y_n\sim U[0,\theta]$, we have $\hat{\theta}=\text{max}\{y_1,\ldots,y_n\}$ \\ 
Exponential: When $Y_1,\ldots,Y_n\sim\text{Exp}(\theta)$, we have $\hat{\theta}=\overline{y}$ \\ 
Normal: When $Y_1,\ldots,Y_n\sim N(\mu,\sigma^2)$, we have $\hat{\mu}=\overline{y}$, and $\sigma^2=\frac{1}{n}\sum(y_i-\overline{y})^2$
\subparagraph{Invariance Property}
$\hat{\theta}$ is the MLE for $\theta\Rightarrow g(\hat{\theta})$ is the MLE for $g(\theta)$.  

\paragraph{Example of Invariance Property}
We have $Y_1,\ldots,Y_n\sim\text{Exp}(\theta)$ are independent. Find the MLE for the median of $Y$. Our data set would be $\{y_1,\ldots,y_n\}$. Step 1: find the median of the exponential, and hopefully it will be a function of theta. Then just use the invariance property. Remember for an exponential distribution, we have $f(y)=\frac{1}{\theta}e^{-\frac{y}{\theta}}, F(y)=1-e^{-\frac{y}{\theta}}$. Suppose the median is $m$. We know $F(m)=\frac{1}{2}\Rightarrow e^{-\frac{m}{\theta}}=\frac{1}{2}\Rightarrow m=-\theta\ln\left(\frac{1}{2}\right)$. Then $\hat{m}$ (MLE for median) would be $-\hat{\theta}\ln\frac{1}{2}=-\overline{y}\ln\frac{1}{2}$ by the invariance property. 

\paragraph{Relative Likelihood Function}
The \textbf{relative likelihood function} $R(\theta)$ is defined as: $R(\theta)=\frac{L(\theta)}{L(\hat{\theta})}$, where $\hat{\theta}$ is the MLE. The log relative likelihood function is given by $r(\theta)=\ln(R(\theta))=l(\theta)-l(\hat{\theta})$, where $l$ is the log-likelihood function. $R(\theta)$ looks like  a normal curve, maximum is $1$ at $R(\hat{\theta})$. 

\paragraph{Likelihood for Multinomial}
$(\theta_1,\ldots,\theta_m)$, with $\theta_i=P(\text{success for the }i\text{'th player})$. $n$ trials, and $y_i=$number of wins by player $i$. It is easy to see that $\hat{\theta_i}=\frac{y_i}{n}$ 

\paragraph{Model Selection}
\begin{itemize}
    \item Compare the histogram to the probability function 
    \item Compare the ECDF to the theoretical CDF 
\end{itemize}

\paragraph{The Q-Q plot}
Let our data set be $\{y_1,\ldots,y_n\}$. The normal $Q-Q$ plot is the plot of $(z_{(\alpha)},y_{(\alpha)})$, where $z_{(\alpha)}$ is the $\alpha^{th}$ quantile of $Z\sim N(0,1)$, and $y_{(\alpha)}$ is the $\alpha^{th}$ quantile of your data set. If you draw a $Q-Q$ plot and it's a straight line, normality is a good assumption. If $Y\sim N(\mu,\sigma^2)$, then the $Q-Q$ plot will be a straight line. If it looks like an "s", it will probably be uniform, and a "u" shape will correspond to an exponential. 

\section*{Lecture 12}
\paragraph{Model Selection}
If we have $Y_i$ is some random variable, and we get $\{y_1,\ldots,y_n\}$ as our data points, we suspect that these data set comes from some distribution. To select a model, just compare the given data set with the expected data set from the model 

\paragraph{The PPDAC}
\begin{itemize}
    \item P: Problem: A clear statement of the study's objectives
    \item P: Plan: the procedures used to carry out the study including how we will collect the data
    \item D: Data: the physical collection of the data, as described in the Plan
    \item A: Analysis: the analysis of the data collected in light of the Problem and the Plan 
    \item C: Conclusion: The conclusions that are drawn about the Problem and their limitations
\end{itemize}

\subparagraph{Problem}
The elements of the Problem addresses questions starting with "what": 
\begin{itemize}
    \item What conclusions are we trying to draw? 
    \item What group of things or people do we want the conclusions to apply? 
    \item What variates can we define? 
    \item What are the questions we are trying to answer?
\end{itemize}

Types of Problems:
\begin{itemize}
    \item Descriptive: The problem is to determine a particular attribute of a population. e.g., national unemployment rate
    \item Hypothesis Testing (causative): Determine the existence or non-existence of a causal relatioship between two variates. e.g., does taking a low dose of aspirin reduce heart disease?
    \item Predictive: Predict the response of a variate for a given unit. Mostly in finance or economics. 
\end{itemize}
To help define problems we first define a couple terms: 
\begin{itemize}
    \item \textbf{Target population} or \textbf{process} is the collection of units to which the experimenters conducting the empirical study wish the conclusions to apply 
    \item \textbf{Variate} is a characteristic associated with each unit 
    \item \textbf{Attribute} is a function of the variates over a population
\end{itemize}


\subparagraph{Plan}
How do we collect our sample? \textbf{Study population} is the population from which your sample is drawn, ie. a random sample of 100 people between 18-35 are selected from the KW area. 18-35 year olds in KW is a study population. Study population does not need to be a subset of the target population. The \textbf{sampling protocol} is the procedure used to select a sample of units from the study population.\\ 
Types of errors: 
\begin{itemize}
    \item \textbf{Study error}: the attributes in the study population differ from the attributes in the target population
    \item \textbf{Sample error}: the attributes in the sample differ from the attributes in the study population 
    \item \textbf{Measurement error}: the measured value and the true value of a variate are not identical
\end{itemize}


\subparagraph{Data}
Recall the types of data:
\begin{itemize}
    \item Discrete
    \item Continuous
    \item Categorical
    \item Ordinal
\end{itemize}
The data step is to just collect data according to the Plan. Any deviations from the Plan should be noted. \textbf{Response bias} is when incorrect data is collected due to people being dumb.  

\subparagraph{Analysis}
Analyze the data using previously learned ways (graphs and fitting to models). 
\subparagraph{Conclusion}
Answer the questions posed in the Problem. Should attempt to discuss potential errors as described in the Plan and any limitations to the conclusions.

\section*{Lecture 13}
oopsie

\section*{Lecture 14}
\paragraph{Set-up for Sampling Distributions}
$\theta:$ unknown population parameter. $Y_i\sim f(y_i;\theta)$, with $i=1,\ldots,n$ and $f$ is our probability function. We have $\{y_1,\ldots,y_n\}$ as our data points. \\ 
Question: Given our data set, what are the "reasonable" values of $\theta$? We construct an interval: $[l,u]$, with $l=l(y_1,\ldots,y_n)$, and $u=u(y_1,\ldots,y_n)$ such that the interval will contain $\theta$ with a "high degree of confidence". 
\paragraph{Likelihood Interval}
Take any $p\in(0,1)$. The 100p\% likelihood interval is $\{\theta:R(\theta)\geq p\}$. \\ 
\textbf{Example:} Suppose $p=0.1$. 10\% likelihood is $\{\theta:R(\theta)\geq0.1\}$. Remember $R(\theta)=\frac{L(\theta)}{L(\hat{\theta})}$, so the 10\% likelihood is all $\theta$ such that $\frac{L(\theta)}{L(\hat{\theta})}\geq 0.1$ 
\subparagraph{Convention} 
\begin{itemize}
    \item If $R(\theta)\geq0.5$, then $\theta$ is \textbf{extremely plausible} 
    \item If $0.1\leq R(\theta)<0.5$, then $\theta$ is \textbf{plausible}
    \item If $0.01\leq R(\theta)<0.1$, then $\theta$ is \textbf{implausible}
    \item If $R(\theta)<0.01$, then $\theta$ is \textbf{extremely implausible}
\end{itemize}
\paragraph{Example}
Suppose $Y\sim\text{Bin}(n,\theta)$. Say $n=200,y=80$. Is $\theta=0.5$ plausible? \\ 
Solution: $L(\theta)={200\choose y}\theta^y(1-\theta)^{200-y}$. By inspection we can see $\hat{\theta}=\frac{80}{200}=0.4$. Then $R(\theta)=\frac{L(\theta)}{L(\hat\theta)}=\frac{{200\choose y}\theta^y(1-\theta)^{200-y}}{{200\choose y}\hat\theta^y(1-\hat\theta)^{200-y}}=\frac{\theta^y(1-\theta)^{200-y}}{(0.4)^y(0.6)^{200-y}}$. So is $\theta=0.5$ plausible? Just plug in $\theta=0.5,y=80$ into the above expression, and refer to the list above. Plugging in the values we get $R(\theta)=0.0178$, so $\theta=0.5$ is not plausible. 

\paragraph{somethingsomething Confidence Interval}
Insight: Our sample mean, variances, etc. are also outcomes of some random experiment. If we have $Y_1,\ldots,Y_n\sim N(\mu,\sigma^2)$ independent, then $\bar{Y}\sim N(\mu,\frac{\sigma^2}{n})$ is called our sampling distribution of the sample mean.  
\paragraph{Our different Thetas}
\begin{itemize}
    \item $\theta$ is the population mean, or $\mu$ 
    \item $\hat{\theta}$ is the best guess based on sample on what $\theta$ is, or $\bar{y}$ 
    \item $\tilde{\theta}$ is the distribution from which $\hat{\theta}$ was drawn, or $\bar{Y}$ \\
\end{itemize}
\paragraph{Example}
$Y_1,\ldots,Y_n\sim N(\mu,49)$. A sample of 25 observations are collected. Find the 95\% confidence interval for $\mu$. \\ 
Solution: We want to find $L,U$ such that $P(L\leq\mu\leq U)=0.95$. We know from STAT 230 that $\bar{Y}\sim N(\mu,\frac{49}{25})$, so $\frac{\bar{Y}-\mu}{\frac{7}{5}}=Z\sim N(0,1)$. Looking it up, we find that we get $P(-1.96\leq Z\leq 1.96)=0.95\Rightarrow P(-1.96\leq \frac{\bar{Y}-\mu}{\frac{7}{5}}\leq1.96)=0.95$. 

\section*{Lecture 15}
\paragraph{Model for Confidence Interval}
$Y_i\sim f(y_i;\theta)$, where $\theta$ is our parameter of interest, $n$ is our sample size. We have $\{y_1,\ldots,y_n\}$ as our data. \\ 
\textbf{Objective}: To construct an interval $[l,u]$, $l,u$ are computed using the data set, such that we can be "reasonably confident" that $\theta$ lies in the above interval. \\ 
\textbf{Two approaches}: 
\begin{itemize}
    \item Through $R(\theta)$
    \item Sampling distributions
\end{itemize}

\paragraph{Likelihood Interval}
Recall from last class: The 100\%p likelihood interval: $\{\theta:R(\theta)\geq p\}$. 

\paragraph{Sampling distributions}
Problem: We are given a pre specifide probability: 90\%, 95\%, 99\%\\ 
Objective: Estimate (\textbf{confidence interval}) the random interval (\textbf{coverage interval}) that contains $\theta$  with that specified probability.\\ 
Find the sampling distribution of our estimators. 

\paragraph{Normal Example}
Reminder: \begin{itemize}
    \item $\theta$: unknown constant : $(\mu,\sigma^2)$
    \item $\hat{\theta}$: number from our sample, estimate : $(\bar{y},\hat\sigma^2)$
    \item $\tilde{\theta}$: random variable from which $\hat{\theta}$ is an outcome: $(\overline{Y},S^2)$
\end{itemize}
The Stat 231 scores are normally distributed with mean $\mu$ and variance $100$. A sample of 25 students are collected wuth $\bar{y}=75$. Find a 99\% confidence interval for $\mu$. \\ 
Solution: $Y_i\sim N(\mu,100)$. MLE: $\bar{y}=75$. Note this is our estimate. Our estimator is $\overline{Y}\sim N\left(\mu,\frac{100}{25}\right)$. From stat230, $\frac{\overline{Y}-\mu}{\sqrt{\frac{100}{25}}}=Z=N(0,1)$. Then we can go to the $Z$-table and find the 99\% interval for Z. This should be $2.58$. Then $P(-2.58\leq Z\leq2.58)=0.99\Rightarrow P(-2.58\leq\frac{\overline{Y}-\mu}{2}\leq2.58)=0.99\Rightarrow P(\overline{Y}-2.58\times2\leq\mu\leq\overline{Y}+2.58\times2)=0.99$ This is our coverage interval. Our confidence interval is $(\bar{y}-2.58\times2,\bar{y}+2.58\times2)$. So our confidence interval is $(75-2.58\times2,75+2.58\times2)$, or our 99\% confidence interval is $75\pm2.58\times2$. In general, it will be $\bar{y}\pm Z^*\frac{\sigma}{\sqrt{n}}$, where $Z^*$ is some value from the $Z$ table. The $\frac{\sigma}{\sqrt{n}}$ is called the \textbf{margin of error}.

\paragraph{Interpretation}
The confidence interval is the \underline{best estimate} of the random interval that contains $\theta$ with the pre-specified high probability. ALTERNATIVELY: If we all did the same experiment and constructed our own confidence intervals, approximately 99\% of those intervals will contain $\mu$ 

\paragraph{Binomial Example}
Suppose $Y\sim\text{Bin}(n,\theta)$. A sample of 1500 people are taken, and 800 of them voted for Trump. find the 95\% confidence interval for $\theta$. \\ 
Solution: $\hat{\theta}=\frac{y}{n}=\frac{800}{1500}=\frac{8}{15}=0.5\overline{3}$. By the CLT to Binomial, $\tilde{\theta}\approx N\left(\theta,\frac{\tilde\theta(1-\tilde\theta)}{n}\right)$, and then $\frac{\tilde{\theta}-\theta}{\sqrt{\frac{\tilde{\theta}(1-\tilde{\theta})}{n}}}=Z\sim N(0,1)$. Finding the 95\% CI for $\theta$, from the $Z-$table, we get $P(-1.96\leq Z\leq1.96)=0.95\Rightarrow P\left(-1.96\leq\frac{\tilde{\theta}-\theta}{\sqrt{\frac{\tilde{\theta}(1-\tilde{\theta})}{n}}}\leq1.96\right)=0.95\Rightarrow P\left(\tilde{\theta}-1.96\sqrt{\frac{\tilde{\theta}(1-\tilde{\theta})}{n}}\leq\theta\leq\tilde{\theta}+1.96\sqrt{\frac{\tilde{\theta}(1-\tilde{\theta})}{n}}\right)=0.95$. This is our coverage interval. Then our confidence interval is $\theta\pm1.96\sqrt{\frac{\tilde{\theta}(1-\tilde{\theta})}{n}}$, or $0.33\pm1.96\sqrt{\frac{0.533\times0.467}{1500}}$. The $1.96\sqrt{\frac{0.533\times0.467}{1500}}$ is our margin of error.





\end{document}