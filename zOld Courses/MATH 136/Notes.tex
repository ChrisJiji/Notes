\documentclass[10pt,letter]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{setspace}
\onehalfspacing
\usepackage{fullpage}
\begin{document}

\paragraph{Vectors} Instead of viewing the elements of $\mathbb{R}^2$ and $\mathbb{R}^3$ as points, we view them as objects called vectors. In $\mathbb{R}^2$, we interpret $\begin{bmatrix}x_1 \\ x_2\end{bmatrix}$ as $(x_1,x_2)$.
\paragraph{Definitions}: For any positive integer $n$ we define $n$-dimensional Euclidean space $\mathbb{R}^n$ by $\mathbb{R}^n = \left\{\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix} | x_1,\ldots, x_n\in\mathbb{R}\right\}$. Note that for two vectors, $\vec{x}$ and $\vec{y}$, they are only equal iff $x_i=y_i$ for all $i$. We define \textbf{addition} by adding their corresponding components. We define \textbf{scalar multiplication} by multiplying each component of the vector by the scalar. We define \textbf{subtraction} as $\vec{x}-\vec{y}=\vec{x}+(-1)\vec{y}$. 
\paragraph{Linear Combination} Let $\vec{v_1},\ldots,\vec{v_k}$ be vectors in $\mathbb{R}^n$. We call a sum of scalar multiples of these vectors a \textbf{linear combination}. 

\paragraph{Properties of addition and scalar multiplication}. Some properties: If $\vec{x},\vec{y},\vec{w}\in\mathbb{R}^n$, and $c,d\in\mathbb{R}$, then \begin{enumerate}
    \item $\vec{x}+\vec{y}\in\mathbb{R}^n$ 
    \item $(\vec{x}+\vec{y})+\vec{w}=\vec{x}+(\vec{y}+\vec{w})$ 
    \item $\vec{x}+\vec{y}=\vec{y}+\vec{x}$ 
    \item There exists a vector $\vec{0}\in\mathbb{R}^n$ called the \textbf{zero vector}, such that $\vec{x}+\vec{0}=\vec{x}$ for all $\vec{x}\in\mathbb{R}$. 
    \item There exists a vector $(-\vec{x})\in\mathbb{R}^n$ such that $\vec{x}+(-\vec{x})=\vec{0}$ 
    \item $c\vec{x}\in\mathbb{R}^n$ 
    \item $c(d\vec{x})=(cd)\vec{x}$ 
    \item $(c+d)\vec{x}=c\vec{x}+d\vec{x}$ 
    \item $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$
    \item $1\vec{x}=\vec{x}$
\end{enumerate} 

\paragraph{Spanning} Let $B=\{\vec{v_1},\ldots,\vec{v_k}\}$ be a set of vectors in $\mathbb{R}^n$. We define the \textbf{span} $S$ of set $B$ by $S=\text{Span }B=\text{Span}\{\vec{v_1},\ldots,\vec{v_k}\}=\{t_1\vec{v_1}+\ldots+t_k\vec{v_k} | t_1,\ldots,t_k\in\mathbb{R}\}$. We also say that $S$ is \textbf{spanned} by $B$ and that $B$ is a \textbf{spanning set} for $S$. Note: If $\{\vec{v_1},\ldots,\vec{v_k}\}\in\mathbb{R}^n$, then $\text{Span}\{\vec{v_1},\ldots,\vec{v_k}\}\subseteq\mathbb{R}^n$. \\ 
Example: Does $S=\text{Span}\left\{\begin{bmatrix}1\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\end{bmatrix}\right\}$ contain the vector $\vec{x}=\begin{bmatrix}1\\2\\1\end{bmatrix}$? \\ We need: $t_1,t_2,t_3\in\mathbb{R}$ such that $t_1\begin{bmatrix}1\\0\\0\end{bmatrix}+t_2\begin{bmatrix}0\\1\\0\end{bmatrix}+t_3\begin{bmatrix}0\\0\\1\end{bmatrix}= \begin{bmatrix}1\\2\\1\end{bmatrix}$. By inspection, $t_1 = 1,t_2=2, t_3 =1$. Hence, $\vec{x}\in S$. 

\paragraph{Vector Equation} If $S=\text{Span}\{\vec{v_1},\ldots,\vec{v_k}\}$, then a \textbf{vector equation} for $S$ is $\vec{x}=t_1\vec{v_1}+\ldots+t_k\vec{v_k}$, where $t_1,\ldots,t_k\in\mathbb{R}$. Note: When one vector is a linear combination of another, you can find a \textbf{simplified vector equation} of a set. \\ 
Example: A vector equation for $S=\text{Span}\left\{\begin{bmatrix}1\\2\\1\end{bmatrix}, \begin{bmatrix}1\\0\\1\end{bmatrix}\right\}$ is $\vec{x}=t_1\begin{bmatrix}1\\2\\1\end{bmatrix} + t_2\begin{bmatrix}1\\0\\1\end{bmatrix}$, $t_1,t_2\in\mathbb{R}$

\paragraph{Simplifying a Spanning Set} If $\vec{v_k}$ can be written as a linear combination of $\vec{v_1},\ldots,\vec{v_{k-1}}$, then $\text{Span}\{\vec{v_1},\ldots,\vec{v_k}\} = \text{Span}\{\vec{v_1},\ldots,\vec{v_{k-1}}\}$. \\ 
Proof: For every $\vec{x}\in\text{Span}\{\vec{v_1},\ldots,\vec{v_k}\}$, then $\vec{x}$ is also $\in\text{Span}\{\vec{v_1},\ldots,\vec{v_{k-1}}\}$. \\ 
For the forwards direction: write out $\vec{x}$, and replace the final $\vec{v_k}$ with the linear combination of $\vec{v_1},\ldots,\vec{v_{k-1}}$ (the original assumption), then algebra it out.\\ 
The other direction: Set $t_k=0$. \\ 
Example: Write a simplified vector equation for the set $S$ spanned by $\left\{\begin{bmatrix}1\\2\\1\end{bmatrix},\begin{bmatrix}2\\4\\2\end{bmatrix},\begin{bmatrix}0\\0\\0\end{bmatrix}\right\}$ \\ 
Observe that $\begin{bmatrix}2\\4\\2\end{bmatrix} = 2\begin{bmatrix}1\\2\\1\end{bmatrix} + 0 \begin{bmatrix}0\\0\\0\end{bmatrix}$. Thus, $\begin{bmatrix}2\\4\\2\end{bmatrix}$ is a linear combination of the other vectors. So by the above theorem, we can remove it to get $S =\text{Span}\left\{ \begin{bmatrix}1\\2\\1\end{bmatrix}, \begin{bmatrix}0\\0\\0\end{bmatrix}\right\}$. Next, notice that $\begin{bmatrix}0\\0\\0\end{bmatrix} = 0\begin{bmatrix}1\\2\\1\end{bmatrix}$. Hence, it is a linear combination of it. Then we can apply the same theorem again, so $S = \text{Span}\left\{\begin{bmatrix}1\\2\\1\end{bmatrix}\right\}$, so the simplified vector equation for $S$ is $\vec{x}=t\begin{bmatrix}1\\2\\1\end{bmatrix}, t\in\mathbb{R}$.

\paragraph{Linear Independence} If one of the vectors, say $\vec{v_i}$ of a set can be written as a linear combination of the other vectors, then there exists coeffiecients $c_1,\ldots,c_k\in\mathbb{R}$ such that $c_1\vec{v_1}+\ldots+c_{i-1}\vec{v_{i-1}}+c_{i+1}\vec{v_{i+1}}+\ldots+c_k\vec{v_k}=c_i\vec{v}_i$. Basically: If one of the vectors in the set $\{\vec{v}_1,\ldots,\vec{v}_k$ is a linear combination of the others, then the equation $c_1\vec{v}_1+\ldots+c_k\vec{v}_k=\vec{0}$ has a solution where at least one of the coefficients, namely $c_i$, is non-zero. Conversely, If at least one coefficient in the above equation is non-zero, say $c_j$, then we can solve the equation for $\vec{v}_j$ to get $\vec{v}_j=-\frac{c_1}{c_j}\vec{v}_1-\ldots-\frac{c_{j-1}}{c_j}\vec{v}_{j-1}-\frac{c_{j+1}}{c_j}\vec{v}_{j+1}-\ldots-\frac{c_k}{c_j}\vec{v}_k$. Hence $\vec{v}_j\in\text{Span}\{\vec{v}_1,\ldots,\vec{v}_{j-1},\vec{v}_{j+1},\ldots, \vec{v}_k$. \\ 
DEFINITION: A set of vectors $\{\vec{v}_1,\ldots,\vec{v}_k\}$ in $\mathbb{R}^n$ is said to be \textbf{linearly dependent} if there exist coefficients $c_1,\ldots,c_k$ not all zero such that $\vec{0}=c_1\vec{v}_1+\ldots+c_k\vec{v}_k$. \\ 
A set of vectors $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is said to be \textbf{linearly independent} if the only solution to $\vec{0}=c_1\vec{v}_1+\ldots+c_k\vec{v}_k$ is $c_1=c_2=\ldots=c_k=0$, called the \textbf{trivial solution}. Notice that $\vec{0}=c_1\vec{v}_1+\ldots+c_k\vec{v}_k$ always has the trivial solution. To prove a set is linearly independent we must prove that the \text{only} solution is the trivial solution. To prove a set is linearly dependent we must show there is a solution to the equation where one of the coefficients is non-zero. \\ 
\textbf{Theorem 1.1.3} A set of vectors is linearly dependent if and only if one of the vectors can be written as a linear combination of the others. A corrollary of this and the previous theorem is that a spanning set can be simplified if and only if the set is linearly dependent. \\ 
Examples: Determine if the set $\left\{\begin{bmatrix}2\\1\\1\end{bmatrix},\begin{bmatrix}1\\2\\1\end{bmatrix},\begin{bmatrix}1\\-4\\-1\end{bmatrix}\right\}$ is linearly independent or linearly dependent. If the set is linearly dependent, find a vector which is in the span of the others. By definition, we need to determine if the equation $$\begin{bmatrix}0\\0\\0\end{bmatrix} = c_1\begin{bmatrix}2\\1\\1\end{bmatrix}+c_2\begin{bmatrix}1\\2\\1\end{bmatrix}+c_3\begin{bmatrix}1\\-4\\-1\end{bmatrix}$$ has no solution other than the trivial solution. Simplyfying gives $$\begin{bmatrix}0\\0\\0\end{bmatrix} = \begin{bmatrix}2c_1+c_2+c_3\\c_1+2c_2-4c_3\\c_1+c_2-c_3\end{bmatrix}$$ Comparing entries, we get $2c_1+c_2+c_3 = 0$, $c_1+2c_2-4c_3=0$, and $c_1+c_2-c_3=0$. Solving using substitution and elimination we find that there are infinitely many solutions. Therefore, the set is linearly dependent, and we need to find a vector which is in the span of the others. First: Note that one solution is $c_1=2, c_2=-3, and c_3=-1$. Since $c_3\neq0$, we can solve this equation for that vector to get $\begin{bmatrix}1\\-4\\-1\end{bmatrix} = 2\begin{bmatrix}2\\1\\1\end{bmatrix}-3\begin{bmatrix}1\\2\\1\end{bmatrix}$. Note: that if any $c_i=0$, then that vector cannot be written as a linear combination of the others. \\ 
\textbf{Theorem 1.1.4} If a set of vectors $\{\vec{v}_1,\ldots,\vec{v}_k\}$ contains the zero vector, then it is linearly dependent. \\ 

\paragraph{basis}If a subset of $S$ of $\mathbb{R}^n$ is spanned by a set of vectors $\{\vec{v}_1,\ldots,\vec{v}_k\}$ where the set is linearly independent, then $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is called a \textbf{basis} for $S$. We define a basis for the set $\{\vec{0}\}$ to be the empty set. \\
Examples: Prove that $\mathfrak{B}=\left\{\begin{bmatrix}1\\0\end{bmatrix}, \begin{bmatrix}0\\1\end{bmatrix}\right\}$ is a basis for $\mathbb{R}^2$. \\ 
We need to prove that $\mathfrak{B}$ spans $\mathbb{R}^2$ and that $\mathfrak{B}$ is linearly independent. To prove that $\mathfrak{B}$ spans $\mathbb{R}^2$, we need to show that every vector in $\vec{x} = \begin{bmatrix}x_1\\x_2\end{bmatrix}$ can be written as a linear combination of the vectors in $\mathfrak{B}$. Observe that $\begin{bmatrix}x_1\\x_2\end{bmatrix} = x_1\begin{bmatrix}1\\0\end{bmatrix}+x_2\begin{bmatrix}0\\1\end{bmatrix}$, hence $\mathfrak{B}$ spans $\mathbb{R}^2$. Now consider $\begin{bmatrix}0\\0\end{bmatrix}=c_1\begin{bmatrix}1\\0\end{bmatrix}+c_2\begin{bmatrix}0\\1\end{bmatrix}=\begin{bmatrix}c_1\\c_2\end{bmatrix}$. This gives $c_1=c_2=0$. Hence, $\mathfrak{B}$ is also linearly independent, therefore it is a basis for $\mathbb{R}^2$. This is called a standard basis \\ 
\textbf{Definition} In $\mathbb{R}^n$, let $\vec{e}_i$ represent the vector whose $i-$th component is $1$ and all other components are $0$. The set $\{\vec{e}_1,\ldots,\vec{e}_n\}$ is called the \textbf{standard basis} for $\mathbb{R}^n$. For example, the standard basis for $\mathbb{R}^3$ is $\{\vec{e}_1,\vec{e}_2,\vec{e}_3\}=\left\{\begin{bmatrix}1\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\end{bmatrix}\right\}$. \\ 
Example of non-standard basis: Prove that $\mathfrak{B}=\left\{\begin{bmatrix}1\\3\end{bmatrix},\begin{bmatrix}-1\\-1\end{bmatrix}\right\}$ is a basis for $\mathbb{R}^2$. \\ 
We must prove that it spans $\mathbb{R}^2$, and that it is linearly independent. Consider $$\begin{bmatrix}x_1\\x_2\end{bmatrix}=c_1\begin{bmatrix}1\\3\end{bmatrix}+c_2\begin{bmatrix}-1\\-1\end{bmatrix} = \begin{bmatrix}c_1-c_2\\3c_1-c_2\end{bmatrix}$$ This gives us $c_1-c_2=x_1,3c_1-c_2=x_2$. Solving using substitution and elimnation we get $c_1=\frac{x_2-x_1}{2}$ and $c_2=\frac{x_2-3x_1}{2}$. Thus, $\mathfrak{B}$ spans $\mathbb{R}^2$. Also, if we observe that if we take $x_1=x_2=0$, then we get that the only solution is $c_1=c_2=0$, therefore, the set is also linearly independent, and is a basis for $\mathbb{R}^2$. 

\paragraph{Surfaces in Higher Dimensions} Let $\vec{v}_1,\vec{b}\in\mathbb{R}^n$ with $\vec{v}_1\neq\vec{0}$. Then we call the set with vector equation $\vec{x}=c_1\vec{v}_1+\vec{b}$, $c_1\in\mathbb{R}$ a \textbf{line} in $\mathbb{R}^n$ which passes through $\vec{b}$. \\ 
Let $\vec{v}_1,\vec{v}_2,\vec{b}\in\mathbb{R}^n$ with $\{\vec{v}_1,\vec{v}_2\}$ being a linearly independent set. Then we call the set with vector equation $\vec{x}=c_1\vec{v}_1+c_2\vec{v}_2+\vec{b}$, $c_1,c_2\in\mathbb{R}$ a \textbf{plane} in $\mathbb{R}^n$ which passes through $\vec{b}$. \\ 
Let $\vec{v}_1,\ldots,\vec{v}_{n-1},\vec{b}\in\mathbb{R}^n$ with $\{\vec{v}_1,\ldots,\vec{v}_{n-1}\}$ being linearly independent. Then we call the set with vector equation $\vec{x}=c_1\vec{v}_1+\ldots+c_{n-1}\vec{v}_{n-1}+\vec{b}$, $c_i\in\mathbb{R}$ a \textbf{hyperplane} in $\mathbb{R}^n$ which passes through $\vec{b}$. 

\paragraph{Subspaces} A subset $\mathbb{S}$ of $\mathbb{R}^n$ is called a \textbf{subspace} of $\mathbb{R}^n$ if for every $\vec{x},\vec{y},\vec{w}\in\mathbb{S}$ and $c,d\in\mathbb{R}$, we have \begin{enumerate}
    \item $\vec{x}+\vec{y}\in\mathbb{S}$ 
    \item $(\vec{x}+\vec{y})+\vec{w}=\vec{x}+(\vec{y}+\vec{w})$ 
    \item $\vec{x}+\vec{y}=\vec{y}+\vec{x}$ 
    \item There exists a vector $\vec{0}\in\mathbb{S}$ called the \textbf{zero vector}, such that $\vec{x}+\vec{0}=\vec{x}$ for all $\vec{x}\in\mathbb{S}$. 
    \item There exists a vector $(-\vec{x})\in\mathbb{S}$ such that $\vec{x}+(-\vec{x})=\vec{0}$ 
    \item $c\vec{x}\in\mathbb{S}$ 
    \item $c(d\vec{x})=(cd)\vec{x}$ 
    \item $(c+d)\vec{x}=c\vec{x}+d\vec{x}$ 
    \item $c(\vec{x}+\vec{y})=c\vec{x}+c\vec{y}$
    \item $1\vec{x}=\vec{x}$
\end{enumerate} 
Note that these are all the same properties of addition and scalar multiplication in $\mathbb{R}^n$, so trivially S2,S3,S6-S10 hold. Also, if S6 holds, then $\vec{0}=0\vec{v}\in\mathbb{S}$, and $-\vec{v}=(-1)\vec{v}\in\mathbb{S}$, so S4 and S5 also hold. Thus, to check if a non-empty subset $\mathbb{S}$ of $\mathbb{R}^n$ is a subspace of $\mathbb{R}^n$ we can use the following theorem: \\ 
\textbf{Theorem 1.2.1- The Subspace Test} Let $\mathbb{S}$ be a non-empty subset of $\mathbb{R}^n$. If $\vec{x}+\vec{y}\in\mathbb{S}$ and $c\vec{x}\in\mathbb{S}$ for all $\vec{x},\vec{y}\in\mathbb{S}$ and $c\in\mathbb{R}$, then $\mathbb{S}$ is a subspace of $\mathbb{R}^n$. \\ 
Example: Determine if $\mathbb{S}=\left\{\begin{bmatrix}x_1\\x_1\end{bmatrix}|x_1\in\mathbb{R}\right\}$ is a subspace of $\mathbb{R}^2$. \\ 
By definition, $\mathbb{S}$ is a subset of $\mathbb{R}^2$. Taking $x_1=0$, gives $\vec{0}\in\mathbb{S}$. Hence, $\mathbb{S}$ is non-empty. Let $\vec{x},\vec{y}\in\mathbb{S}$. Then by definition of $\mathbb{S}$ there exists $x_1,y_1\in\mathbb{R}$ such that $\vec{x}=\begin{bmatrix}x_1\\x_1\end{bmatrix},\vec{y}=\begin{bmatrix}y_1\\y_1\end{bmatrix}$. We have $\vec{x}+\vec{y}=\begin{bmatrix}x_1+y_1\\x_1+y_1\end{bmatrix}\in\mathbb{S}$. And for any $c\in\mathbb{R}$ we get $c\vec{x}=\begin{bmatrix}cx_1\\cx_1\end{bmatrix}\in\mathbb{S}$, therefore $\mathbb{S}$ satisfies all the conditions of the Subspace Test, so $\mathbb{S}$ is a subspace of $\mathbb{R}^2$. \\ 
Example: Determine if $\mathbb{S}=\left\{\begin{bmatrix}x_1\\x_2\end{bmatrix}|x_1^2-x_2^2=0\in\mathbb{R}\right\}$ is a subspace of $\mathbb{R}^2$. \\ 
First, observe that since the conditions are not a linear equation. This makes us believe that it is not a subspace. Observe that $\vec{x}=\begin{bmatrix}1\\1\end{bmatrix}$ and $\vec{y}=\begin{bmatrix}-1\\1\end{bmatrix}$ are both in $\mathbb{S}$. Then $\vec{x}+\vec{y}=\begin{bmatrix}0\\2\end{bmatrix}$, which is not in $\mathbb{S}$, since $0^2-2^2\neq0$. Therefore, $\mathbb{S}$ is not a subspace. \\ 
Example: Prove that $\mathbb{S}=\text{Span}\left\{\begin{bmatrix}1\\2\\-1\end{bmatrix},\begin{bmatrix}3\\1\\0\end{bmatrix}\right\}$ is a subspace of $\mathbb{R}^3$. \\ 
By definition of spanning and Theorem 1.1.1, we get that $\mathbb{S}$ is a subset of $\mathbb{R}^3$. Moreover, we have that $$\vec{0}=0\begin{bmatrix}1\\2\\-1\end{bmatrix}+0\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}$$ Let $\vec{x},\vec{y}\in\mathbb{S}$. Then there exists $s_1,s_2,t_1,t_2\in\mathbb{R}$ such that $$\vec{x}=s_1\begin{bmatrix}1\\2\\-1\end{bmatrix}+s_2\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}$$ $$\vec{y}=t_1\begin{bmatrix}1\\2\\-1\end{bmatrix}+t_2\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}$$ Therefore, we get that $$\vec{x}+\vec{y}=s_1\begin{bmatrix}1\\2\\-1\end{bmatrix}+s_2\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}+t_1\begin{bmatrix}1\\2\\-1\end{bmatrix}+t_2\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}=(s_1+t_1)\begin{bmatrix}1\\2\\-1\end{bmatrix} + (s_2+t_2)\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}$$ Similarly, we get $$c\vec{x}=c\left(s_1\begin{bmatrix}1\\2\\-1\end{bmatrix}+s_2\begin{bmatrix}3\\1\\0\end{bmatrix}\right)=(cs_1)\begin{bmatrix}1\\2\\-1\end{bmatrix} + (cs_2)\begin{bmatrix}3\\1\\0\end{bmatrix}\in\mathbb{S}$$ Therefore $\mathbb{S}$ is a subspace of $\mathbb{R}^3$ by the subspace test. \\

\textbf{Theorem 1.2.2} If $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is a set of vectors in $\mathbb{R}^n$, then Span$\{\vec{v}_1,\ldots,\vec{v}_k\}$ is a subspace of $\mathbb{R}^n$.   
\paragraph{Finding a basis}
\begin{enumerate}
    \item Find a general form of a vector in $\mathbb{S}$ 
    \item Use the general form to find a spanning set for $\mathbb{S}$ 
    \item Use theorem 1.1.2 as many times as necessary to get a linearly independent spanning set. 
\end{enumerate}
Example: Find a basis for the subspace $\mathbb{S}_1=\left\{\begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}\in\mathbb{R}^4|x_1-x_2+x_4=0\right\}$ \\ 
Observe that if $\vec{x}\begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix}\in\mathbb{S}_1$, then $x_1-x_2+x_4=0\Rightarrow x_1=x_2-x_4$. Thus, every vector $\vec{x}$ has the form $$\vec{x}=\begin{bmatrix}x_2-x_4\\x_2\\x_3\\x_4\end{bmatrix}=x_2\begin{bmatrix}1\\1\\0\\0\end{bmatrix}+x_3\begin{bmatrix}0\\0\\1\\0\end{bmatrix}+x_4\begin{bmatrix}-1\\0\\0\\1\end{bmatrix}$$ Thus we have $\mathfrak{B}=\left\{\begin{bmatrix}1\\1\\0\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\\0\end{bmatrix},\begin{bmatrix}-1\\0\\0\\1\end{bmatrix}\right\}$ is a spanning set for $\mathbb{S}_1$. Then, since every linear combination of the vectors in $\mathfrak{B}$ are in $\mathbb{S}$, Span$\mathfrak{B}$ is equal to $\mathbb{S}$. Now, we need to find if the spanning set can be simplified. Note: This is the same as asking if the set is linearly independent or not. Consider $$c_1\begin{bmatrix}1\\1\\0\\0\end{bmatrix}+c_2\begin{bmatrix}0\\0\\1\\0\end{bmatrix}+c_3\begin{bmatrix}-1\\0\\0\\1\end{bmatrix}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix}$$ Simplifying the left hand side gives us $$\begin{bmatrix}c_1-c_3\\c_1\\c_2\\c_3\end{bmatrix} = \begin{bmatrix}0\\0\\0\\0\end{bmatrix}$$. Therefore, $c_1=c_2=c_3=0$ is the only solution, and so it is linearly dependent, and a basis for $\mathbb{S}_1$. \\ 
Example 2: Find a basis for the subspace $\mathbb{S}_2=\left\{\begin{bmatrix}a+c\\b-c\\2a+b+c\end{bmatrix}\in\mathbb{R}^3|a,b,c\in\mathbb{R}\right\}$ \\ 
By definition of $\mathbb{S}_2$ we have that every vector in $\mathbb{S}_2$ has the form $$\vec{x}=\begin{bmatrix}a+c\\b-c\\2a+b+c\end{bmatrix}=a\begin{bmatrix}1\\0\\2\end{bmatrix}+b\begin{bmatrix}0\\1\\1\end{bmatrix}+c\begin{bmatrix}1\\-1\\1\end{bmatrix}$$ This vector equation for $\mathbb{S}_2$ shows that these vectors form a spanning set for $\mathbb{S}_2$. Again, we need to determine if the set is linearly independent or not. Consider $$c_1\begin{bmatrix}1\\0\\2\end{bmatrix}+c_2\begin{bmatrix}0\\1\\1\end{bmatrix}+c_3\begin{bmatrix}1\\-1\\1\end{bmatrix}=\begin{bmatrix}0\\0\\0\end{bmatrix}$$ Observe that one solution is $c_1=1,c_2=-1,c_3=-1$. Hence, the set is linearly dependent. In particular, we see that $$\begin{bmatrix}1\\0\\2\end{bmatrix}=\begin{bmatrix}0\\1\\1\end{bmatrix}+\begin{bmatrix}1\\-1\\1\end{bmatrix}$$ Hence by theorem 1.1.2 we get $$\mathbb{S}_2=\text{Span}\left\{\begin{bmatrix}0\\1\\1\end{bmatrix},\begin{bmatrix}1\\-1\\1\end{bmatrix}\right\}$$ Since neither vector in the spanning set is a scalar multiple of the other, it must be linearly independent. Hence $\mathfrak{B} = \left\{\begin{bmatrix}0\\1\\1\end{bmatrix},\begin{bmatrix}1\\-1\\1\end{bmatrix}\right\}$ is a basis for $\mathbb{S}_2$. 

\paragraph{Dot Product} Recall that $\begin{bmatrix}x_1\\x_2\end{bmatrix}\cdot \begin{bmatrix}y_1\\y_2\end{bmatrix} = x_1y_1+x_2y_2$. Furthermore, the length of $\vec{x}$ is given by $||\vec{x}||=\sqrt{x_1^2+x_2^2}=\sqrt{\vec{x}\cdot\vec{x}}$. Additionally, (Theorem 1.3.1) $\vec{x}\cdot\vec{y}=||\vec{x}||\,||\vec{y}||\cos\theta$. \\ 
The \textbf{dot product} of $\vec{x}\cdot\vec{y}$ is defined by $\vec{x}\cdot\vec{y}=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\cdot\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}=x_1y_1+\ldots+x_ny_n=\sum_{i=1}^nx_iy_i$. It is also called the \textbf{scalar product} or the \textbf{standard inner product}. \\ 
\textbf{Theorem 1.3.2} Properties of the dot product: If $\vec{x},\vec{y},\vec{z}\in\mathbb{R}^n$ and $s,t\in\mathbb{R}$, then \begin{enumerate}
    \item $\vec{x}\cdot\vec{x}\geq0$ and $\vec{x}\cdot\vec{x}=0\Leftrightarrow \vec{x}=\vec{0}$ 
    \item $\vec{x}\cdot\vec{y}=\vec{y}\cdot\vec{x}$ 
    \item $\vec{x}\cdot(s\vec{y}+t\vec{z})=s(\vec{x}\cdot\vec{y})+t(\vec{x}\cdot\vec{z})$  
\end{enumerate}

\paragraph{Length} The \textbf{length} $||\vec{x}||$ of $\vec{x}$ in $\mathbb{R}^n$ is defined by $$||\vec{x}||=\sqrt{x_1^2+\ldots+x_n^2}=\sqrt{\vec{x}\cdot\vec{x}}$$. A vector $\vec{x}\in\mathbb{R}^n$ for which $||\vec{x}||=1$ is called a \textbf{unit vector}. Here are some properties: (Theorem 1.3.3) \begin{enumerate}
    \item $||\vec{x}||\geq0$ and $||\vec{x}||=0\Leftrightarrow \vec{x}=\vec{0}$ 
    \item $||c\vec{x}||=|c|\,||\vec{x}||$ 
    \item $|\vec{x}\cdot\vec{y}| \leq ||\vec{x}||\,||\vec{y}||$ 
    \item $||\vec{x}+\vec{y}||\leq||\vec{x}||+||\vec{y}||$ 
\end{enumerate}

\paragraph{Orthogonality} We define the \textbf{angle} between two vectors $\vec{x}$ and $\vec{y}$ to be an angle $\theta$ such that $\vec{x}\cdot\vec{y}=||\vec{x}||\,||\vec{y}||\cos\theta$. If $\vec{x}\cdot\vec{y}=0$, then we say that $\vec{x}$ and $\vec{y}$ are \textbf{orthogonal}. By this definition, $\vec{0}$ is orthogonal to every vector in $\mathbb{R}^n$.  

\paragraph{Cross Product} Let $\vec{v},\vec{w}\in\mathbb{R}^3$. Then the \textbf{cross product} of $\vec{v}=\begin{bmatrix}v_1\\v_2\\v_3\end{bmatrix}$ and $\vec{w}=\begin{bmatrix}w_1\\w_2\\w_3\end{bmatrix}$ is $\vec{v}\times\vec{w}=\begin{bmatrix}v_2w_3-v_3w_2\\v_3w_1-v_1w_3\\v_1w_2-v_2w_1\end{bmatrix}$ Here are some properties (Theorem 1.3.4): \begin{enumerate}
    \item If $\vec{n}=\vec{v}\times\vec{w}$, then for any $\vec{y}\in\text{Span}\{\vec{v},\vec{w}\}$, we have $\vec{y}\cdot\vec{n}=0$. In other words, any linear combination of $\vec{v}$ and $\vec{w}$ is orthogonal to $\vec{n}$
    \item $\vec{v}\times\vec{w}=-\vec{w}\times\vec{v}$ 
    \item $\vec{v}\times\vec{v}=\vec{0}$ 
    \item $\vec{v}\times\vec{w}=\vec{0}$ iff either $\vec{v}=\vec{0}$ or $\vec{w}$ is a scalar multiple of $\vec{v}$ 
    \item $\vec{v}\times(\vec{w}+\vec{x})=\vec{v}\times\vec{w}+\vec{v}\times\vec{x}$ 
    \item $(c\vec{v})\times\vec{w}]=c(\vec{v}\times\vec{w})$ 
    \item $||\vec{v}\times\vec{w}||=||\vec{v}||\,||\vec{w}||\,|\sin\theta|$ 
\end{enumerate}
Note that the cross product is not associative. ie. $\vec{v}\times(\vec{w}\times\vec{x})\neq(\vec{v}\times\vec{w})\times\vec{x}$

\paragraph{Scalar Equation of a Plane(Theorem 1.3.5)}Let $\vec{v},\vec{w},\vec{b}\in\mathbb{R}^3$ with $\{\vec{v},\vec{w}\}$ being linearly independent and let $P$ be a plane in $\mathbb{R}^3$ with vector equation $\vec{x}=s\vec{v}+t\vec{w}+\vec{b}$ with $s,t\in\mathbb{R}$. If $\vec{n}=\vec{v}\times\vec{w}$, then an equation for the plane is $(\vec{x}-\vec{b})\cdot\vec{n}=0$. It is common to rearrange and expand this equation to $n_1x_1+n_2x_2+n_3x_3=b_1n_1+b_2n_2+b_3n_3$. This is called a \textbf{scalar equation} for the plane, and $\vec{n}=\begin{bmatrix}n_1\\n_2\\n_3\end{bmatrix}$ is called the \textbf{normal vector} for the plane. Note that every plane has infinitely many scalar equations, and every scalar equation has a corresponding normal vector. \\ 
Example: Find a scalar equation of the plane with normal vector $\vec{n}=\begin{bmatrix}3\\0\\-2\end{bmatrix}$ that passes through $B(4,-1/3,1)$. \\ 
Using the formula, we get that the scalar equation is $$n_1x_1+n_2x_2+n_3x_3=b_1n_1+b_2n_2+b_3n_3$$ $$3x_1+0x_2+-2x_3=4(3)-\frac{1}{3}(0)+1(-2)$$ $$3x_1-2x_3=10$$ \\
Example 2: What is a normal vector for the plane with scalar equation $x_1+4x_2=10$.\\ 
By the definition of the scalar equation, the coefficients of $x_1,x_2,x_3$ are the components of the normal vector $\vec{n}$. Thus $\vec{n}=\begin{bmatrix}1\\4\\0\end{bmatrix}$ \\ 
Example 3: Find a normal vector to the plan with vector equation $\vec{x}=\begin{bmatrix}1\\2\\2\end{bmatrix}+s\begin{bmatrix}3\\5\\-1\end{bmatrix}+t\begin{bmatrix}-1\\2\\3\end{bmatrix}$. Then, find a scalar equation of the plane. \\ 
By theorem 1.3.5, a normal vector to the plane is $$\vec{n}=\begin{bmatrix}3\\5\\-1\end{bmatrix}\times\begin{bmatrix}-1\\2\\3\end{bmatrix}=\begin{bmatrix}17\\-8\\11\end{bmatrix}$$. Since $B(1,2,2)$ is a point on the plane, we get that a scalar equation for the plane is $$17x_1-8x_2+11x_3=17(1)-8(2)+11(2)$$ $$17x_1-8x_2+11x_3=23$$

\paragraph{Scalar equation of a Hyperplane} If $\{\vec{v}_1,\ldots,\vec{v}_{m-1}\}$ is a linearly independent set in $\mathbb{R}^m$ and $\vec{b}\in\mathbb{R}^m$, then by definition $$\vec{x}=c_1\vec{v}_1+\ldots+c_{m-1}\vec{v}_{m-1}+\vec{b}$$ is a vector equation for a hyperplane in $\mathbb{R}^m$. Let $\vec{n}=\begin{bmatrix}n_1\\\vdots\\n_m\end{bmatrix}\in\mathbb{R}^m$ be a vector which is orthogonal to every vector $\vec{v}_i$ for $1\leq i\leq m-1$. Then we find the standard form for a scalar equation of a hyperplane in $\mathbb{R}^m$ is $$n_1x_1+\ldots+n_mx_m=\vec{b}\cdot\vec{n}$$ \\ 
Example: Find the normal vector of the hyperplane $3x_1-2x_2+3x_3-x_4=11$ in $\mathbb{R}^4$. Just like the case in $\mathbb{R}^3$, by definition, $\vec{n}=\begin{bmatrix}3\\-2\\3\\-1\end{bmatrix}$

\paragraph{Projections} Let $\vec{a},\vec{b}\in\mathbb{R}^n$ with $\vec{a}\neq\vec{0}$. The \textbf{projection} of $\vec{b}$ onto $\vec{a}$ is defined by $\text{proj}_{\vec{a}}\vec{b}=\frac{\vec{b}\cdot\vec{a}}{||\vec{a}||^2}\vec{a}$. The \textbf{perpendicular} of $\vec{b}$ onto $\vec{a}$ is defined by $\text{perp}_{\vec{a}}\vec{b}=\vec{b}-\text{proj}_{\vec{a}}\vec{b}$. \\ 
Example: Let $\vec{a}=\begin{bmatrix}3\\4\end{bmatrix}$ and let $\vec{b}=\begin{bmatrix}-1\\-1\end{bmatrix}$. Find $\text{proj}_{\vec{a}}\vec{b}$ and $\text{perp}_{\vec{a}}\vec{b}$. \\ 
We have $\text{proj}_{\vec{a}}\vec{b}=\frac{\vec{b}\cdot\vec{a}}{||\vec{a}||^2}\vec{a}=\frac{-7}{25}\begin{bmatrix}3\\4\end{bmatrix}=\begin{bmatrix}-21/25\\-28/25\end{bmatrix}$\\ 
$\text{perp}_{\vec{a}}\vec{b}=\vec{b}-\text{proj}_{\vec{a}}\vec{b}=\begin{bmatrix}-1\\-1\end{bmatrix}-\begin{bmatrix}-21/25\\-28/25\end{bmatrix}=\begin{bmatrix}-4/25\\3/25\end{bmatrix}$

\paragraph{Projection onto a Plane} The projection of $\vec{u}\in\mathbb{R}^3$ onto a plane $P$ with normal vector $\vec{n}$ is defined by $\text{proj}_P\vec{u}=\text{perp}_{\vec{n}}\vec{u}$. \\ 
Example: Find the projection of $\vec{b}=\begin{bmatrix}2\\3\\1\end{bmatrix}$ onto the plane $3x_1-x_2+4x_3=0$. \\ 
We see that $\vec{n}=\begin{bmatrix}3\\-1\\4\end{bmatrix}$. Hence the projection of $\vec{b}$ onto the plane is $\text{proj}_P\vec{u}=\text{perp}_{\vec{n}}\vec{u}=\vec{b}-\text{proj}_{\vec{n}}\vec{b}=\begin{bmatrix}2\\3\\1\end{bmatrix}-\frac{\vec{b}\cdot\vec{n}}{||\vec{n}||^2}\vec{n}=\begin{bmatrix}31/26\\85/26\\-2/26\end{bmatrix}$

\paragraph{Systems of Linear Equations} An equation in $n$ variables $x_1,\ldots,x_n$ that can be written in the form $a_1x_1+\ldots+a_nx_n=b$ where $a_1,\ldots,a_n,b$ are constants is called a \textbf{linear equation}. The constants $a_i$ are called \textbf{coefficients} of the equation and $b$ is called the \textbf{right-hand side}. The form $a_1x_1+\ldots+a_nx_n=b$ is called the \textbf{standard form} of the linear equation. A set of $m$ linear equations in the same variables is called a \textbf{system of $m$ linear equations in $n$ variables}. A general system of $m$ linear equations in $n$ variables has the form $$a_{11}x_1+a_{12}x_2+\ldots+a_{1n}x_n=b_1$$ $$a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n=b_2$$ $$\vdots$$ $$a_{m1}x_1+a_{m2}x_n+\ldots+a_{mn}x_n=b_m$$. observe that the coefficient $a_{ij}$ represents the coefficient of $x_j$ in the $i$-th equation. A \textbf{solution} to a system of $m$ linear equations in $n$ variables is a vector $\begin{bmatrix}s_1\\\vdots\\s_n\end{bmatrix}\in\mathbb{R}^n$ such that all $m$ equations are satisfied when we set $x_1=s_1$, $x_2=s_2$, $\ldots$, $x_n=s_n$. The set of all solutions of a system of linear equations is called the \textbf{solution set} of the system. If a system of linear equations has at least one solution, then it is said to be \textbf{consistent}. Otherwise, it is said to be \textbf{inconsistent}. 

\paragraph{Solution of Systems} If a system of linear equations has two distinct solutions $\vec{s}=\begin{bmatrix}s_1\\\vdots\\s_n\end{bmatrix}$ and $\vec{t}=\begin{bmatrix}t_1\\\vdots\\t_n\end{bmatrix}$, then $\vec{x}=\vec{s}+c(\vec{s}-\vec{t})$ is a distinct solution for each $c\in\mathbb{R}$. IE. A system of linear equations can only have: \begin{itemize}
    \item no solutions
    \item one solution 
    \item infinitely many solutions
\end{itemize}

\paragraph{Matrix Representation of a System} Two systems of linear equations which have the same solution set are said to be \textbf{equivalent}. This is where augmented matrices are introduced, which I really don't want to write out in LaTeX. We can denote $\vec{a}_i$ to be all the coefficients of $x_i$. Then, we can denote the coefficient matrix of the system by $A=[\vec{a}_1\cdots\vec{a}_n]$. If we let $\vec{b}=\begin{bmatrix}b_1\\\vdots\\b_m\end{bmatrix}$, then we can denote the augmented matrix as $[A|\vec{b}]$\\

\paragraph{Elementary Row Operations} There are three elementary row operations (EROs): 
\begin{enumerate}
    \item Multiply a row by a non-zero scalar 
    \item Add a multiple of one row to another row 
    \item Swap one row with another row 
\end{enumerate}
\textbf{Theorem 2.2.1}: If the augmented matrix $[A_2|\vec{b}_2]$ can be obtained from the augmented matrix $[A_1|\vec{b}_1]$ by performing EROs, then the corresponding systems of linear equations are equivalent. \\ 
Two matrices $A$ and $B$ are said to be \textbf{row equivalent} if there exists a sequence of elementary row operations that transform $A$ into $B$. We write $A\sim B$. The procedure of applying elementary row operations to transform $A$ into $B$ is called \textbf{row reducing}. \\ 
Here's some notation: \begin{itemize}
    \item $cR_i$ to indicate multiplying the $i$-th row by $c\neq0$. 
    \item $R_i+cR_j$ to indicate adding $c$ times the $j^{th}$ row to the $i^{th}$ row. 
    \item $R_i\leftrightarrow R_j$ to indicate swapping the $i^{th}$ row and the $j^{th}$ row. 
\end{itemize}
Example: Solve the following systems of equations: $$x_1+x_2+x_3=1$$ $$2x_2-6x_3=2$$
$$3x_1+6x_2-5x_3=4$$ \\ 
The augmented matrix for the system is $\begin{bmatrix}1&1&1&|1\\0&2&-6&|2\\3&6&-5&|4\end{bmatrix}$. $$R_3-3R_1\sim \begin{bmatrix}1&1&1&|1\\0&2&-6&|2\\0&3&-8&|1\end{bmatrix}$$ $$\frac{1}{2}R_2\sim\begin{bmatrix}1&1&1&|1\\0&1&-3&|1\\3&6&-5&|4\end{bmatrix}$$ $$R_1-R_2, R_3-3R_2 \sim\begin{bmatrix}1&0&4&|0\\0&1&-3&|1\\0&0&1&|-2\end{bmatrix}$$ $$R_1-4R_3, R_2+3R_3\sim \begin{bmatrix}1&0&0&|8\\0&1&0&|-5\\0&0&1&|-2\end{bmatrix}$$
From this we can see the new system of linear equations is $x_1=8$, $x_2=-5$, $x_3=-2$. This is called the Gauss-Jordan elimination. 

\paragraph{Reduced Row Echelon Form} A matrix $R$ is said to be in \textbf{reduced row echelon form (RREF)} if: \begin{enumerate}
    \item All rows containing a non-zero entry are above rows which only contain zeros 
    \item The first non-zero entry in each non-zero row is 1, called a \textbf{leading one}
    \item The leading one in each non-zero row is to the right of the leading one in any row above it 
    \item A leading one is the only non-zero entry in its column
\end{enumerate}
\textbf{Theorem 2.2.2} Every matrix has a unique reduced row echelon form. \\ 
Any variable whose column does not contain a leading one in the RREF of the coefficient matrix of a system of linear equations is called a \textbf{free variable}. If we find that a system of linear equations has one or more free variables and is consistent, then the system has infinitely many solutions and each free variable will correspond to a parameter in the general solution of the system. In particular, after converting the RREF of the augmented matrix back to equation form, we always move all free-variables to the right side of the equation and replace them with parameters. \\ 
Example: $$x_1+2x_2+x_3+x_4=1$$ $$x_3-2x_4=2$$ Converting to augmented matrix form, $$\left[\begin{array}{rrrr|r}
    1 & 2 & 1 & 1 & 1 \\
    0 & 0 & 1 & -2 & 2
\end{array}\right]$$ 
$$R_1-R_2\sim \left[\begin{array}{rrrr|r}
    1 & 2 & 0 & 3 & -1 \\
    0 & 0 & 1 & -2 & 2
\end{array}\right]$$ Note that second and fourth columns do not have leading ones, and hence are free variables. Converting back to equation form, we get $x_1 = -1-2x_2-3x_4$ and $x_3 = 2+2x_4$. Hence every solution $\vec{x}$ has the form 
$$\vec{x}=\begin{bmatrix}x_1\\x_2\\x_3\\x_4\end{bmatrix} = \begin{bmatrix}-1-2x_2-3x_4\\x_2\\2+2x_4\\x_4\end{bmatrix} =\begin{bmatrix}-1\\0\\2\\0\end{bmatrix} +x_2\begin{bmatrix}-2\\1\\0\\0\end{bmatrix} +x_4\begin{bmatrix}-3\\0\\2\\1\end{bmatrix} $$ for $x_2,x_4\in\mathbb{R}$. 

\paragraph{Homogeneous Systems} A system of linear equations is said to be a \textbf{homogeneous system} if the right hand side only contains zeros. That is, it has the form $[A|\vec{0}]$. \\ 
Example: Determine if $\left\{\begin{bmatrix}1\\1\\0\\2\end{bmatrix} ,\begin{bmatrix}2\\3\\2\\1\end{bmatrix} ,\begin{bmatrix}1\\2\\2\\-1\end{bmatrix} ,\begin{bmatrix}-1\\1\\7\\8\end{bmatrix} \right\}$ \\ 
Consider $$c_1\begin{bmatrix}1\\1\\0\\2\end{bmatrix} +c_2\begin{bmatrix}2\\3\\2\\1\end{bmatrix} +c_3\begin{bmatrix}1\\2\\2\\-1\end{bmatrix} +c_4\begin{bmatrix}-1\\1\\7\\8\end{bmatrix}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix}$$ Comparing entries gives the homogeneous system $$c_1+2c_2+c_3-c_4=0$$ $$c_1+3c_2+2c_3+c_4=0$$ $$2c_2+2c_3+7c_4=0$$ $$2c_1+c_2-c_3+8c_4=0$$. Therefore, this is a homogeneous system and we can transform this into an augmented matrix. $$\left[\begin{array}{rrrr|r}
    1 & 2 & 1 & -1 & 0 \\
    1 & 3 & 2 & 1 & 0 \\ 
    0&2&2&7&0\\
    2&1&-1&8&0 
\end{array}\right] \sim \left[\begin{array}{rrrr|r}
    1 & 0 & -1 & 0 & 0 \\
    0 & 1 & 1 & 0 & 0 \\ 
    0& 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0
\end{array}\right]$$
Note that since $x_3$ is a free variable, there are infinitely many solutions and hence the set is linearly dependent. \\ 
Some properties of homogeneous systems: \begin{enumerate}
    \item In some applications we will not actually need to find solutions. We may just be interested in determining if a system is inconsistent, consistent with a unique solution, or consistent with infinitely many solutions. 
    \item $\vec{x}=\vec{0}$ is always a solution to a homogeneous system. Thus, every homogeneous system is consistent. 
    \item When solving a homogeneous system, we can just row reduce the coefficient matrix since the right hand side will always only contain 0s.
\end{enumerate} 
\textbf{Theorem 2.2.3} The solution set of a homogeneous of $m$ linear equations in $n$ variables is a subspace of $\mathbb{R}^n$. This leads to a definition: The solution set of a homogeneous system is called the \textbf{solution space} of the system. 

\paragraph{Rank} The \textbf{rank} of a matrix $A$ is the number of leading ones in the RREF of the matrix and is denoted rank $A$. \\ 
\textbf{Theorem 2.2.4} If $A$ is a matrix with $m$ rows and $n$ columns, then rank $A\leq \min(m,n)$. \\ 
\textbf{Theorem 2.2.5} Let $A$ be the coefficient matrix of a system of $m$ linear equations in $n$ unknowns $[A|\vec{b}]$ \begin{enumerate}
    \item If the rank of $A$ is less than the rank of the augmented matrix $[A|\vec{b}]$, then the system is inconsistent. 
    \item If the system $[A|\vec{b}]$ is consistent, then the system contains $(n-\text{rank }A)$ free variables. In other words, a consistent system of $m$ linear equations in $n$ variables with coefficient matrix $A$ has a unique solution if and only if rank $A=n$. 
    \item rank $A=m$ if and only if $[A|\vec{b}]$ is consistent for every $\vec{b}\in\mathbb{R}^m$. 
\end{enumerate}
\textbf{Theorem 2.2.6} Let $[A|\vec{b}]$ be a consistent system of $m$ linear equations in $n$ variables with RREF $[R|\vec{c}]$. If rank $A=k\leq n$, then a vector equation of the solution set of $[A|\vec{b}]$ has the form $\vec{x}=\vec{d}+t_1\vec{v}_1+\ldots+t_{n-k}\vec{v}_{n-k}$, where $\vec{d}\in\mathbb{R}^n$ and $\{\vec{v}_1,\ldots,\vec{v}_{n-k}\}$ is linearly independent set in $\mathbb{R}^n$. In particular, the solution set of $[A|\vec{b}]$ is an $(n-k)$-flat in $\mathbb{R}^n$.  

\paragraph{Matrices} An $m\times n$ matrix $A$ is a rectangular array with $m$ rows and $n$ columns. We denote the entry in the $i^{th}$ row and the $j^{th}$ column by $a_{ij}$ or $(A)_{ij}$. That is, $$A = \begin{bmatrix}a_{11}&\ldots&a_{1n}\\ \vdots &\ddots&\vdots\\a_{m1}&\ldots&a_{mn}\end{bmatrix}$$ The set of all $m\times n$ matrices with real entries is denoted by $M_{m\times n}(\mathbb{R})$. Equality, addition, scalar multiplication, and linear combinations is exactly the same as with vectors. Here are some other properties of matrices: For all $A,B,C\in M_{m\times n}(\mathbb{R})$ and $s,t\in\mathbb{R}$, we have \begin{enumerate}
    \item $A+B\in M_{m\times n}(\mathbb{R})$ 
    \item $(A+B)+C=A+(B+C)$
    \item $A+B=B+A$ 
    \item There exists a matrix, denoted by $O_{m,n}$ such that $A+O_{m,n}=A$ for all $A$. In particular, $O_{m,n}$ is the $m\times n$ matrix with all entries zero and is called the \textbf{zero matrix}. 
    \item There exists an $m\times n$ matrix $(-A)$, with the property that $A+(-A)=O_{m,n}$. $(-A)$ is called the \textbf{additive inverse} of $A$. 
    \item $sA\in M_{m\times n}(\mathbb{R})$ 
    \item $s(tA)=(st)A$ 
    \item $(s+t)A=sA+tA$ 
    \item $s(A+B)=sA+sB$ 
    \item $1A = A$ 
\end{enumerate}

\paragraph{Transpose} Let $A\in M_{m\times n}(\mathbb{R})$. We define the \textbf{transpose} of $A$, denoted $A^T$, to be the $n\times m$ matrix such that $(A^T)_{ij}=(A)_{ji}$. \\
Examples: If $A = \begin{bmatrix}-2\\3\\1\end{bmatrix}$, then $A^T=\begin{bmatrix}-2&3&2\end{bmatrix}$ \\ 
If $B = \begin{bmatrix}1&3&4\\2&1&0\end{bmatrix}$, then $B^T=\begin{bmatrix}1&2\\3&1\\4&0\end{bmatrix}$ \\ 
If $C = \begin{bmatrix}1&0\\0&3\end{bmatrix}$, then $C^T=\begin{bmatrix}1&0\\0&3\end{bmatrix}$\\ 
Notice that $C^T=C$. Such matrices are called \textbf{symmetric}. \\ 
If $\vec{a}_1=\begin{bmatrix}3\\5\\-1\\2\end{bmatrix}$ and $\vec{a}_2=\begin{bmatrix}0\\4\\6\\-9\end{bmatrix}$, then the matrix $A=\begin{bmatrix}\vec{a}_1^T\\\vec{a}_2^T\end{bmatrix}$ is the matrix $A=\begin{bmatrix}3&5&-1&2\\0&4&6&-9\end{bmatrix}$ \\ 
Let $A = \begin{bmatrix}1&3\\0&-2\end{bmatrix}=\begin{bmatrix}\vec{a}_1^T\\\vec{a}_2^T\end{bmatrix}$. What are $\vec{a}_1$ and $\vec{a}_2$? Trivially, $\vec{a}_1=\begin{bmatrix}1\\3\end{bmatrix}$, and $\vec{a}_2=\begin{bmatrix}0\\-2\end{bmatrix}$ \\ 
Here are some properties of transpose (theorem 3.1.2): For any $A,B\in M_{m\times n}(\mathbb{R})$, and scalar $c\in\mathbb{R}$, we have  \begin{enumerate}
    \item $(A^T)^T=A$ 
    \item $(A+B)^T = A^T+B^T$ 
    \item $(cA)^T = cA^T$
\end{enumerate} 
Proof of 1: By the definition of the transpose we have $((A^T)^T)_{ij}=(A^T)_{ji}=A_{ij}$. \\ 
Proof of 2: $[(A+B)^T]_{ij}=(A+B)_{ji}=A_{ji}+B_{ji}=(A^T)_{ij}+(B^T)_{ij}$\\ 
Proof of 3: $(cA^T)_{ij}=c(A^T)_{ij}=c(A)_{ji}=(cA)_{ji}=[(cA)^T]_{ij}$ 

\paragraph{Matrix-Vector Multiplication} Let $A = \begin{bmatrix}\vec{a}_1^T\\\vdots\\\vec{a}_m^T\end{bmatrix}$ be an $m\times n$ matrix. The components of $\vec{a}_i$ are the coefficients from the $i^{th}$ equation in the system. Hence, we get that $\vec{b}_i=a_{i1}x_1+\ldots+a_{in}x_n=\vec{a}_i\cdot\vec{x}$. Therefore, the entire system of linear equations may be written in the form $\begin{bmatrix}\vec{a}_1\cdot\vec{x}\\\vdots\\\vec{a}_m\cdot\vec{x}\end{bmatrix}=\begin{bmatrix}b_1\\\vdots\\b_m\end{bmatrix}$ From this, we make our definition that $A\vec{x} = \begin{bmatrix}\vec{a}_1\cdot\vec{x}\\\vdots\\\vec{a}_m\cdot\vec{x}\end{bmatrix}$. \\ 
Example: Let $A = \begin{bmatrix}0&1&-3\\4&8&-2\end{bmatrix}$. Calculate $A\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$. \\ 
$$A\vec{x}=\begin{bmatrix}0&1&-3\\4&8&-2\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix} = \begin{bmatrix}0(x_1)+1(x_2)+(-3)(x_3)\\4(x_1)+8(x_2)+(-2)(x_3)\end{bmatrix} = \begin{bmatrix}x_2-3x_3\\4x_1+8x_2-2x_3\end{bmatrix}$$ 
The first entry of $A\vec{x}$ is the dot product of the first row of $A$ and $\vec{x}$, whereas the second entry is the dot product of the second row of $A$ and $\vec{x}$. Note: to multiply $A\vec{x}$, the number of columns of $A$ must equal the number of entries in $\vec{x}$ so that the dot products are defined. In particular, if $A$ is $m\times n$, then $\vec{x}$ must be in $\mathbb{R}^n$. \\ 
Example 2: Let $A= \begin{bmatrix}4&5\\-2&3\\1&-1\end{bmatrix}$. Calculate $A\vec{x}$ where \begin{itemize}
    \item $\vec{x}=\begin{bmatrix}1\\0\end{bmatrix}$. We get $A\vec{x}=\begin{bmatrix}4(1)+5(0)\\-2(1)+3(0)\\1(1)+(-1)(0)\end{bmatrix} = \begin{bmatrix}4\\-2\\1\end{bmatrix}$ 
    \item $\vec{x}=\begin{bmatrix}0\\1\end{bmatrix}$. We get $A\vec{x}=\begin{bmatrix}4(0)+5(1)\\-2(0)+3(1)\\1(0)+(-1)(1)\end{bmatrix} = \begin{bmatrix}5\\3\\-1\end{bmatrix}$ 
    \item $\vec{x}=\begin{bmatrix}2\\-3\end{bmatrix}$. We get $A\vec{x}=\begin{bmatrix}4(2)+5(-3)\\-2(2)+3(-3)\\1(2)+(-1)(-3)\end{bmatrix} = \begin{bmatrix}-7\\-13\\5\end{bmatrix}$ 
\end{itemize}
Let $\vec{x},\vec{y}\in\mathbb{R}^n$. Notice that $\vec{y}^T$ is a $1\times n$ matrix. Hence, $\vec{y}^T\vec{x}=\vec{y}\cdot\vec{x}$.  

\paragraph{Another Definition of Matrix-Vector Multiplication} Let $A$ be an $m\times n$ matrix whose columns are denoted $\vec{a}_i$ for $1\leq i\leq n$. Then, for any $\vec{x}=\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\in\mathbb{R}^n$, we define $A\vec{x}=x_1\vec{a}_1+\ldots+x_n\vec{a}_n$. \\ 
Example: Let $A = \begin{bmatrix}4&5\\-2&3\\1&-1\end{bmatrix}$. Calculate $A\vec{x}$ where \begin{itemize}
    \item $\vec{x}=\begin{bmatrix}1\\0\end{bmatrix}$. We get $A\vec{x}=1\begin{bmatrix}4\\-2\\1\end{bmatrix}+0\begin{bmatrix}5\\3\\-1\end{bmatrix} = \begin{bmatrix}4\\-2\\1\end{bmatrix}$ 
    \item $\vec{x}=\begin{bmatrix}0\\1\end{bmatrix}$. We get $A\vec{x}=0\begin{bmatrix}4\\-2\\1\end{bmatrix}+1\begin{bmatrix}5\\3\\-1\end{bmatrix} = \begin{bmatrix}5\\3\\-1\end{bmatrix}$ 
    \item $\vec{x}=\begin{bmatrix}2\\-3\end{bmatrix}$. We get $A\vec{x}=2\begin{bmatrix}4\\-2\\1\end{bmatrix}-3\begin{bmatrix}5\\3\\-1\end{bmatrix} = \begin{bmatrix}-7\\-13\\5\end{bmatrix}$ 
\end{itemize} 
Observe that if $\vec{e}_i$ is the $i^{th}$ standard basis vector of $\mathbb{R}^n$ and $A=[\vec{a}_1\ldots\vec{a}_n]$, then $A\vec{e}_i=0\vec{a}_1+\ldots+0\vec{a}_{i-1}+1\vec{a}_i+0\vec{a}_{i+1}+\ldots+0\vec{a}_n=\vec{a}_i$ 

\paragraph{Matrix Multiplication} For an $m\times n$ matrix $A$, and an $n\times p$ matrix $B=[\vec{b}_1\ldots\vec{b}_p]$, we define $AB$ to be the $m\times p$ matrix $$AB=A[\vec{b}_1\ldots\vec{b}_p]=[A\vec{b}_1\ldots A\vec{b}_p]$$ Notice that if $\vec{a}_i^T$ are the rows of $A$, then the $ij^{th}$ entry of $AB$ is the $i^{th}$ entry of $A\vec{b}_j$. That is, $$(AB)_{ij} = \vec{a}_j\cdot\vec{b}_j=\vec{a}_i^T\vec{b}_j=a_{i1}b_{1j}+a_{i2}b_{2j}+\ldots+a_{in}b_{nj}=\sum_{k=1}^n(A)_{ik}(B)_{kj}$$ Observe that the number of columns of $A$ must equal the number of rows of $B$ for this to be defined. \\ 
Example: Find $AB$ where $A=\begin{bmatrix}1&-3\\2&0\\-1&4\end{bmatrix}$ and $B=\begin{bmatrix}0&1\\5&-2\end{bmatrix}$\\ 
$$AB = \begin{bmatrix}1(0)+(-3)(5)&1(1)+(-3)(-2)\\2(0)+0(5)&2(1)+0(-2) \\ (-1)(0)+4(5)& (-1)(1)+4(-2)\end{bmatrix}= \begin{bmatrix}-15&7\\0&2\\20&-9\end{bmatrix}$$\\ 
Here are some properties (theorem 3.1.3): If $A,B,C$ are matrices of the correct size, and $t\in\mathbb{R}$, then \begin{enumerate}
    \item $A(B+C) = AB+AC$ 
    \item $t(AB)=(tA)B=A(tB)$ 
    \item $A(BC)=(AB)C$ 
    \item $(AB)^T = B^TA^T$
\end{enumerate}
Note: $AB\neq BA$, and if $A=B$, then we can multiply by $D$ to either have $DA=DB$ or $AD=BD$. Similarly, if $AC=BC$, $A\neq B$.  \\ 
\textbf{Theorem 3.1.4} If $A$ and $B$ are $m\times n$ matrices such that $A\vec{x}=B\vec{x}$ for every $\vec{x}\in\mathbb{R}^n$, then $A=B$. \\ 
Proof: We are required to prove $A=B$, ie. $a_{ij}=b_{ij}$ for all $i,j$. Let $A=[\vec{a}_1\ldots\vec{a}_n]$, and $B=[\vec{b}_1\ldots\vec{b}_n]$. Let $\vec{e}_i$ denote the $i^{th}$ standard basis vector. Then, since $A\vec{x}=B\vec{x}$ for all $\vec{x}$, we have that $\vec{a}_i=A\vec{e}_i=B\vec{e}_i=\vec{b}_i$. Hence $A=B$. 

\paragraph{The Identity Matrix} Theorem 3.1.5: If $I=[\vec{e}_1\ldots\vec{e}_n]$, then $AI=A=IA$ for any $n\times n$ matrix $A$. Note: this identity is unique. Definition: The $n\times n$ matrix $I$ such that $(I)_{ii}=1$ for $1\leq i\leq n$, and $(I)_{ij}=0$ whenever $i\neq j$ is called the \textbf{identity matrix}. Sometimes denoted as $I_n$ to stress the size of the matrix. 

\paragraph{Block Matrices} If $A$ is an $m\times n$ matrix, then we can write $A$ as the $k\times l$ \textbf{block matrix} $A = \begin{bmatrix}A_{11}&\ldots&A_{1l}\\\vdots&\ddots&\vdots\\A_{k1}&\ldots&A_{kl}\end{bmatrix}$ where $A_{ij}$ is a block such that all blocks in the $i^{th}$ row have the same number of rows and all blocks in the $j^{th}$ column have the same number of columns. \\ 
Example: Let $A = \begin{bmatrix}1&-1&3&4\\0&0&0&0\\0&3&1&2\end{bmatrix}$. We can write $A$ as $A = \begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}$, where $A_{11}=\begin{bmatrix}1&-1\end{bmatrix}, A_{12}=\begin{bmatrix}3&4\end{bmatrix},A_{21}=\begin{bmatrix}0&0\\0&3\end{bmatrix},A_{22}=\begin{bmatrix}0&0\\1&2\end{bmatrix}$\\ 
Note we actually defined matrix-matrix multiplication in terms of blocks. We wrote the second matrix $B$ as block matrix $B=[B_1\ldots B_n]$ where each block is a column of $B$ and we wrote the first matrix $A$ as a single block. We then got $AB=[AB_1\ldots AB_n]$. \\ 
Example: If $A = \begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}, B=\begin{bmatrix}B_{11}&B_{12}&B_{13}\\B_{21}&B_{22}&B_{23}\end{bmatrix}$, then $$AB=\begin{bmatrix}A_{11}B_{11}+A_{12}B_{21}&A_{11}B_{12}+A_{12}B_{22}&A_{11}B_{13}+A_{12}B_{23} \\ A_{21}B_{11}+A_{22}B_{21}&A_{21}B_{12}+A_{22}B_{22}&A_{21}B_{13}+A_{22}B_{23}\end{bmatrix}$$

\paragraph{Functions} Recall that a \textbf{function} is a rule associates each element $a\in A$ with a unique element $f(a)\in B$. We write $f:A\rightarrow B$ and we call $A$ the \textbf{domain} and $B$ the \textbf{codomain}. Thevalue of $f(a)$ is called the \textbf{image of $a$ under $f$}. 

\paragraph{Matrix Mappings} For any $A\in M_{m\times n}(\mathbb{R})$ a function $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ defined by $f(\vec{x})=A\vec{x}$ is called a \textbf{matrix mapping}. Note: If $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$, then we should write $f\left(\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\right)=\begin{bmatrix}y_1\\\vdots\\y_m\end{bmatrix}$. However, we will often write $f(x_1,\ldots,x_n)=(y_1,\ldots,y_m)$ instead. \\ 
Example: Let $A=\begin{bmatrix}2&3\\-4&0\\5&1\end{bmatrix}$ and define $f(\vec{x})=A\vec{x}$.\begin{itemize}
    \item What is the domain and codomain of $f$?  \\ 
For $A\vec{x}$ to be defined, $\vec{x}$ must have 2 rows since $A$ has 2 columns. Hence, the domain of $f$ is $\mathbb{R}^2$. Also, since $A$ has 3 rows, the product $A\vec{x}$ will have 3 rows. Hence, the codomain of $f$ is $\mathbb{R}^3$. We write $f:\mathbb{R}^2\rightarrow\mathbb{R}^3$. 
\item Calculate $f(\vec{v})$ where $\vec{v}=\begin{bmatrix}2\\-1\end{bmatrix}$ \\ 
$f(\vec{v})=A\vec{v}=\begin{bmatrix}2&3\\-4&0\\5&1\end{bmatrix}\begin{bmatrix}2\\-1\end{bmatrix} = \begin{bmatrix}1\\-8\\9\end{bmatrix}$. We may write $f(2,-1)=(1,-8,9)$. 
    \item Define $f(\vec{x})=A\vec{x}$. Determine $f(\vec{x})$ for any $\vec{x}\in\mathbb{R}^2$. We have $f(\vec{x})=A\vec{x}=\begin{bmatrix}2&3\\-4&0\\5&1\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix} = \begin{bmatrix}2x_1+3x_2\\-4x_1\\5x_1+x_2\end{bmatrix}$
\end{itemize} 
Note that $A=[f(1,0)\quad f(0,1)]$ ($f(1,0)$ produces the first column, and $f(0,1)$ produces the second). Furthermore, $f(x_1,x_2)=[f(1,0)\quad f(0,1)]\begin{bmatrix}x_1\\x_2\end{bmatrix} = x_1g(1,0) +x_2g(0,1)4. $\\ 
\textbf{Theorem 3.2.1} Let $A$ be an $m\times n$ matrix and let $f(\vec{x})=A\vec{x}$. Then, for any vectors $\vec{x},\vec{y}\in\mathbb{R}^n$, and $s,t\in\mathbb{R}$ we have $f(s\vec{x}+t\vec{y}) = sf(\vec{x})+tf(\vec{y})$. This is called the \textbf{linearity property}. 

\paragraph{Linear Mapping} A function $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is called a \textbf{linear mapping} if it has the property that $L(s\vec{x}+t\vec{y})=sL(\vec{x})+tL(\vec{y})$ for every $\vec{x},\vec{y}\in\mathbb{R}^n$ and $s,t\in\mathbb{R}$. Notes: \begin{enumerate}
    \item Linear transformation and linear mapping are exactly the same thing 
    \item A linear mapping $L:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is sometimes called a linear operator. 
    \item If $L$ is a linear mapping, then $L(t_1\vec{v}_1+\ldots+t_k\vec{v}_k)=t_1L(\vec{v}_1)+\ldots+t_kL(\vec{v}_k)$. 
\end{enumerate}
Examples: Show that the mapping $f:\mathbb{R}^2\rightarrow\mathbb{R}^2$ defined by $f(x_1,x_2)=(2x_1+x_2,-3x_1+5x_2)$ is linear. \\ 
To prove it is linear we must show that it satisfies the linearity property. Let $\vec{x}=\begin{bmatrix}x_1\\x_2\end{bmatrix}, \vec{y}=\begin{bmatrix}y_1\\y_2\end{bmatrix}$. Then \begin{align*}f(s\vec{x}+t\vec{y})&=f(s(x_1,x_2)+t(y_1,y_2)) \\ &= f(sx_1+ty_1,sx_2+ty_2)\\ &= (2(sx_1+ty_1)+(sx_2+ty_2), -3(sx_1+ty_1)+5(sx_2+ty_2)) \\&= s(2x_1+x_2,-3x_1+5x_2)+t(2y_1+y_2,-3y_1+5y_2) \\&= sf(x_1,x_2)+tf(y_1,y_2)\\&=sf(\vec{x})+tf(\vec{y})\end{align*}. Hence, $f$ is linear. 

\paragraph{Matrices and Linear Mappings} (theorem 3.2.2) If $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is a linear mapping, then $L$ can be represented as a matrix mapping with the corresponding $m$ by $n$ matrix $[L]$ given by $[L]=[L(\vec{e}_1)\ldots L(\vec{e}_n)]$.  \\ 
Proof: Let $\vec{x}\in\mathbb{R}^n$. Then $\vec{x}=x_1\vec{e}_1+\ldots+x_n\vec{e}_n$. Hence, \begin{align*} L(\vec{x})&=L(x_1\vec{e}_1+\ldots+x_n\vec{e}_n)\\ &= x_1L(\vec{e}_1)+\ldots+x_nL(\vec{e}_n)\\ &=[L(\vec{e}_1)\ldots L(\vec{e}_n)]\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}\\&=[L]\vec{x}\end{align*} \\ 
Definition: Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear mapping. The matrix $[L] = [L(\vec{e}_1)\quad L(\vec{e}_2)\ldots L(\vec{e}_n)]$ is called the \textbf{standard matrix of $L$} and has the property that $L(\vec{x})=[L]\vec{x}$. \\ 
Example: Let $\vec{a}=\begin{bmatrix}3\\4\end{bmatrix}$. Find the standard matrix of proj$_{\vec{a}}:\mathbb{R}^2\rightarrow\mathbb{R}^2$ and use it to find proj$_{\vec{a}}\begin{bmatrix}1\\2\end{bmatrix}$ \\ 
By theorem 3.2.2, the first column of $[proj_{\vec{a}}]$ is proj$_{\vec{a}}(\vec{e}_1)$. $$proj_{\vec{a}}(\vec{e}_1)=\frac{\vec{e}_1\cdot\vec{a}}{||\vec{a}||^2}\vec{a} = \frac{3}{25}\begin{bmatrix}3\\4\end{bmatrix} = \begin{bmatrix}9/25\\12/25\end{bmatrix}$$ The second column of $[proj_{\vec{a}}]$ is proj$_{\vec{a}}(\vec{e}_1)$ $$proj_{\vec{a}}(\vec{e}_2)=\frac{\vec{e}_1\cdot\vec{a}}{||\vec{a}||^2}\vec{a} = \frac{4}{25}\begin{bmatrix}3\\4\end{bmatrix} = \begin{bmatrix}12/25\\16/25\end{bmatrix}$$ Hence the standard matrix of the linear mapping is $$[proj_{\vec{a}}] = [proj_{\vec{a}}(\vec{e}_1)\quad proj_{\vec{a}}(\vec{e}_2)] = \begin{bmatrix}9/25&12/25\\12/25&16/25\end{bmatrix}$$ By definition of the standard matrix, we get that $$proj_{\vec{a}}\begin{bmatrix}1\\2\end{bmatrix}=[proj_{\vec{a}}]\begin{bmatrix}1\\2\end{bmatrix} = \begin{bmatrix}9/25&12/25\\12/25&16/25\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix} = \begin{bmatrix}33/25\\44/25\end{bmatrix}$$

\paragraph{Rotations} $R_\theta:\mathbb{R}^2\rightarrow\mathbb{R}^2$ is defined to be the mapping that rotates the vector $\vec{x}$ by an angle $\theta$. Using trigonometry we can find that $R_\theta(x_1,x_2)=(x_1\cos\theta-x_2\sin\theta, x_1\sin\theta+x_2\cos\theta)$ The standard matrix of $R_\theta$ is $[R_\theta]=[R_\theta(\vec{e}_1)\quad R_\theta(\vec{e}_2)] = \begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}$ This matrix is called a \textbf{rotation matrix}. \\ 
Example: Determine $R_{\pi/3}\left(\begin{bmatrix}2\\1\end{bmatrix}\right)$ \\ 
Using the rotation matrix, we get $R_{\pi/3}\left(\begin{bmatrix}2\\1\end{bmatrix}\right)=\begin{bmatrix}\cos\pi/3&-\sin\pi/3\\\sin\pi/3&\cos\pi/3\end{bmatrix}\begin{bmatrix}2\\1\end{bmatrix} = \begin{bmatrix}1-\sqrt{3}/2\\\sqrt{3}+1/2\end{bmatrix}$

\paragraph{Reflections} Let refl$_P:\mathbb{R}^n\rightarrow\mathbb{R}^n$ denote the function which maps a vector $\vec{x}$ to its mirror image in a hyperplane $P$. We define refl$_P(\vec{x})=\vec{x}-2\text{perp}_P(\vec{x})=\vec{x}-2\text{proj}_{\vec{n}}(\vec{x})$. \\ 
Example: Let $\vec{x}=\begin{bmatrix}1\\2\\3\end{bmatrix}$ Find the reflection of $\vec{x}$ over the plane $P$ with scalar equation $x_1-2x_2+2x_3=0$. Then find the standard matrix of the reflection and use the standard matrix to verify your answer. \\ 
First, we have that the normal vector for $P$ is $\vec{n}=\begin{bmatrix}1\\-2\\2\end{bmatrix}$. Hence, 
$$refl_P(\vec{x})=\vec{x}-2proj_{\vec{n}}(\vec{x})=\vec{x}-2\frac{\vec{x}\cdot\vec{n}}{||\vec{n}||^2}\vec{n}=\begin{bmatrix}1\\2\\3\end{bmatrix}-2\left(\frac{3}{9}\begin{bmatrix}1\\-2\\2\end{bmatrix}\right)=\begin{bmatrix}1/3\\10/3\\5/3\end{bmatrix}$$ 
$$refl_P(\vec{e}_1)=\vec{e}_1-2\frac{\vec{e}_1\cdot\vec{n}}{||\vec{n}||^2}\vec{n}=\begin{bmatrix}1\\0\\0\end{bmatrix}-2\left(\frac{1}{9}\begin{bmatrix}1\\-2\\2\end{bmatrix}\right)=\begin{bmatrix}7/9\\4/9\\-4/9\end{bmatrix}$$
$$refl_P(\vec{e}_2)=\vec{e}_2-2\frac{\vec{e}_1\cdot\vec{n}}{||\vec{n}||^2}\vec{n}=\begin{bmatrix}0\\1\\0\end{bmatrix}-2\left(\frac{-2}{9}\begin{bmatrix}1\\-2\\2\end{bmatrix}\right)=\begin{bmatrix}4/9\\1/9\\8/9\end{bmatrix}$$
$$refl_P(\vec{e}_e)=\vec{e}_3-2\frac{\vec{e}_1\cdot\vec{n}}{||\vec{n}||^2}\vec{n}=\begin{bmatrix}0\\0\\1\end{bmatrix}-2\left(\frac{2}{9}\begin{bmatrix}1\\-2\\2\end{bmatrix}\right)=\begin{bmatrix}-4/9\\8/9\\1/9\end{bmatrix}$$
Hence the standard matrix of the reflection is $$[\text{refl}_P]=[\text{refl}_P(\vec{e}_1)\quad \text{refl}_P(\vec{e}_2)\quad \text{refl}_P(\vec{e}_3)]\begin{bmatrix}7/9&4/9&-4/9\\4/9&1/9&8/9\\-4/9&8/9&1/9\end{bmatrix}$$ 
Therefore, refl$_P(\vec{x})=\begin{bmatrix}7/9&4/9&-4/9\\4/9&1/9&8/9\\-4/9&8/9&1/9\end{bmatrix}\begin{bmatrix}1\\2\\3\end{bmatrix}=\begin{bmatrix}1/3\\10/3\\5/3\end{bmatrix}$

\paragraph{Range} Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$. We define the \textbf{range} of $L$ to be the set of all images that $L$ produces for $\vec{x}\in\mathbb{R}^n$. That is, $\text{Range}(L)=\{L(\vec{x})|\vec{x}\in\mathbb{R}^n\}$. \\ 
Example: Determine which of the following vectors is in the range of the linear mapping $L:\mathbb{R}^2\rightarrow\mathbb{R}^3$ defined by $L(x_1,x_2)=(3x_1+x_2,2x_1-x_2,2x_2)$ \begin{itemize}
    \item $\vec{y}_1=\begin{bmatrix}6\\9\\-6\end{bmatrix}$\\ By definition, $\vec{y}_1\in\text{Range}(L)$ if there exists a vector $\vec{x}\in\mathbb{R}^2$ such that $L(\vec{x})=\vec{y}_1$. Hence, we need to determine if there exists $x_1$ and $x_2$ such that $\begin{bmatrix}6\\9\\-6\end{bmatrix}=L(x_1,x_2)=\begin{bmatrix}3x_1+x_2\\2x_1-x_2\\2x_2\end{bmatrix}$ Comparing entries we get $$3x_1+x_2=6$$ $$2x_1-x_2=9$$ $$2x_2=-6$$ Solving, we find that $x_2=-3$ and hence $x_1=3$. Therefore, $L(3,-3)=\vec{y}_1$, and so $\vec{y}_1$ is in Range$(L)$. 
    \item $\vec{y}_2=\begin{bmatrix}4\\1\\1\end{bmatrix}$ \\ We need to find $\begin{bmatrix}4\\1\\1\end{bmatrix}=L(x_1,x_2)=\begin{bmatrix}3x_1+x_2\\2x_1-x_2\\2x_2\end{bmatrix}$. Comparing entries we get $$3x_1+2x_2=4$$ $$2x_1-x_2=1$$ $$2x_2=1$$ We find that the system is inconsistent and hence $\vec{y}_2$ is not in the range of $L$.  
    \item $\vec{0}$ We observe that $L(0,0)=\vec{0}$, and hence $\vec{0}$ is in the range of $L$. 
\end{itemize}
Another example: Find a basis for the range of $L(x_1,x_2,x_3)=(x_1+x_2,0,x_3)$ \\ 
Every vector $\vec{y}\in\text{Range}(L)$ has the form $L(\vec{x})=\begin{bmatrix}x_1+x_2\\0\\x_3\end{bmatrix}=(x_1+x_2)\begin{bmatrix}1\\0\\0\end{bmatrix}+x_3\begin{bmatrix}0\\0\\1\end{bmatrix}$ Hence $\mathfrak{B}=\left\{\begin{bmatrix}1\\0\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\end{bmatrix}\right\}$ spans Range$(L)$ and is clearly linearly independent. Thus, it is a basis for Range$(L)$. \\ 
Another example: Find a basis for the range of $f(\vec{x})=A\vec{x}$ where $A=\begin{bmatrix}1&2\\2&4\end{bmatrix}$.\\ 
Every $\vec{y}\in Range(f)$ has the form $$\vec{y}=f(\vec{x})=\begin{bmatrix}1&2\\2&4\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}x_1+2x_2\\2x_1+4x_2\end{bmatrix}=x_1\begin{bmatrix}1\\2\end{bmatrix}+x_2\begin{bmatrix}2\\4\end{bmatrix}$$ Hence Range$(f)=Span\left\{\begin{bmatrix}1\\2\end{bmatrix},\begin{bmatrix}2\\4\end{bmatrix}\right\}=Span\left\{\begin{bmatrix}1\\2\end{bmatrix}\right\}$ Since $\left\{\begin{bmatrix}1\\2\end{bmatrix}\right\}$ is linearly independent, it is a basis for Range$(f)$. \\ 
\textbf{Lemma 3.3.1} If $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is linear, then $L(\vec{0})=\vec{0}$ \\ 
\textbf{Theorem 3.3.2} If $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is a linear mapping, then Range$(L)$ is a subspace of the codomain, $\mathbb{R}^m$. Proof is using the subspace test. 

\paragraph{Kernel} Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear mapping. The \textbf{kernel} (sometimes called the \textbf{null space}) is the set of all vectors in the domain that have image $\vec{0}$ under $L$. In set notation, $ker(L)=\{\vec{x}\in\mathbb{R}^n|L(\vec{x})=\vec{0}\}$. \\ 
Examples: Determine which of the following vectors are in the kernel of $L(x_1,x_2,x_3)=(x_1+x_2,0,x_3)$ \begin{itemize}
    \item $\vec{0}$. \\ Observe that $L(0,0,0) = (0,0,0)$, so $\vec{0}\in ker(L)$. 
    \item $\vec{x}_1=\begin{bmatrix}1\\2\\1\end{bmatrix}$ \\ We have $L(1,2,1)=(3,0,1)$, so $\vec{x}_1$ is not in the kernel of $L$. 
    \item $\vec{x}_2=\begin{bmatrix}5\\-5\\0\end{bmatrix}$ \\ We have $L(5,-5,0) = (0,0,0)$, so $\vec{x}_2$ is in the kernel of $L$.
\end{itemize}
\textbf{Theorem 3.3.3} If $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is linear, then ker$(L)$ is a subspace of $\mathbb{R}^n$. \\ 
Example: Find a basis for the kernel of $L(x_1,x_2,x_3)=(x_1+x_2,0,x_3)$ \\ 
Every vector $\vec{x}\in ker(L)$ satisfies $$\begin{bmatrix}0\\0\\0\end{bmatrix}=f(\vec{x})=\begin{bmatrix}1&3\\-1&5\\3&2\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}x_1+3x_2\\-x_1+5x_2\\3x_1+2x_2\end{bmatrix}$$ Comparing entries we get the homogeneous system $$x_1+3x_2=0$$ $$-x_1+5x_2=0$$ $$3x_1+2x_2=0$$ Solving, we find it has a unique solution $x_1=x_2=0$, therefore, ker$(L)=\{\vec{0}\}$ and so by definition, a basis for ker$(L)$ is the empty set. 

\paragraph{Nullspace} \textbf{Theorem 3.3.4} Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a linear mapping with standard matrix $A=[L]$. Then $\vec{x}\in ker(L)$ if and only if $A\vec{x}=\vec{0}$. This motivates the definition of a null space: Let $A$ be an $m\times n$ matrix. The set of all $\vec{x}\in\mathbb{R}^n$ such that $A\vec{x}=\vec{0}$ is called the \textbf{nullspace} of A and is denoted Null$(A)=\{\vec{x}\in\mathbb{R}^n|A\vec{x}=\vec{0}\}$. \\ 
Observe that we have called the nullspace of a matrix a 'space' since theorem 3.3.4 proves that the nullspace of any matrix $A$ is the kernel of the matrix mapping $L(\vec{x})=A\vec{x}$ and hence the nullspace is a subspace of $\mathbb{R}^n$ by theorem 3.3.3. Moreover, we have that if $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is linear, then ker$(L)=Null([L])$. \\ 
Example: Determine whether $\vec{x}=\begin{bmatrix}-4\\5\\2\end{bmatrix}$ is in the nullspace of $A = \begin{bmatrix}1&1&-1\\2&1&3\\0&1&-5\end{bmatrix}$ \\ 
We have $A\vec{x}=\begin{bmatrix}1&1&-1\\2&1&3\\0&1&-5\end{bmatrix}\begin{bmatrix}-4\\5\\2\end{bmatrix}=\begin{bmatrix}-1\\3\\-5\end{bmatrix}$ Hence $A\vec{x}\neq\vec{0}$, so $\vec{x}\not\in Null(A)$

\paragraph{Column Space} Theorem 3.3.5 If $[L]=[\vec{v}_1\ldots\vec{v}_n]$ is the standard matrix of a linear mapping $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$, then $Range(L)=Span\{\vec{v}_1,\ldots,\vec{v}_n\}$. Let $A=[\vec{a}_1\ldots\vec{a}_n]$ be an $m\times n$ matrix. The \textbf{column space} of A is $Col(A)=Span\{\vec{a}_1,\ldots,\vec{a}_n\}=\{A\vec{x}|\vec{x}\in\mathbb{R}^n\}$. Theorem 3.3.5 shows that if $L$ is a linear mapping, then $Range(L)=Col([L])$. \\ 
Example: Determine whether $\begin{bmatrix}4\\0\\8\end{bmatrix}$ is in the column space of $A = \begin{bmatrix}1&1&-1\\2&1&3\\-1&3&-5\end{bmatrix}$ \\ 
We need to determine if $\begin{bmatrix}4\\0\\8\end{bmatrix}$ is a linear combination of the columns of $A$. That is, we need to determine if we can find $c_1,c_2,c_3$ such that $$\begin{bmatrix}4\\0\\8\end{bmatrix}=c_1\begin{bmatrix}1\\2\\0\end{bmatrix}+c_2\begin{bmatrix}1\\1\\1\end{bmatrix}+c_3\begin{bmatrix}-1\\3\\5\end{bmatrix}= \begin{bmatrix}c_1+c_2-c_3\\2c_1+c_2+3c_3\\c_2-5c_3\end{bmatrix}$$. Comparing entries, we get $$c_1+c_2-c_3=4$$ $$2c_1+c_2+3c_3=0$$ $$c_2-5c_3=8$$ Row reducing the augmented matrix we get $$c_1+4c_3=-4$$ $$c_2-5c_3=8$$ Since this has a solution, it is in the column space of $A$. \\ 
Example: Let $A=\begin{bmatrix}1&2&1\\1&3&0\\0&1&2\end{bmatrix}$ Find a basis for the column space and nullspace of $A$. \\ 
By definition, Col$(A)$ is spanned by $\left\{\begin{bmatrix}1\\1\\0\end{bmatrix},\begin{bmatrix}2\\3\\1\end{bmatrix},\begin{bmatrix}1\\0\\2\end{bmatrix}\right\}$. Consider $$\begin{bmatrix}0\\0\\0\end{bmatrix}=c_1\begin{bmatrix}1\\1\\0\end{bmatrix}+c_2\begin{bmatrix}2\\3\\1\end{bmatrix}+c_3\begin{bmatrix}1\\0\\2\end{bmatrix}=\begin{bmatrix}c_1+2c_2+c_3\\c_1+3c_2\\c_2+2c_3\end{bmatrix}$$ Row reducing the coefficient matrix gives $\begin{bmatrix}1&2&1\\1&3&0\\0&1&2\end{bmatrix}\sim\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$ Hence the system has the unique solution $c_1=c_2=c_3=0$. Therefore, $\left\{\begin{bmatrix}1\\1\\0\end{bmatrix},\begin{bmatrix}2\\3\\1\end{bmatrix},\begin{bmatrix}1\\0\\2\end{bmatrix}\right\}$ is also linearly independent and hence a basis for Col$(A)$. \\ 
We now want to find a basis for the nullspace of $A$. For every vector $\vec{x}\in Null(A)$ satisfies $$\begin{bmatrix}0\\0\\0\end{bmatrix}=A\vec{x}=x_1\begin{bmatrix}1\\1\\0\end{bmatrix}+x_2\begin{bmatrix}2\\3\\1\end{bmatrix}+x_3\begin{bmatrix}1\\0\\2\end{bmatrix}=\begin{bmatrix}x_1+2x_2+x_3\\x_1+3x_2\\x_2+2x_3\end{bmatrix}$$ Notice that this is the same system as before, and hence the only solution is $x_1=x_2=x_3=0$, and therefore the only vector $\vec{x}\in Null(A)$ is $\vec{x}=\vec{0}$. Thus, we say a basis for Null$(A)$ is the empty set. \\ 
Example: Let $B=\begin{bmatrix}1&2&3\\2&-1&1\end{bmatrix}$ Find a basis for Col$(B)$ and Null$(B)$ \\ 
By definition, $\left\{\begin{bmatrix}1\\2\end{bmatrix},\begin{bmatrix}2\\-1\end{bmatrix},\begin{bmatrix}3\\1\end{bmatrix}\right\}$ spans Col$(B)$. However, we observe that $\begin{bmatrix}1\\2\end{bmatrix}+\begin{bmatrix}2\\-1\end{bmatrix}=\begin{bmatrix}3\\1\end{bmatrix}$ Hence, Col$(B)=Span\left\{\begin{bmatrix}1\\2\end{bmatrix},\begin{bmatrix}2\\-1\end{bmatrix}\right\}$ and $\left\{\begin{bmatrix}1\\2\end{bmatrix},\begin{bmatrix}2\\-1\end{bmatrix}\right\}$ is clearly linearly independent, so it is a basis for Col$(B)$. \\ 
Every vector $\vec{x}\in Null(B)$ satisfies $\begin{bmatrix}0\\0\end{bmatrix}=B\vec{x}=x_1\begin{bmatrix}1\\2\end{bmatrix}+x_2\begin{bmatrix}2\\-1\end{bmatrix}+x_3\begin{bmatrix}3\\1\end{bmatrix}=\begin{bmatrix}x_1+2x_2+3x_3\\2x_1-x_2+x_3\end{bmatrix}$. Row reducing the coefficient matrix gives $$\begin{bmatrix}1&2&3\\2&-1&1\end{bmatrix}\sim\begin{bmatrix}1&0&1\\0&1&1\end{bmatrix}$$
Thus $x_1=-x_3$ and $x_2=-x_3$, so $\vec{x}$ has the form $\vec{x}=\begin{bmatrix}-x_3\\-x_3\\x_3\end{bmatrix}=x_3\begin{bmatrix}-1\\-1\\1\end{bmatrix}$. Therefore, $\left\{\begin{bmatrix}-1\\-1\\1\end{bmatrix}\right\}$ spans Null$(B)$ and is clearly linearly independent, so it is a basis for Null$(B)$. 

\paragraph{Row Space} Recall that if $A$ is an $m\times n$ matrix, then $A\vec{x}$ gives a linear combination of the columns of $A$. Remember that the transpose turns the rows of $A$ into columns. Hence, $A^T\vec{x}$ gives a linear combination of the columns of $A^T$ which is a linear combination of the rows of $A$. \\ 
\textbf{Definition} Let $A$ be an $m\times n$ matrix with rows $\vec{a}_i^T$ for $1\leq i\leq m$. The span of the rows of $A$ is called the \textbf{row space} of $A$ and is denoted $Row(A)=Span\{\vec{a}_1,\ldots,\vec{a}_m\}=\{A^T\vec{x}|\vec{x}\in\mathbb{R}^m\}$ \\ 
Observe that the row space of $A$ equals the column space of $A^T$. Thus to complete the set, we also look at the nullspace of $A^T$. \\ 
\textbf{Definition} Let $A$ be an $m\times n$ matrix. The nullspace of $A^T$ is called the $\textbf{left nullspace}$ of $A$. $Null(A^T)=\{\vec{x}\in\mathbb{R}^m|A^T\vec{a}=\vec{0}\}$. To find a basis for the row space and left nullspace for a matrix $A$, we just find a basis for the column space and the nullspace for $A^T$. 

\paragraph{Operations on Linear Mappings} Say $L$ and $M$ are both linear mappings from $\mathbb{R}^n\rightarrow\mathbb{R}^m$. They are \textbf{equal} ($L=M$) if $L(\vec{x})=M(\vec{x})$ for all $\vec{x}\in\mathbb{R}^n$. Also, the mapping $L+M:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is defined by $(L+M)(\vec{x})=L(\vec{x})+M(\vec{x})$. The mapping $tL:\mathbb{R}^n\rightarrow\mathbb{R}^m$ is defined by $(tL)(\vec{x})=tL(\vec{x})$. \\ 
Examples: Let $L:\mathbb{R}^2\rightarrow\mathbb{R}^3$ be the linear mapping defined by $L(x_1,x_2)=(2x_1+x_2,3x_1-x_2,x_2)$. Let $M:\mathbb{R}^2\rightarrow\mathbb{R}^3$ be defined by $M(x_1,x_2)=(-2x_1-x_2,4x_1+2x_2,-3x_1-x_2)$. Then $L+M$ is $$(L+M)(x_1,x_2)=L(x_1,x_2)+M(x_1,x_2)=\begin{bmatrix}2x_1+x_2\\3x_1-x_3\\x_2\end{bmatrix}+\begin{bmatrix}-2x_1-x_2\\4x_!+2x_1\\-3x_1-x_2\end{bmatrix}=\begin{bmatrix}0\\7x_1+x_2\\-3x_1\end{bmatrix}$$ The mapping $5L$ is defined by $$\begin{bmatrix}10x_1+5x_2\\15x_1-5x_2\\5x_2\end{bmatrix}$$ \\ 
\textbf{Theorem 3.4.1} If $L$ and $M$ are both linear mappings, then $L+M$ and $tL$ are both linear mappings. Moreover, $[L+M]=[L]+[M]$, and $[tL]=t[L]$. \\ 
Here are some properties of the set $\mathbb{L}$ of all possible linear mappings (Theorem 3.4.2): \begin{enumerate}
    \item $L+M\in\mathbb{L}$ 
    \item $(L+M)+N=L+(M+N)$ 
    \item $L+M=M+L$ 
    \item There exists a linear mapping $0$ such that $L+0=L$ for all $L\in\mathbb{L}$. In particular, $0$ is the linear mapping defined by $0(\vec{x}=\vec{0})$. We call this the \textbf{zero mapping} 
    \item There exists an additive inverse $(-L)$ such that $L+(-L)=0$. In particular, it is defined as $(-L)(\vec{x})=-L(\vec{x})$ 
    \item $cL\in\mathbb{L}$ 
    \item $c(dL)=(cd)L$ 
    \item $(c+d)L=cL+dL$ 
    \item $c(L+M) = cL+cM$ 
    \item $1L=L$
\end{enumerate}

\paragraph{Composition of Mapping} If $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ and $M:\mathbb{R}^m\rightarrow\mathbb{R}^p$, then we define the composition $M\circ L:\mathbb{R}^n\rightarrow\mathbb{R}^p$ by $(M\circ L)(\vec{x})=M(L(\vec{x}))$. \\ 
Example: Let $L:\mathbb{R}^2\rightarrow\mathbb{R}^3$ be defined by $L(x_1,x_2)=(x_1+x_2,x_2,x_1)$ and let $M:\mathbb{R}^3\rightarrow\mathbb{R}$ be defined by $M(y_1,y_2,y_3)=y_1+y_2+y_3$. Find $(M\circ L)(1,2)$ and $(M\circ L)(x_1,x_2)$. \\ 
$(M\circ L)(1,2) = M(L(1,2)) = M(3,2,1) = 3+2+1=6$ \\ 
$(M\circ L)(x_1,x_2)=M(L(x_1,x_2))=M(x_1+x_2,x_2,x_1)=(x_1+x_2)+x_2+x_1=2x_1+2x_2$\\ 
\textbf{Theorem 3.4.3} If $M:\mathbb{R}^m\rightarrow\mathbb{R}^p$ and $L:\mathbb{R}^n\rightarrow\mathbb{R}^m$ are both linear mappings then $M\circ L$ is also a linear mapping. Moreover, $[M\circ L]=[M][L]$. \\ 
Example: Find the standard matrix of $R_{\pi/2}\circ R_{3\pi/2}$ \\ 
Recall that the standard matrix of $R_\theta = [R_\theta]=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}$. Using this and theorem 3.4.3 we get $$[R_{\pi/2}\circ R_{3\pi/2}]=[R_{\pi/2}][R_{3\pi/2}]=\begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}0&1\\-1&0\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}$$ Notice that this is the identity mapping, which makes sense since when you rotate by $3\pi/2$ and then by $\pi/2$ you rotate it a full rotation. \\ 
\textbf{Definition} The linear mapping $I:\mathbb{R}^n\rightarrow\mathbb{R}^n$ such that $I(\vec{x})=\vec{x}$ for all $\vec{x}$ is called the \textbf{identity mapping}. \\ 
\textbf{Definition} Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^n$ and $M:\mathbb{R}^n\rightarrow\mathbb{R}^n$ be linear mappings. If $(L\circ M)(\vec{x})=\vec{x}$ and $(M\circ L)(\vec{x})=\vec{x}$ for all $\vec{x}$, then $L$ and $M$ are said to be \textbf{invertible}. We write $M=L^{-1}$ and $L=M^{-1}$.\\ 
Example: Let $L:\mathbb{R}^2\rightarrow\mathbb{R}^2$ be defined by $L(x_1,x_2)=(2x_1+5x_2,x_1+3x_2)$ and define $M:\mathbb{R}^2\rightarrow\mathbb{R}^2$ be defined by $M(x_1,x_2)=(3x_1-5x_2,-x_1+2x_2)$. Then $M=L^{-1}$ and $L=M^{-1}$ since $$(L\circ M)(x_1,x_2) = L(M(x_1,x_2))=L(3x_1-5x_2,-x_2+2x_2)=(2(3x_1-5x_2)+5(-x_2+2x_2),(3x_1-5x_2)+3(-x_2+2x_2))=(x_1,x_2)$$ $$(M\circ L)=M(2x_1+5x_2,x_1+3x_2)=(3(2x_1+5x_2)-5(x_1+3x_2),-(2x_1+5x_2)+2(x_1+3x_2)=(x_1,x_2))$$

\paragraph{Vector Spaces} A \textbf{real vector space} is a set together with an operation of addition, denoted $\vec{x}+\vec{y}$ and an operation of scalar multiplication, denoted $t\vec{x}$ such that for any $\vec{x},\vec{y},\vec{z}\in\mathbb{V}$ and $a,b\in\mathbb{R}$ we have all of the following properties: \begin{enumerate}
    \item $\vec{x}+\vec{y}\in\mathbb{V}$ 
    \item $(\vec{x}+\vec{y})+\vec{z}=\vec{x}+(\vec{y}+\vec{z})$ 
    \item $\vec{x}+\vec{y}=\vec{y}+\vec{x}$ 
    \item There exists a zero vector such that $\vec{x}+\vec{0}=\vec{x}$ 
    \item There exists an additive inverse such that $\vec{x}+(-\vec{x})=\vec{0}$ 
    \item $a\vec{x}\in\mathbb{V}$ 
    \item $a(b\vec{x})=(ab)\vec{x}$ 
    \item $(a+b)\vec{x}=a\vec{x}=b\vec{x}$ 
    \item $a(\vec{x}+\vec{y})=a\vec{x}+a\vec{y}$ 
    \item $1\vec{x}=\vec{x}$
\end{enumerate}
We sometimes denote addition and scalar multiplication by $\oplus$ and $\odot$. Also, the zero vector of a vector space is denoted by $\vec{0}_\mathbb{V}$ to stress which vector space it belongs to. \\ 
Examples: $\mathbb{R}^n$ is a vector space with addition and scalar multiplication defined in the usual way. In $\mathbb{R}^n$, we have seen that $\vec{0}=\begin{bmatrix}0\\\vdots\\0\end{bmatrix}$ and the additive inverse of $\vec{x}$ is $(-\vec{x})=(-1)\vec{x}$ \\ 
Example: $M_{m\times n}(\mathbb{R})$, the set of all $m\times n$ matrices, is a vector space with standard addition and scalar multiplication of matrices. We have seen that $\vec{0}$ is the zero matrix and the additive inverse of $A$ is $(-A)=(-1)A$. \\ 
Example: The set $\mathbb{L}$ of all linear mappings with standard addition and scalar multiplication of linear mappings is a vector space. The zero vector is the linear mapping $0(x)=\vec{0}$ and the additive inverse of $L$ is $(-L)=(-1)L$ \\ 
Example: The set $P_n(\mathbb{R})$ of all polynomials of degree at most $n$ with real coefficients is a vector space with standard addition and scalar multiplication of polynomials. The zero vector is the zero polynomial $z(x)=0=0+0x+\ldots+0x^n$ and the additive inverse of $p(x)=a_0+a_1x+\ldots+a_nx^n$ is $(-p)(x)=(-1)p(x)=-a_0-a_1x-\ldots-a_nx^n$.  \\ 
\textbf{Theorem 4.1.1} If $\mathbb{V}$ is a vector space and $\vec{v}\in\mathbb{V}$, then \begin{enumerate}
    \item $\vec{0}=0\vec{v}$ 
    \item $(-\vec{v})=(-1)\vec{v}$ 
\end{enumerate}
Proof of 1: For all $\vec{v}\in\mathbb{V}$ we have $0\vec{v}=0\vec{v}+\vec{0}$ By V4. By V5, this equals $0\vec{v}=[\vec{v}+(-\vec{v})]$. By V10, $0\vec{v}+[1\vec{v}+(-\vec{v})]$. By V2, $[0\vec{v}+1\vec{v}]+(-\vec{v})$. By V8, $(0+1)\vec{v}+(-\vec{v})$. By operation of numbers in $\mathbb{R}$, $1\vec{v}+(-\vec{v})$. By V10, $\vec{v}+(-\vec{v})$. By V5, $=\vec{0}$, as required. \\ 
Example: Let $\mathbb{D}=\{x\in\mathbb{R}|x>0\}$ and define addition by $x\oplus y=xy$ and scalar multiplication by $t\odot x=x^t$. We will prove that $\mathbb{D}$ with these operations is a vector space. \\ 
Pick $x,y,z\in\mathbb{D}$ and $c,d\in\mathbb{R}$. Since $x,y,z\in\mathbb{D}$, we have that $x>0$, $y>0$ and $z>0$. Proof of V1-V10: \begin{enumerate}
    \item $x\oplus y=xy>0$ since $x>0$ and $y>0$ 
    \item $(x\oplus y)\oplus z=(xy)\oplus z=(xy)z=x(yz)=x\oplus(yz)=x\oplus(y\oplus z)$ 
    \item $x\oplus y=xy=yx=y\oplus x$ 
    \item By theorem 4.1.1, if $\mathbb{D}$ is a vector space, then the zero vector must be $0\odot x=x^0=1$. First observe that $1\in\mathbb{D}$, and that it satisfies $x\oplus 1=x1=x$. Therefore, $\vec{0}=1$ 
    \item By theorem 4.1.1, if $\mathbb{D}$ is a vector space, then the additive inverse of any element in $\mathbb{D}$ must be $(-1)\odot x=x^{-1}=\frac{1}{x}$. Observe that $\frac{1}{x}>0$ since $x>0$, hence $(-1)\odot x\in\mathbb{D}$. Also, $\frac{1}{x}\oplus x=1$, hence $x^{-1}$ is the additive inverse. 
    \item $c\odot x=x^c>0$, since $x>0$. Hence $c\odot x\in\mathbb{D}$ 
    \item $c\odot(d\odot x)=c\odot x^d = (x^d)^c=x^{cd}=(cd)\odot x$ 
    \item $(c+d)\odot x=x^{c+d}=x^cx^d=x^c\oplus x^d=(c\odot x)\oplus(d\odot x)$ 
    \item $c\odot (x\oplus y)=c\odot(xy)=(xy)^c=x^cy^c=x^c\oplus y^c=(c\odot x)\oplus(c\odot y)$ 
    \item $1\odot x=x^1=x$ 
\end{enumerate}
Example: Is the empty set a vector space? \\ 
By V4, a vector space must contain at least the zero vector, therefore the empty set cannot be a vector space. \\ 
Example: Let $\mathbb{V}=\{(x,y)|x,y\in\mathbb{R}$ with addition defined by $(x_1,y_1)\oplus(x_2,y_2)=(2x_1+x_2,y_1+2y_2)$ and scalar multiplication defined by $t\odot(x_1,y_1)=(tx_1,ty_1)$. Is $\mathbb{V}$ a vector space? \\ 
First, notice that scalar multiplication is standard, so if there is a problem, it would be with addition. Next, we observe that $(1,2)\oplus (3,5)=(2(1)+3,2+2(5))=(5,12)$, but $(3,5)\oplus(1,2)=(2(3)+1,5+2(2))=(7,9)$. Therefore, $(1,2)\oplus(3,5)\neq(3,5)\oplus(1,2)$, and so V3 does not hold, so $\mathbb{V}$ is not a vector space. 

\paragraph{Subspaces again} If $\mathbb{S}$ is a non-empty subset of a vector space $\mathbb{V}$, and $\mathbb{S}$ is also a vector space using te same operations as $\mathbb{V}$, then $\mathbb{S}$ is called a \textbf{subspace} of $\mathbb{V}$. \\ 
\textbf{Theorem 4.1.2- Subspace Test} A non-empty subset $\mathbb{S}$ of a vector space $\mathbb{V}$ is a subspace of $\mathbb{V}$ if for all $\vec{x},\vec{y}\in\mathbb{S}$ and $t\in\mathbb{R}$ we have $\vec{x}+\vec{y}\in\mathbb{S}$ (closed under addition), and $t\vec{x}\in\mathbb{S}$ (closed under scalar multiplication) under the operations of $\mathbb{V}$. Note: The proof is similar to the proof in $\mathbb{R}^n$, and it is important not to forget to show that $\mathbb{S}$ is a non-empty subset of $\mathbb{V}$. The best way to show that is to show that $\vec{0}_\mathbb{V}\in\mathbb{S}$. \\ \\ 
Example: Let $\mathbb{V}$ be a vector space. Show that $\mathbb{V}$ and $\mathbb{U}=\{\vec{0}_\mathbb{V}\}$ are both subspaces of $\mathbb{V}$ \\ 
Notice that $\mathbb{V}$ is a subset of itself and is a vector space under its own operations, so it satisfies the definition of a subspace. Therefore, every vector space is a subspace of itself. Clearly, $\mathbb{U}$ is a non-empty subset of $\mathbb{V}$. So, we pick any two vectors $\vec{x},\vec{y}\in\mathbb{U}$, therefore $\vec{x}=\vec{y}=\vec{0}_\mathbb{V}$ \\ 
V1: We have $\vec{x}+\vec{y}=\vec{0}_\mathbb{V}+\vec{0}_\mathbb{V}=\vec{0}_\mathbb{V}$, Hence $\vec{x}+\vec{y}\in\mathbb{U}$ \\ 
V6: $t\vec{x}=t\vec{0}_\mathbb{V}=t(0\vec{x})=(t(0))\vec{x}=0\vec{x}=\vec{0}$, thus $t\vec{x}\in\mathbb{U}$. \\ \\
Example: Let $\mathbb{W}=\{p(x)\in P_3(\mathbb{R})|p(1)=0\}$. Show that $\mathbb{W}$ is a vector space under standard addition and scalar multiplication of polynomials. \\ 
We will prove that $\mathbb{W}$ is a vector space by showing that it is a subspace of $P_3(\mathbb{R})$. By definition, $\mathbb{W}$ is a subset of $P_3(\mathbb{R})$. Also, the zero vector of $P_3(\mathbb{R})$ is the zero polynomial $z(x)=0$ which satisfies $z(1)=0$, so $\vec{0}\in\mathbb{W}$. Hence $\mathbb{W}$ is non-empty. \\ 
V1:We have $(p+q)(1)=p(1)+q(1)=0+0=0$ so $(p+q)(x)\in\mathbb{W}$. \\ 
V6: For any $t\in\mathbb{R}$ we have $(tp)(1)=tp(1)=t(0)=0$, so $(tp)(x)\in\mathbb{W}$. Therefore, $\mathbb{W}$ is a subspace of $P_3(\mathbb{R})$ by the subspace test. \\ \\ 
Example: Let $\mathbb{S}=\{ax^2+bx+c\in P_2(\mathbb{R})|a^2-b^2=0\}$. Is $\mathbb{S}$ a subspace of $P_2(\mathbb{R})$. \\ 
Notice that the parameters include a square. This makes us think it is not closed under addition. Observe that $x^2+x$ and $2x^2-2x$ are in $\mathbb{S}$, but their sum $(x^2+x)+(2x^2-2x)=3x^2-x$ is not in $\mathbb{S}$, so $\mathbb{S}$ is not a subspace. \\ \\ 
Example: Let $\mathbb{S}=\left\{\begin{bmatrix}x_1&x_2\\x_3&x_4\end{bmatrix}|x_1+x_2=2x_4\right\}$ Is $\mathbb{S}$ a subspace of $M_{2\times 2}(\mathbb{R})$? \\ 
By definition, $\mathbb{S}$ is a subset of $M_{2\times 2}(\mathbb{R})$. Also, the zero vector exists, and it is in $\mathbb{S}$, and so $\mathbb{S}$ is not empty. \\ 
V1: We get $A+B = \begin{bmatrix}a_1+b_1&a_2+b_2\\a_3+b_3&a_4+b_4\end{bmatrix}$, where $(a_1+b_1)+(a_2+b_2)=(a_1+a_2)+(b_1+b_2)=2a_4+2b_4=2(a_4+b_4)$, so $A+B\in\mathbb{S}$. \\ 
V6: We have $tA=\begin{bmatrix}ta_1&ta_2\\ta_3&ta_4\end{bmatrix}$ where $ta_1+ta_2=t(a_1+a_2)=t(2a_4)=2(ta_4)$. Thus, $tA\in\mathbb{S}$. Therefore, $\mathbb{S}$ is a subspace of $M_{2\times2}(\mathbb{R})$ by the subspace test. 

\paragraph{Spanning} Let $\mathfrak{B}=\{\vec{v}_1,\ldots,\vec{v}_k\}$ be a set of vectors in a vector space. The \textbf{span} of $\mathfrak{B}$ is defined by Span$\mathfrak{B}=\{c_1\vec{v}_1+\ldots+c_k\vec{v}_k|c_1,\ldots,c_k\in\mathbb{R}\}$. \\ 
Example: Let $\mathfrak{B}=\left\{\begin{bmatrix}2&3\\1&2\end{bmatrix},\begin{bmatrix}1&2\\-1&2\end{bmatrix},\begin{bmatrix}3&4\\1&-1\end{bmatrix}\right\}$. Determine if $A=\begin{bmatrix}2&2\\6&3\end{bmatrix}$ is in Span$\mathfrak{B}$. \\ 
By definition, $A$ is in the span of $\mathfrak{B}$ if there exists $c_1,c_2,c_3\in\mathbb{R}$ such that $$\begin{bmatrix}2&2\\6&3\end{bmatrix}=c_1\begin{bmatrix}2&3\\1&2\end{bmatrix}+c_2\begin{bmatrix}1&2\\-1&2\end{bmatrix}+c_3\begin{bmatrix}3&4\\1&-1\end{bmatrix}=\begin{bmatrix}2c_1+c_2+3c_3&3c_1+2c_2+4c_3\\c_1-c_2+c_3&2c_1+2c_2-c_3\end{bmatrix}$$ Comparing entries we get the system of linear equations $$2c_1+c_2+3c_3=2$$ $$3c_1+2c_2+4c_3=2$$ $$c_1-c_2+c_3=6$$ $$2c_1+2c_2-c_3=3$$. Row reducing, we get $c_1=4,c_2=-3,c_4=-1$. Therefore the system is consistent, and $A\in\text{Span}\mathfrak{B}$. \\
Example: Prove that the subspace $\mathbb{S}=\{a+bx+cx^2\in P_2(\mathbb{R})|a+b=c\}$ is spanned by $\mathfrak{B}=\{1+x^2,1-x\}$. \\ 
Let $p(x)\in\mathbb{S}$. Then we have that $p(x)=a+bx+cx^2$ where $a+b=c$. Hence, we can write $p(x)=a+bx+(a+b)x^2$. (To show that Span$\mathfrak{B}=\mathbb{S}$ we need to show that Span$\mathfrak{B}\subseteq\mathbb{S}$ and $\mathbb{S}\subseteq Span\mathfrak{B}$. However, it is easy to see that every vector in $\mathfrak{B}$ belongs to $\mathbb{S}$ and therefore since $\mathbb{S}$ is closed under linear combinations Span$\mathfrak{B}\subseteq\mathbb{S}$.) Consider $a+bx+(a+b)x^2=c_1(1+x^2)+c_2(1-x)=(c_1+c_2)-c_2x+c_1x^2$. Comparing coefficients we get the system $$c_1+c_2=a$$ $$-c_2=b$$ $$c_1=a+b$$ We see that the system has solution $c_1=a+b$ and $c_2=-b$. Therefore we have that $a+bx+(a+b)x^2=(a+b)(1+x^2)-b(1-x)$, and so therefore $\mathbb{S}=Span\mathfrak{B}$. \\ 
\textbf{Theorem 4.1.3} if $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is a set of vectors in a vector space $\mathbb{V}$, then Span$\{\vec{v}_1,\ldots,\vec{v}_k\}$ is a subspace of $\mathbb{V}$ \\ 
\textbf{Theorem 4.1.4} Let $\{\vec{v}_1,\ldots,\vec{v}_k\}$ be a set of vectors in a vector space $\mathbb{V}$. If $\vec{v}_i\in Span\{\vec{v}_1,\ldots,\vec{v}_k\}$, then $Span\{\vec{v}_1,\ldots,\vec{v}_{i-1},\vec{v}_{i+1},\ldots,\vec{v}_k\}=Span\{\vec{v}_1,\ldots,\vec{v}_k\}$. The proofs of these are similar to the ones above. 

\paragraph{Linear Independence} Let $\mathfrak{B}=\{\vec{v}_1,\ldots,\vec{v}_k\}$ be a set of vectors in a vector space $\mathbb{V}$. If $c_1\vec{v}_1+\ldots+c_k\vec{v}_k=\vec{0}$ has a  solution that some $c_i\neq 0$, then $\mathfrak{B}$ is said to be linearly dependent. If the only solution is $c_1=\ldots=c_k=0$, then $\mathfrak{B}$ is said to be linearly independent. \\ 
Example: Determine if the set $\{1+x-2x^2,1-2x+x^2,-2+x+x^2\}$ in $P_2(\mathbb{R})$ is linearly independent of linearly dependent. Consider $$0=c_1(1+x-2x^2)+c_2(1-2x+x^2)+c_3(-2+x+x^2)$$ $$=(c_1+c_2-2c_3)+(c_1-2c_2+c_3)x+(-2c_1+c_2+c_3)x^2$$ Comparing like powers of $x$ we get the matrix and row reducing we get $\begin{bmatrix}1&1&-2\\1&-2&1\\-2&1&1\end{bmatrix}\sim \begin{bmatrix}1&0&-1\\0&1&-1\\0&0&0\end{bmatrix}$ Thus the system has infinitely many solutions, and is linearly dependent. \\ 
Example: Determine if the set $\left\{\begin{bmatrix}1&-2\\3&1\end{bmatrix},\begin{bmatrix}2&-5\\6&4\end{bmatrix},\begin{bmatrix}3&1\\1&-4\end{bmatrix}\right\}$ is linearly independent or linearly dependent. \\ 
Consider $$\begin{bmatrix}0&0\\0&0\end{bmatrix}=c_1\begin{bmatrix}1&-2\\3&1\end{bmatrix}+c_2\begin{bmatrix}2&-5\\6&4\end{bmatrix}+c_3\begin{bmatrix}3&1\\1&-4\end{bmatrix}=\begin{bmatrix}c_1+2c_2+3c_3&-2c_1-5c_2+c_3\\3c_1+6c_2+c_3&c_1+4c_2-4c_3\end{bmatrix}$$
Comparing entries we get the homogeneous system $$\begin{bmatrix}1&2&3\\-2&-5&1\\3&6&1\\1&4&-4\end{bmatrix}\sim\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\\0&0&0\end{bmatrix}$$ Hence the only solution is $c_1=c_2=c_3=0$, and the set is linearly independent. \\ 
\textbf{Theorem 4.1.5} A set of vectors $\{\vec{v}_1,\ldots,\vec{v}_k\}$ in a vector space $\mathbb{V}$ is linearly dependent if and only if $\vec{v}_i\in Span\{\vec{v}_1,\ldots,\vec{v}_{i-1},\vec{v}_{i+1},\ldots,\vec{v}_k\}$ for some $i,1\leq i\leq k$. \\ 
\textbf{Theorem 4.1.6} Any set of vectors $\{\vec{v}_1,\ldots,\vec{v}_k\}$ in a vector space $\mathbb{V}$ which contains the zero vector is linearly dependent.

\paragraph{Bases} If $\mathfrak{B}$ is a linearly independent spanning set for a vector space $\mathbb{V}$, then $\mathfrak{B}$ is called a \textbf{basis} for $\mathbb{V}$. That is, if $\mathfrak{B}=\{\vec{v}_1,\ldots,\vec{v}_k\}$ is a basis for $\mathbb{V}$, then Span$\{\vec{v}_1,\ldots,\vec{v}_k\}=\mathbb{V}$ and $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is linearly independent. Note that, as before, we define a basis for the vector space $\{\vec{0}\}$ to be the empty set. \\ 
\textbf{Unique Representation Theorem} If $\mathfrak{B}=\{\vec{v}_1,\ldots,\vec{v}_k\}$ is a basis for vector space $\mathbb{V}$, then every $\vec{v}\in\mathbb{V}$ can be written as a unique linear combination of the vectors in $\mathfrak{B}$. \\ 
Proof: Let $\vec{v}\in\mathbb{V}$. Since $\mathfrak{B}$ spans $\mathbb{V}$, we have that there exists $c_1,\ldots,c_n$ such that $c_1\vec{v}_1+\ldots+c_n\vec{v}_n=\vec{v}$. Assume that for some $\vec{v}\in\mathbb{V}$ we have $c_1\vec{v}_1+\ldots+c_n\vec{v}_n=\vec{v}=d_1\vec{v}_1+\ldots+d_n\vec{v}_n$. Using properties V3,V4,V5, and V8 we can rearrange this to get $(c_1-d_1)\vec{v}_1+\ldots+(c_n-d_n)\vec{v}_n=\vec{0}$. Since $\mathfrak{B}$ is linearly independent, this implies that $c_i-d_i=0$ for all $i$, thus for all $i$, $c_i=d_i$, so therefore the linear combination is unique. \\ 
Example: Prove that $\mathfrak{B}=\{1-x,1+x+x^2,1+x^2\}$ is a basis for $P_2(\mathbb{R})$. \\ 
To prove that it is a basis, we need to show that $Span\mathfrak{B}=P_2(\mathbb{R})$ and that $\mathfrak{B}$ is linearly independent. Consider $$a+bx+cx^2=t_1(1-x)+t_2(1+x+x^2)+t_x(1+x^3)$$ $$(t_1+t_2+t_3)+(-t_1+t_2)x+(t_2+t_3)x^2$$, comparing like powers of $x$ we get the linear system of equations $$t_1+t_2+t_3=a$$ $$-t_1+t_2=b$$ $$t_2+t_3=c$$ Row reducint the coefficient matrix we get $$\begin{bmatrix}1&1&1\\-1&1&0\\0&1&1\end{bmatrix}\sim\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$$. Therefore, the rank of the coefficient matrix equals its number of rows, so the system is consistent for all $a,b,c\in\mathbb{R}$ by theorem 2.2.5. Thus $\mathfrak{B}$ is a spanning set for $P_2(\mathbb{R})$. To prove that it is linearly independent, we consider $0=c_1(1-x)+c_2(1+x+x^2)+c_3(1+x^2)$. Note that this will correspond to a homogeneous system with the exact same coefficient matrix as above, therefore the only solution is $c_1=c_2=c_3=0$, and so it is linearly independent. Therefore, $\mathfrak{B}$ is a basis for $P_2(\mathbb{R})$. \\ 
Example: Let $A=\begin{bmatrix}1&1\\0&0\end{bmatrix}$ and let $\mathbb{W}$ be the subspace of $M_{2\times 2}(\mathbb{R})$ defined by $\mathbb{W}=\{X\in M_{2\times 2}(\mathbb{R})|AX=XA\}$. Prove that $\mathfrak{B}=\left\{\begin{bmatrix}1&1\\0&0\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix}\right\}$ is a basis for $\mathbb{W}$ \\ 
We first observe that neither vector in $\mathfrak{b}$ is a scalar multiple of the other, so it is linearly independent by theorem 4.1.5. Every $X=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in\mathbb{W}$ satisfies $AX=XA$. Expanding this gives  $$ \begin{bmatrix}1&1\\0&0\end{bmatrix}\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}a&b\\c&d\end{bmatrix}\begin{bmatrix}1&1\\0&0\end{bmatrix}\Rightarrow\begin{bmatrix}a+c&b+d\\0&0\end{bmatrix}=\begin{bmatrix}a&a\\c&c\end{bmatrix}$$ So $c=0$ and $b+d=a$. Thus every matrix $X\in\mathbb{W}$ has the form $$\begin{bmatrix}b+d&b\\0&d\end{bmatrix}=b\begin{bmatrix}1&1\\0&0\end{bmatrix}+d\begin{bmatrix}1&0\\0&1\end{bmatrix}$$ Therefore $\mathfrak{B}$ spans $\mathbb{W}$ and so $\mathfrak{B}$ is a basis for $\mathbb{W}$. \\ 
\paragraph{Finding a Basis}
\begin{enumerate}
    \item Find the general form of a vector $\vec{X}\in\mathbb{V}$ 
    \item Write the general form of $\vec{x}$ as a linear combination of vectors 
    \item Check if $\mathfrak{B}$ is linearly independent, if it is, stop as $\mathfrak{B}$ is a basis.
    \item If it's linearly dependent, remove vectors until it is. 
\end{enumerate} 
Example: Find a basis for $\mathbb{S}=\left\{\begin{bmatrix}x_1&x_2\\x_3&x_4\end{bmatrix}|x_1+x_2+x_3=0\right\}$ \\ 
We have that $x_3=-x_1-x_2$, so any matrix will have the form $$\begin{bmatrix}x_1&x_2\\-x_1-x_2&x_4\end{bmatrix}=x_1\begin{bmatrix}1&0\\-1&0\end{bmatrix}+x_2\begin{bmatrix}0&1\\-1&0\end{bmatrix}+x_4\begin{bmatrix}0&0\\0&1\end{bmatrix}$$ Thus, $\mathbb{S}$ is spanned by those 3 vectors. Next, determine if it is linearly independent (check if those three matrices can equal the zero matrix with any coefficients). Comparing the resulting matrix, we find that it is linearly independent, and hence a basis for $\mathbb{S}$. \\ 
Example: Find a basis for the subspace $\mathbb{P}=\{a+bx+cx^2\in P_2(\mathbb{R})|a+c=b\}$.\\ 
We see that every polynomial in $\mathbb{P}$ has the form $$a+bx+cx^2=a+(a+c)x+cx^2=a(1+x)+c(x+x^2)$$ Thus $\mathfrak{B}=\{1+x,x+x^2\}$ spans $\mathbb{P}$. Note that neither is a scalar multiple of the other, so it is also the basis for $\mathbb{P}$. 

\paragraph{Dimension of a Vector Space} Suppose that $\mathcal{B}=\{\vec{v}_1,\ldots,\vec{v}_n\}$ is a basis for $\mathbb{V}$. If $\{\vec{w}_1,\ldots,\vec{w}_k\}$ is a linearly independent set in $\mathbb{V}$, then $k\leq n$. IE. A basis is also a maximum linearly independent set. \\ 
\textbf{4.2.2} If $\{\vec{v}_1,\ldots,\vec{v}_n\}$ and $\{\vec{w}_1,\ldots,\vec{w}_n\}$ are both bases of a vector space $\mathbb{V}$, then $k=n$. \\ 
\textbf{Definition} If a vector space $\mathbb{V}$ has a basis with $n$ elements, then we define \textbf{dimension} of $\mathbb{V}$ to be $n$, write dim$\mathbb{V}=n$ and we say that $\mathbb{V}$ is an \textbf{$n$-dimensional vector space}. If $\mathbb{V}$ has no basis with finitely many elements, then $\mathbb{V}$ is called \textbf{infinitely dimensional}. Here are some dimensions of basic vector spaces \begin{enumerate}
    \item The dimension of $\mathbb{R}^n$ is $n$ 
    \item The dimension of $M_{m\times n}(\mathbb{R})$ is $mn$ 
    \item The dimension of $P_n(\mathbb{R})$ is $n+1$. 
\end{enumerate}
Example: Find the dimension of the subspace $\mathbb{W}=\{p(x)\in P_3(\mathbb{R})|p(3)=0\}$ \\ 
To determine the dimension, we just need to find a basis for $\mathbb{W}$. If $p(3)=0$, then by factor theorem we get $$p(x)=(x-3)(a+bx+cx^2)=a(x-3)+b(x^2-3x)+c(x^3-3x^2)$$ So then $\mathcal{B}=\{x-3,x^2-3x,x^3-3x^2\}$ is a spanning set for $\mathbb{W}$. Now, we need to prove it is linearly independent. Note that none of them are scalar multiples, so it is linearly independent. Hence $\mathcal{B}$ is a basis for $\mathbb{W}$ and so dim$\mathbb{W}=3$. \\ 
\textbf{Theorem 4.2.3} If $\mathbb{V}$ is a $n$-dimensional vector space with $n>0$, then \begin{enumerate}
    \item no set of more than $n$ vectors in $\mathbb{V}$ can be linearly independent 
    \item no set of fewer than $n$ vectors can span $\mathbb{V}$ 
    \item a set $\mathcal{B}$ with $n$ elements is a spanning set for $\mathbb{V}$ if and only if it is linearly independent
\end{enumerate}
Example: Find a basis for the plane with equation $2x_1+x_2-x_3=0$ and then extend the basis to be a basis for $\mathbb{R}^3$. \\ 
Since we know a plane in $\mathbb{R}^3$ has dimension 2, by theorem 4.2.3, we just need to find a set of 2 linearly independent vectors, and that will form a basis of the plane. Also, for the 2 vectors to be linearly independent, they can't be scalar multiples. Hence, we pick $\vec{v}_1=\begin{bmatrix}1\\0\\2\end{bmatrix}$ and $\vec{v}_2=\begin{bmatrix}0\\1\\1\end{bmatrix}$. Thus, these two vectors are a set of 2 linearly independent vectors in the plane and hence form a basis for the plane by theorem 4.2.3. Now, since $\mathbb{R}^3$ is 3-dimensional, we just need to add one more linearly independent vector. By theorem 4.1.5, we know that if $\vec{v}_3$ is not in the span of $\{\vec{v}_1,\vec{v}_2\}$, then the set with all 3 will be linearly independent. To do that, just pick any vector that doesn't satisfy the equation of the plane. Observe that $\vec{v}_3=\begin{bmatrix}1\\0\\0\end{bmatrix}$ does not satisfy the condition of the plane, and hence $\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}$ is a basis for $\mathbb{R}^3$. \\ 
Example: Let $A_1=\begin{bmatrix}1&1\\0&0\end{bmatrix}$ and $A_2=\begin{bmatrix}1&1\\1&-1\end{bmatrix}$. Extend the set to be a basis for $M_{2\times 2}(\mathbb{R})$. \\ 
Note that neither is a scalar multiple of the other, so they form a linearly independent set. We also know that the dimension is 4, by theorem 4.2.3, to extend the set to be a basis, we just need to add 2 more independent matrices to the set. Let $E_1,E_2,E_3,E_4$ denote the standard basis vectors for $M_{2\times 2}(\mathbb{R})$. Then clearly $\{A_1,A_2,E_1,E_2,E_3,E_4\}$ clearly spans $M_{2\times 2}(\mathbb{R})$. Now, we just need to remove the ones that are linear combinations of the others. Consider $$c_1A_1+c_2A_2+c_3E_1+c_4E_2+c_5E_3+c_6E_4=\begin{bmatrix}0&0\\0&0\end{bmatrix}$$ Row reducing the coefficient matrix of this gives $$\begin{bmatrix}1&1&1&0&0&0\\1&1&0&1&0&0\\0&1&0&0&1&0\\0&-1&0&0&0&1\end{bmatrix}\sim\begin{bmatrix}1&0&0&1&0&1\\0&1&0&0&0&-1\\0&0&1&-1&0&0\\0&0&0&0&1&1\end{bmatrix}$$ So a vector equation for the solution space is $$\begin{bmatrix}c_1\\c_2\\c_3\\c_4\\c_5\\c_6\end{bmatrix}=s\begin{bmatrix}-1\\0\\1\\1\\0\\0\end{bmatrix}+t\begin{bmatrix}-1\\1\\0\\0\\-1\\1\end{bmatrix}$$ Taking $s=1$ and $t=0$ we get that $E_2=A_1-E_1$. Taking $s=0$ and $t=1$ we get $E_4=A_1-A_2+E_3$. Consequently, $E_2$ and $E_4$ are linear combinations of the others, and $\mathcal{B}=\{A_1,A_2,E_1,E_3\}$ is a set of 4 vectors which spans $M_{2\times 2}(\mathbb{R})$ and hence is a basis for it by theorem 4.2.3.\\ 
\textbf{Theorem 4.2.4} If $\mathbb{V}$ is an n-dimensional vector space and $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is linearly independent set in $\mathbb{V}$ with $k<n$, then there exists vectors $\vec{w}_{k+1},\ldots,\vec{w}_n$ in $\mathbb{V}$ such that $\{\vec{v}_1,\ldots,\vec{v}_k,\vec{w}_{k+1},\ldots,\vec{w}_n\}$ is a basis for $\mathbb{V}$. \\ 
\textbf{Corollary 4.2.5} if $\mathbb{S}$ is a subspace of a finite dimensional vector space $\mathbb{V}$, then dim$\mathbb{S}\leq dim\mathbb{V}$. 

\paragraph{Coordinates} Let $\mathcal{B} =\{\vec{v}_1,\ldots,\vec{v}_k\}$ be any basis for a vector space $\mathbb{V}$. If $\vec{v}=c_1\vec{v}_1+\ldots+c_n\vec{v}_n\in\mathbb{V}$, then we call $c_1,\ldots,c_n$ the \textbf{coordinates of $\vec{v}$ with respect to the basis $\mathcal{B}$}. We define the \textbf{coordinate vector of $\vec{v}$ with respect to the basis $\mathcal{B}$} by $[\vec{v}]_\mathcal{B}=\begin{bmatrix}c_1\\\vdots\\c_n\end{bmatrix}$ In short, we call it the $\mathcal{B}$ coordinate vector. \\ \\
Example: Consider the basis $\mathcal{B}=\left\{\begin{bmatrix}1\\1\\0\end{bmatrix},\begin{bmatrix}0\\1\\1\end{bmatrix},\begin{bmatrix}1\\0\\1\end{bmatrix}\right\}$, for $\mathbb{R}^3$. If $[\vec{v}]_\mathcal{B}=\begin{bmatrix}2\\-1\\3\end{bmatrix}$, then what is $\vec{v}$? \\ 
By definition, we have $\vec{v}=2\begin{bmatrix}1\\1\\0\end{bmatrix}-\begin{bmatrix}0\\1\\1\end{bmatrix}+3\begin{bmatrix}1\\0\\1\end{bmatrix}=\begin{bmatrix}5\\1\\2\end{bmatrix}$ It is important to notice that the order of the basis vectors matters. Therefore, when we talk about a basis, we always mean an ordered basis. \\ \\
Example: Consider the basis $\mathcal{C}=\{x+x^2,-1+x,3+x^2\}$ for $P_2(\mathbb{R})$. If $[p(x)]_\mathcal{C}=\begin{bmatrix}1\\-3\\2\end{bmatrix}$, then what is $p(x)$? \\ 
By definition, $p(x)=1(x+x^2)-3(-1+x)+2(3+x^2)=9-2x+3x^2$. \\ 
Example: Find the coordinate vector of $p(x)=3-5x+4x^2$ with respect to the basis $\mathcal{B}=\{1,x-1,(x-1)^2\}$. \\ 
We want to find $c_1,c_2,c_3$ such that $3-5x+4x^2=c_1(1)+c_2(x-1)+c_3(x-1)^2=(c_1-c_2+c_3)+(c_2-2c_3)x+c_3x^2$. Matching coefficients gives a system of linear equations with augmented matrix $$\left[\begin{array}{rrr|r}
   1 & -1 & 1 & 3\\
    0 & 1 & -2 & -5 \\
    0 & 0 & 1 & 4 
\end{array}\right]$$. By solving we find that $c_3=4,c_2=3,c_1=2$. Hence $[3-5x+4x^2]_\mathcal{B}=\begin{bmatrix}2\\3\\4\end{bmatrix}$ \\\\ 
Example: Find the coordinate vector of $\begin{bmatrix}1&2\\3&4\end{bmatrix}$ with respect to the basis $\mathcal{B}=\left\{\begin{bmatrix}1&1\\0&0\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix},\begin{bmatrix}0&1\\1&0\end{bmatrix},\begin{bmatrix}0&1\\0&1\end{bmatrix}\right\}$ of $M_{2\times 2}(\mathbb{R})$. \\ 
We want to find $c_1,c_2,c_3,c_4$ such that $$\begin{bmatrix}1&2\\3&4\end{bmatrix}=c_1\begin{bmatrix}1&1\\0&0\end{bmatrix}+c_2\begin{bmatrix}1&0\\0&1\end{bmatrix}+c_3\begin{bmatrix}0&1\\1&0\end{bmatrix}+c_4\begin{bmatrix}0&1\\0&1\end{bmatrix}$$ Comparing entries gives the following augmented matrix: $$\left[\begin{array}{rrrr|r}
   1 & 1 & 0&0 & 1\\
    1 & 0 & 1 & 1&2 \\
    0 & 0 & 1&0 & 3 \\
    0&1&0&1&4
\end{array}\right]\sim\left[\begin{array}{rrrr|r}
   1 & 0 & 0 & 0&-2\\
    0 &1& 0 & 0 & 3 \\
    0 & 0 & 1 & 0&3\\ 
    0&0&0&1&1
\end{array}\right]$$ Hence $[\begin{bmatrix}1&2\\3&4\end{bmatrix}]_\mathcal{B}=\begin{bmatrix}-2\\3\\3\\1\end{bmatrix}$ \\
\textbf{Theorem 4.3.2} If $\mathbb{V}$ is a vector space with basis $\mathcal{B}=\{\vec{v}_1,\ldots,\vec{v}_n\}$ then for any $\vec{v},\vec{w}\in\mathbb{V}$, and any real scalars $s,t$, we have $[s\vec{v}+t\vec{w}]_\mathcal{B}=s[\vec{v}]_\mathcal{B}+t[\vec{w}]_\mathcal{B}$. 

\paragraph{Change of Coordinates} Let $\mathcal{B}=\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}$, and $\mathcal{C}=\{\vec{w}_1,\vec{w}_2,\vec{w}_3\}$ be two bases for $\mathbb{R}^3$. Our goal is to find the $\mathcal{C}$-coordinate vector of $\vec{x}$ in $\mathbb{R}^3$ if we are only given the $\mathcal{B}$ coordinate vector of $\vec{x}$. Assume that $[\vec{x}]_\mathcal{B}=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}$. This means that $\vec{x}=b_1\vec{v}_1+b_2\vec{v}_2+b_3\vec{v}_3$. Observe that $$[\vec{x}]_\mathcal{C}=[b_1\vec{v}_1+b_2\vec{v}_2+b_3\vec{v}_3]_\mathcal{C}=b_1[\vec{v}_1]_\mathcal{C}+b_2[\vec{v}_2]_\mathcal{C}+b_3[\vec{v}_3]_\mathcal{C}=\left[[\vec{v}_1]_\mathcal{C}\quad[\vec{v}_2]_\mathcal{C}\quad[\vec{v}_3]_\mathcal{C}\right]\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}$$ We call this matrix $_\mathcal{C}P_\mathcal{B}=\left[[\vec{v}_1]_\mathcal{C}\quad[\vec{v}_2]_\mathcal{C}\quad[\vec{v}_3]_\mathcal{C}\right]$ the \textbf{change of coordinates matrix from $\mathcal{B}$-coordinates to $\mathcal{C}$-coordinates.} \\ \\
Example: Let $\mathcal{C}=\left\{\begin{bmatrix}1\\3\\-1\end{bmatrix},\begin{bmatrix}2\\1\\1\end{bmatrix},\begin{bmatrix}3\\4\\1\end{bmatrix}\right\}$ be a basis for $\mathbb{R}^3$. Find the $\mathcal{C}$-coordinate vector of any $\vec{x}\in\mathbb{R}^3$. \\ 
To find the C-coordinates of any vector $\vec{x}$, we will find the change of coordinates matrix from C-coordinates to standard coordinates. Our previous work shows us that $_CP_B=\left[[\vec{e}_1]_C\quad[\vec{e}_2]_C\quad[\vec{e}_3]_C\right]$. We need to find all of the following variables: $$a_1\begin{bmatrix}1\\3\\-1\end{bmatrix}+a_2\begin{bmatrix}2\\1\\1\end{bmatrix}+a_3\begin{bmatrix}3\\4\\1\end{bmatrix}=\begin{bmatrix}1\\0\\0\end{bmatrix}$$$$b_1\begin{bmatrix}1\\3\\-1\end{bmatrix}+b_2\begin{bmatrix}2\\1\\1\end{bmatrix}+b_3\begin{bmatrix}3\\4\\1\end{bmatrix}=\begin{bmatrix}0\\1\\0\end{bmatrix}$$$$c_1\begin{bmatrix}1\\3\\-1\end{bmatrix}+c_2\begin{bmatrix}2\\1\\1\end{bmatrix}+c_3\begin{bmatrix}3\\4\\1\end{bmatrix}=\begin{bmatrix}0\\0\\1\end{bmatrix}$$ Notice that this gives three identical coefficient matrices, and so we can row reduce one multiple augmented matrix: $$\left[\begin{array}{rrr|rrr}
   1&2&3&1&0&0\\
    3&1&4&0&1&0\\
    -1&1&1&0&0&1
\end{array}\right]\sim\left[\begin{array}{rrr|rrr}
   1&0&0&3/5&-1/5&-1\\
    0&1&0&7/5&-4/5&-1 \\
    0&0&1&-4/5&3/5&1
\end{array}\right]$$ Thus, $$_CP_B=\left[[\vec{e}_1]_C\quad[\vec{e}_2]_C\quad[\vec{e}_3]_C\right]=\begin{bmatrix}3/5&-1/5&-1\\7/5&-4/5&-1\\-4/5&3/5&1\end{bmatrix}$$ So $$[\vec{x}]_C=_CP_B\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}\frac{3}{5}x_1-\frac{1}{5}x_2-x_3\\\frac{7}{5}x_1-\frac{4}{5}-x_3\\-\frac{4}{5}+\frac{3}{5}+x_3\end{bmatrix}$$ \\ 
\textbf{Definition} Let $\mathcal{B}=\{\vec{v}_1,\ldots,\vec{v}_n\}$ and $C$ both be bases for a vector space $\mathbb{V}$. The matrix $_CP_B=\left[[\vec{v}_1]_C\ldots[\vec{v}_n]_C\right]$ is called the \textbf{change of coordinates matrix from B-coordinates to C-coordinates}. It satisfies $[\vec{x}]_C=_CP_B[\vec{x}]_B$. \\ 
\textbf{Theorem 4.3.3} If $B$ and $C$ are both bases of a finite dimensional vector space $\mathbb{V}$, then the change of coordinate matrices $_CP_B$ and $_BP_C$ satisfy $_CP_B\,_BP_C=I=\,_BP_C\,_CP_B$.  

\paragraph{Left and Right Inverses} Let $A$ be an $m\times n$ matrix. If $B$ is an $n\times m$ matrix such that $AB=I_m$, then $B$ is called a \textbf{right inverse} of $A$. If $C$ is an $n\times m$ matrix such that $CA=I_n$, then $C$ is called the \textbf{left inverse} of $A$. \\ 
\textbf{Theorem 5.1.1} If $A$ is an $m\times n$ matrix with $m>n$, then $A$ cannot have a right inverse. \\ 
\textbf{Corollary 5.1.2} If $A$ is an $m\times n$ matrix with $m<n$, then $A$ cannot have a left inverse \\ 
\textbf{Definition} An $n\times n$ matrix is called a \textbf{square matrix}. \\ 

\paragraph{Matrix Inverses} Let $A$ be a square matrix. If $B$ is a matrix such that $AB=I=BA$, then $B$ is called an \textbf{inverse} of $A$ and is denoted $B=A^{-1}$. If $A$ has an inverse, then $A$ is said to be \textbf{invertible}.\\ 
\textbf{Theorem 5.1.3} If $A$ is invertible, then the inverse of $A$ is unique. \\ 
\textbf{Theorem 5.1.4} If $A$ and $B$ are $n\times n$ matrices such that $AB=I$, then $A$ is the inverse of $B$ and $B$ is the inverse of $A$. Moreover, the RREF of $A$ and $B$ is $I$. \\ 
Proof: Consider $B\vec{x}=\vec{0}$. This gives $$\vec{0}=A\vec{0}=A(B\vec{x})=(AB)\vec{x}=I\vec{x}=\vec{x}$$ Hence by theorem 2.2.5 the RREF of $B$ has a leading one in each column and hence in each row since $B$ is $n\times n$. Consequently, the RREF of $B$ is $I$. To prove that $BA=I$ we use theorem 3.1.4. Since the RREF of $B$ is $I$, for every $\vec{b}\in\mathbb{R}^n$ there exists a vector $\vec{x}\in\mathbb{R}^n$ such that $B\vec{x}=\vec{b}$. Now we get $$(BA)\vec{b}=(BA)(B\vec{x})=B(AB)\vec{x}=B(I)\vec{x}=B\vec{x}=\vec{b}$$ Thus, $(BA)\vec{b}=I\vec{b}$, so $BA=I$. If $A$ is invertible, then $A^{-1}$ can be found by row reducing the augmented system $[A|I]$ to $[I|A^{-1}]$. \\ 
\textbf{Theorem 5.1.5} If $A$ is an $n\times n$ matrix with RREF $I$, then $A$ is invertible. \\ 
Note: If $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, then $A$ is invertibe if and only if $ad-bc\neq 0$. Furthermore, $A^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d&-b\\-c&a\end{bmatrix}$. $ad-bc$ is called the \textbf{determinant} of $A$. \\ 
Here are some properties of invertible matrices: (Theorem 5.1.6). If $A$ and $B$ are invertible $n\times n$ matrices and $k$ is a non-zero real number, then \begin{enumerate}
    \item $(kA)^{-1}=\frac{1}{k}A^{-1}$ 
    \item $(AB)^{-1}=B^{-1}A^{-1}$ 
    \item $(A^T)^{-1}=(A^{-1})^T$
\end{enumerate}
Proof of 1: Observe that $(kA)(\frac{1}{k}A^{-1})=\frac{k}{k}AA^{-1}=I$. Thus, $(kA)^{-1}=\frac{1}{k}A^{-1}$. \\ 
\textbf{Theorem 5.1.7-Invertible Matrix Theorem} For an $n\times n$ matrix $A$, the following are all equivalent. \begin{enumerate}
    \item $A$ is invertible 
    \item The RREF of A is I 
    \item rank$A=n$ 
    \item The system of equations $A\vec{x}=\vec{b}$ is consistent with a unique solution for all $\vec{b}$ 
    \item The nullspace of $A$ is $\{\vec{0}\}$ 
    \item The columns of $A$ form a basis for $\mathbb{R}^n$ 
    \item The rows of $A$ form a basis for $\mathbb{R}^n$ 
    \item $A^T$ is invertible 
\end{enumerate} Notice that 4 gives a nice shortcut that $\vec{x}=A^{-1}\vec{b}$. \\ \\ 
Example: Solve the system of linear equations $2x_1+4x_2=3$, $-x_1-5x_2=5$. The system has coefficient matrix $A=\begin{bmatrix}2&4\\-1&-5\end{bmatrix}$ and $\vec{b}=\begin{bmatrix}3\\5\end{bmatrix}$. Recall that $A^{-1}$ for a $2\times 2$ matrix is $A^{-1}=\frac{1}{-6}\begin{bmatrix}-5&-4\\1&2\end{bmatrix}$ Thus, the unique solution of $A\vec{x}=\vec{b}$ is $$A^{-1}\vec{b}=\frac{1}{-6}\begin{bmatrix}-5&-4\\1&2\end{bmatrix}\begin{bmatrix}3\\5\end{bmatrix}=\begin{bmatrix}35/6\\-13/6\end{bmatrix}$$ 

\paragraph{Elementary Matrices} An \textbf{elementary matrix} is an $n\times n$ matrix that is created by performing a single elementary row-operation on the $n\times n$ identity matrix. \\ 
\textbf{Theorem 5.2.1} If $A$ is an $m\times n$ matrix and $E$ is the $m\times m$ elementary matrix corresponding to the row operation $R_i+cR_j$, for $i\neq j$, then $EA$ is the matrix obtained from $A$ by performing the elementary row operation $R_i+cR_j$ on $A$.\\
\textbf{Theorem 5.2.2} If $A$ is an $m\times n$ matrix and $E$ is the $m\times m$ elementary matrix corresponding to the row operation $cR_i$, for $c\neq0$, then $EA$ is the matrix obtained from $A$ by performing the elementary row operation $cR_i$ on $A$. \\
\textbf{Theorem 5.2.3} If $A$ is an $m\times n$ matrix and $E$ is the $m\times m$ elementary matrix corresponding to the row operation $R_i\leftrightarrow R_j$, for $i\neq j$, then $EA$ is the matrix obtained from $A$ by performing the elementary row operation $R_i\leftrightarrow R_j$ on $A$. \\
\textbf{Corollary 5.2.4} If $A$ is an $m\times n$ matrix and $E$ is an $m\times m$ elementary matrix, then $rank(EA)=rank\,A$\\
Since elementary row operations are all reversible, all elementary matrices are invertible. In particular, the inverse of an elementary matrix is the elementary matrix associated with the inverse elementary row operation. 

\paragraph{Matrix Decomposition} If $A$ is an $m\times n$ matrix with row reduced echelon form $R$, then there exists a sequence $E_1,\ldots,E_k$ of $m\times m$ elementary matrices such that $E_k\ldots E_2E_1A=R$. in particular, $A=E_1^{-1}E_2^{-1}\ldots E_k^{-1}R$. \\ 
Examples: Let $A=\begin{bmatrix}1&2&0&0\\-2&-3&1&0\\1&0&-2&5\end{bmatrix}$. Write $A$ as a product of elementary matrices and its RREF. \\ 
We row reduce $A$ to its RREF keeping track of the elementary row operations used. $$\begin{bmatrix}1&2&0&0\\-2&-3&1&0\\1&0&-2&5\end{bmatrix}$$ $R_2+3R_1$, $R_3-R_1$ $$\sim\begin{bmatrix}1&2&0&0\\0&1&1&0\\0&-2&-2&5\end{bmatrix}$$ $R_1-2R_2$, $R_3+2R_2$ $$\sim\begin{bmatrix}1&0&-2&0\\0&1&1&0\\0&0&0&5\end{bmatrix}$$ $\frac{1}{5}R_3$ $$\sim\begin{bmatrix}1&0&-2&0\\0&1&1&0\\0&0&0&1\end{bmatrix}=R$$ Thus our elementary matrices are $$E_1=\begin{bmatrix}1&0&0\\2&1&0\\0&0&1\end{bmatrix}, E_2 = \begin{bmatrix}1&0&0\\0&1&0\\-1&0&1\end{bmatrix}, E_3=\begin{bmatrix}1&0&0\\0&-2&0\\0&0&1\end{bmatrix}, E_4=\begin{bmatrix}1&0&0\\0&1&0\\0&2&1\end{bmatrix},E_5=\begin{bmatrix}1&0&0\\0&1&0\\0&0&\frac{1}{5}\end{bmatrix}$$ Then theorem 5.2.5 says that $A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}R$ $$=\begin{bmatrix}1&0&0\\-2&1&0\\0&0&1\end{bmatrix}\begin{bmatrix}1&0&0\\0&1&0\\1&0&1\end{bmatrix}\begin{bmatrix}1&2&0\\0&1&0\\0&0&1\end{bmatrix}\begin{bmatrix}1&0&0\\0&1&0\\0&-2&1\end{bmatrix}\begin{bmatrix}1&0&0\\0&1&0\\0&0&5\end{bmatrix}\begin{bmatrix}1&0&-2&0\\0&1&1&0\\0&0&0&5\end{bmatrix}$$ \\ 
\textbf{Corollary 5.2.6} If $A$ is an invertible matrix, then $A$ and $A^{-1}$ can be written as a product of elementary matrices. \\ 
Example: Let $A=\begin{bmatrix}-2&3\\1&0\end{bmatrix}$ Write $A$ and $A^{-1}$ as a product of elementary matrices. \\ 
We first reduce $A$ to $I$ keeping track of the elementary row operations used. $$\begin{bmatrix}-2&3\\1&0\end{bmatrix}R_1\leftrightarrow R_2\sim \begin{bmatrix}1&0\\-2&3\end{bmatrix} R_2+2R_1\sim \begin{bmatrix}1&0\\0&3\end{bmatrix} \frac{1}{3}R_2\sim\begin{bmatrix}1&0\\0&1\end{bmatrix}$$ Thus, we have $$E_1 = \begin{bmatrix}0&1\\1&0\end{bmatrix}, E_2 = \begin{bmatrix}1&0\\2&1\end{bmatrix}, E_3=\begin{bmatrix}1&0\\0&\frac{1}{3}\end{bmatrix}$$ Hence $E_3E_2E_1=I$ and so $A^{-1}=E_3E_2E_1$. We find that $$E_3E_2E_2=E_3\begin{bmatrix}0&1\\1&2\end{bmatrix}=\begin{bmatrix}0&1\\\frac{1}{3}&\frac{2}{3}\end{bmatrix}$$ Also, we have $A=E_1^{-1}E_2^{-1}E_3^{-1}=\begin{bmatrix}0&1\\1&0\end{bmatrix}\begin{bmatrix}1&0\\-2&1\end{bmatrix}\begin{bmatrix}1&0\\0&3\end{bmatrix}$

\paragraph{Determinants} We notate the determinant of a matrix as $det\begin{bmatrix}-2&3\\1&0\end{bmatrix}=\left|\begin{matrix}-2&3\\1&0\end{matrix}\right|$ \\ 
\textbf{Definition} Let $A$ be an $n\times n$ matrix with $n\geq 2$. Let $A(i,j)$ be the $(n-1)\times(n-1)$ matrix obtained from $A$ by deleting the $i^{th}$ row and the $j^{th}$ column. The \textbf{cofactor} of $a_{ij}$ is $C_{ij}=(-1)^{i+j}\text{det}\,A(i,j)$ \\ 
Example: Find the cofactors of $A=\begin{bmatrix}1&2&3\\4&0&8\\-1&7&5\end{bmatrix}$ \\ 
We have $$C_{11}=(-1)^{1+1}\left|\begin{matrix}0&8\\7&5\end{matrix}\right| = 0(5)-8(7)=-56$$ $$C_{12}=(-1)^{1+2}\left|\begin{matrix}4&8\\-1&5\end{matrix}\right|=(-1)[4(5)-(-1)(8)]=-28$$ $$C_{13}=(-1)^{1+3}\left|\begin{matrix}4&0\\-1&7\end{matrix}\right|=4(7)-(-1)(0)=28$$ $$C_{21}=(-1)^{2+1}\left|\begin{matrix}2&3\\7&5\end{matrix}\right|=(-1)[2(5)-(7)(3)]=11$$ etc. \\ 
We can now rewrite the determinant for a $3\times 3$ matrix as $det\,A=a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}$. \\ 
\textbf{Definition}: Let $A$ be an $n\times n$ matrix. If $n=1$, we define the \textbf{determinant} of $A$ by $det\,A=det[a]=a$. If $n>1$, then we define the \textbf{determinant} of $A$ by $det\,A=a_{11}C_{11}+a_{12}C_{12}+\ldots+a_{1n}C_{1n}$. \\ 
\textbf{Theorem 5.3.1} Let $A$ be an $n\times n$ matrix with $n>1$. For $1\leq i\leq n$ we get $det\,A=a_{i1}C_{i1}+\ldots+a_{in}C_{in}$ called the \textbf{cofactor expansion across the $i^{th}$ row}. Also, for $1\leq j\leq n$ we get $det\,A=a_{1j}C_{1j}+\ldots+a_{nj}C_{nj}$ called the \textbf{cofactor expansion across the $j^{th}$ column}. This is useful because we can just use the cofactor expansion along the row or column with the most zeros. \\ 
\textbf{Definition} An $n\times n$ matrix $U$ is said to be \textbf{upper triangular} if $u_{ij}=0$ whenever $i>j$. An $n\times n$ matrix $L$ is said to be \textbf{lower triangular} if $l_{ij}=0$ whenever $i<j$. \\
\textbf{Theorem 5.3.2} If an $n\times n$ matrix $A$ is upper triangular or lower triangular, then $det\,A=a_{11}a_{22}\ldots a_{nn}$. \\ 
\textbf{Theorem 5.3.3} If $B$ is the matrix obtained from $A$ by multiplying one row of $A$ by a non-zero constant $c$, then $det\,B=c\text{det}A$\\ 
\textbf{Theorem 5.3.4} If $B$ is the matrix obtained from $A$ by swapping two rows of $A$, then $detB=-detA$\\ 
\textbf{Theorem 5.3.5} If $B$ is the matrix obtained from $A$ by adding $c$ times the $k-th$ row of A to the $j-th$ row, then $detB=detA$. \\ 
Example: Calculate $\left|\begin{matrix}2&1&3\\4&3&8\\-2&3&-2\end{matrix}\right|$ \\ 
Since adding a multiple of one row to another does not change the determinant, $$\left|\begin{matrix}2&1&3\\4&3&8\\-2&3&-2\end{matrix}\right|=\left|\begin{matrix}2&1&3\\0&1&2\\0&4&1\end{matrix}\right|=\left|\begin{matrix}2&1&3\\0&1&2\\0&0&-7\end{matrix}\right|$$ Since this is now upper triangular, the determinant is equal to the product of the diagonal entries, $=(2)(1)(-7)=-14$. \\ 
\textbf{Theorem 5.3.6} If $A$ is an $n\times n$ matrix, then $detA=detA^T$. \\ 
This gives us the following tips and tricks: \begin{enumerate}
    \item Adding a multiple of one row/column to another row/column does not change the determinant 
    \item Swapping two rows/columns multiplies the determinant by -1 
    \item Multiplying a column by a scalar multiplies the determinant by that scalar. 
\end{enumerate} 
\textbf{Corollary 5.3.7} If $A$ is an $n\times n$ matrix, and $E$ is an $n\times n$ elementary matrix, then $det\,EA=det\,Edet\,A$. \\ 
\textbf{Theorem 5.3.8- Addition to the invertible matrix theorem} An $n\times n$ matrix $A$ is invertible if and only if det$A\neq 0$ \\ 
\textbf{Theorem 5.3.9} If $A$ and $B$ are $n\times n$ matrices, then det$(AB)=det\,A\,det\,B$. 
\textbf{Corollary 5.3.10} If $A$ is an invertible matrix, then $detA^{-1}=\frac{1}{det\,A}$ 

\paragraph{Cofactor Method} Let $A$ be an $n\times n$ matrix. The \textbf{adjugate} of $A$ is the matrix defined by $(\text{adj}A)_{ij}=C_{ji}$. In particular, adj$A=(cofA)^T$\\ 
Example: Calculate the adjugate of $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$ \\ 
$adjA=\begin{bmatrix}C_{11}&C_{21}\\C_{12}&C_{22}\end{bmatrix}=\begin{bmatrix}d&-b\\-c&a\end{bmatrix}$ \\ 
\textbf{Theorem 5.4.2} If $A$ is an invertible $n\times n$ matrix, then $A^{-1}=\frac{1}{det\,A}adj\,A$. This is called the cofactor method of finding the inverse. \\ 
Example: Find the inverse of $A=\begin{bmatrix}2&1&-1\\1&3&-1\\2&-1&2\end{bmatrix}$ using the cofactor method. \\ 
We first find $$det\,A=\left|\begin{matrix}2&1&0\\1&3&2\\2&-1&1\end{matrix}\right|=13$$ We get $$adj\,A=\begin{bmatrix}C_{11}&C_{21}&C_{31}\\C_{12}&C_{22}&C_{32}\\C_{13}&C_{23}&C_{33}\end{bmatrix} = \begin{bmatrix}5&-1&2\\-4&6&1\\-7&4&5\end{bmatrix}$$ hence, $A^{-1}=\frac{1}{13}\begin{bmatrix}5&-1&2\\-4&6&1\\-7&4&5\end{bmatrix}$ \\ 
There are two reasons why we would use the cofactor method. \begin{enumerate}
    \item The cofactor method gives us a formula for each entry of the inverse of a matrix. In particular, the $ij-th$ entry of $A^{-1}=\frac{1}{det\,A}C_{ji}$
    \item It can be used to solve for when the matrix has variables
\end{enumerate}
\paragraph{Cramer's Rule} Theorem 5.4.3 : If $A$ is an $n\times n$ invertible matrix, then the solution $\vec{x}$ of $A\vec{x}=\vec{b}$ is given by $x_i=\frac{detA_i}{detA},1\leq i\leq n$, where $A_i$ is the matrix obtained from $A$ by replacing the $i^{th}$ column of $A$ by $\vec{b}$. \\ 
Example: Solve the system of linear equations $$5x_1+x_2-x_3=4$$ $$9x_1+x_2-x_3=1$$ $$x_1-x_3+5x_3=2$$. The coefficient matrix is $A=\begin{bmatrix}5&1&-1\\9&1&-1\\1&-1&5\end{bmatrix}$, so det$A=-16$. Hence, Cramer's Rule gives $$x_1=\frac{1}{-16}\left|\begin{matrix}4&1&-1\\1&1&-1\\2&-1&5\end{matrix}\right|=-\frac{3}{4}$$ $$x_2=\frac{1}{-16}\left|\begin{matrix}5&4&-1\\9&1&-1\\1&2&5\end{matrix}\right|=\frac{83}{8}$$ $$x_3=\frac{1}{-16}\left|\begin{matrix}5&1&4\\9&1&1\\1&-1&2\end{matrix}\right|=\frac{21}{8}$$ \\ 
Example: Assuming that $A=\begin{bmatrix}a&1&4\\0&b&3\\c&1&-1\end{bmatrix}$ is invertible, find $x_1$ in the solution of $A\vec{x}=\begin{bmatrix}-2\\-3\\1\end{bmatrix}$ \\ 
First, we have $detA=-ab+3a+3c-4cb$. Thus, Cramer's Rule gives $x_1=\frac{1}{detA}\left|\begin{matrix}-2&1&4\\-3&b&3\\1&1&-1\end{matrix}\right|=\frac{-2b-6}{-ab-3a+3c-4cb}$. 

\paragraph{Area of a Parallelogram} Let $\vec{u}=\begin{bmatrix}u_1\\u_2\end{bmatrix}$ and $\vec{v}=\begin{bmatrix}v_1\\v_2\end{bmatrix}$ be two vectors in $\mathbb{R}^2$. We can form a parallelogram in $\mathbb{R}$ \textbf{induced by $\vec{u}$ and $\vec{v}$} by using the corner points $(0,0),(u_1,u_2),(v_1,v_2),(u_1+v_1,u_2+v_2)$. In this case, $A=|\text{det}[\vec{u}\quad\vec{v}]|=|det\begin{bmatrix}u_1&v_1\\u_2&v_2\end{bmatrix}|$

\paragraph{Volume of a Parallelpiped} $V=\left|det\begin{bmatrix}u_1&v_1&w_1\\u_2&v_2&w_2\\u_3&v_3&w_3\end{bmatrix}\right|$ 

\paragraph{n-Volume of a Parallelotope} $V=|det[\vec{v}_1\ldots\vec{v}_n]|$. 

\paragraph{Matrix of a Linear Mapping} If $\mathcal{B}=\{\vec{v}_1,\ldots,\vec{v}_n\}$ is a basis for $\mathbb{R}^n$ and Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is a linear operator, then the \textbf{matrix of $L$ with respect to the basis $\mathcal{B}$} is defined to be $[L]_\mathcal{B}=\left[[L(\vec{v}_1)]_\mathcal{B}\cdots[L(\vec{v}_n)]_\mathcal{B}\right]$. It satisfies $[L(\vec{x})]_\mathcal{B}=[L]_\mathcal{B}[\vec{x}]_\mathcal{B}$. For short we may call the matrix of $L$ with respect to the basis $\mathcal{B}$ the \textbf{B-matrix} of $L$. \\ \\
Example: Determine the matrix of the linear operator $L:\mathbb{R}^3\rightarrow R^3$ with respect to the basis $\mathcal{B}=\{\vec{v}_1,\vec{v}_2,\vec{v}_3\}$ where $L(\vec{v}_1)=2\vec{v}_1-3\vec{v}_2,L(\vec{v}_2)=3\vec{v}_1+4\vec{v}_2-\vec{v}_3, L(\vec{v}_3)=-\vec{v}_1+2\vec{v}_2+6\vec{v}_3$. Use the matrix to find $L(\vec{x})$ where $[\vec{x}]_\mathcal{B}=\begin{bmatrix}5\\-3\\1\end{bmatrix}$ \\ 
By definition, $$[L]_\mathcal{B}=\left[[L(\vec{v}_1)]_\mathcal{B}\quad[L(\vec{v}_2)]_\mathcal{B}\quad[L(\vec{v}_3)]_\mathcal{B}\right]=\begin{bmatrix}2&3&-1\\-3&4&2\\0&-1&6\end{bmatrix}$$ Also by definition, we get that $$[L(\vec{x})]_\mathcal{B}=[L]_\mathcal{B}[\vec{x}]_\mathcal{B}=\begin{bmatrix}2&3&-1\\-3&4&2\\0&-1&6\end{bmatrix}\begin{bmatrix}5\\-3\\1\end{bmatrix}=\begin{bmatrix}0\\-25\\9\end{bmatrix}$$ Thus, $L(\vec{x})=0\vec{v}_1-25vec{v}_2+9\vec{v}_3$\\ \\
Example: Let $\vec{a}=\begin{bmatrix}3\\4\end{bmatrix}$ and $\mathcal{B}=\left\{\begin{bmatrix}3\\4\end{bmatrix},\begin{bmatrix}-4\\3\end{bmatrix}\right\}$ Determine the $\mathcal{B}$-matrix of proj$_{\vec{a}}$.  \\ 
We have $$proj_{\vec{a}}\begin{bmatrix}3\\4\end{bmatrix}=\begin{bmatrix}3\\4\end{bmatrix}=1\begin{bmatrix}3\\4\end{bmatrix}+0\begin{bmatrix}-4\\3\end{bmatrix}$$ $$proj_{\vec{a}}\begin{bmatrix}-4\\3\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}=0\begin{bmatrix}3\\4\end{bmatrix}+0\begin{bmatrix}-4\\3\end{bmatrix}$$ Thus, $[proj_{\vec{a}}]_\mathcal{B}=\left[\left[proj_{\vec{a}}\begin{bmatrix}3\\4\end{bmatrix}\right]_\mathcal{B}\quad\left[proj_{\vec{a}}\begin{bmatrix}-4\\3\end{bmatrix}\right]_\mathcal{B}\right]=\begin{bmatrix}1&0\\0&0\end{bmatrix}$ \\ 
\textbf{Definition} An $n\times n$ matrix $D$ is said to be a \textbf{diagonal matrix} if $d_{ij}=0$ for all $i\neq j$. We denote a diagonal matrix by $D=diag(d_{11},d_{22},\ldots,d_{nn})$. Note we could also define a diagonal matrix to be a matrix that is both upper and lower triangular. 

\paragraph{Similar Matrices} (Theorem 6.1.1) If there exists an invertible matrix $P$ such that $P^{-1}AP=B$, then \begin{enumerate}
    \item rank$A=rankB$ 
    \item det$A=detB$ 
    \item tr$A=trB$ where tr$A$ is defined by tr$A=\sum_{i=1}^na_{ii}$ and is called the \textbf{trace} of a matrix. 
\end{enumerate}
Let $A$ and $B$  be $n\times n$ matrices. If there exists an invertible matrix $P$ such that $P^{-1}AP=B$, then $A$ and $B$ are said to be \textbf{similar}. Note that if $P^{-1}AP=B$, then we can take $Q=P^{-1}$ to get that $Q^{-1}BQ=A$\\
Example: Let $A=\begin{bmatrix}1&3\\2&-1\end{bmatrix}$ be the standard matrix of a linear mapping $L:\mathbb{R}^2\rightarrow\mathbb{R}^2$, and let $\mathcal{B}=\left\{\begin{bmatrix}1\\3\end{bmatrix},\begin{bmatrix}2\\7\end{bmatrix}\right\}$. Find $[L]_\mathcal{B}$. \\ 
Note that $[L]_\mathcal{B}$ is similar to the standard matrix $A=[L]$ of $L$. Let $P=\begin{bmatrix}1&2\\3&7\end{bmatrix}$, then we know that $P^{-1}=\frac{1}{1}\begin{bmatrix}7&-2\\-3&1\end{bmatrix}$. Hence, $$[L]_\mathcal{B}=P^{-1}AP=\begin{bmatrix}72&167\\-31&-72\end{bmatrix}$$

\paragraph{Eigenvalues and Eigenvectors} Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^n$ be a linear operator. Assume that there is a basis $\mathcal{B}=\{\vec{v}_1,\ldots,\vec{v}_n\}$ such that the matrix of $L$ with respect to $\mathcal{B}$ is diagonal. Let $[L]_\mathcal{B}=diag(\lambda_1,\ldots,\lambda_n)$. Because $[L]_\mathcal{B}$ is similar to the standard matrix $A=[L]$ of $L$. In particular, using the change of coordinates matrix $P=[\vec{v}_1\ldots\vec{v}_n]$ we get $P^{-1}AP=diag(\lambda_1,\ldots,\lambda_n)$ implies that $A\vec{v}_i=\lambda\vec{v}_i$ for all $i$. On the other hand, $[L(\vec{v}_i)]_\mathcal{B}=[\lambda_i\vec{v}_i]_\mathcal{B}=\lambda_i\vec{e}_i$. Hence, $[L]_\mathcal{B}=[\lambda_1\vec{e}_1\cdots\lambda_n\vec{e}_n]=diag(\lambda_1,\ldots,\lambda_n)$. So $[L]_\mathcal{B}$ is diagonal. \\ 
Let $A$ be an $n\times n$ matrix. If there is a non-zero vector $\vec{v}$ such that $A\vec{v}=\lambda\vec{v}$ for some scalar $\lambda$, then $\lambda$ is called an \textbf{eigenvalue} of $A$ and $\vec{v}$ is called an \textbf{eigenvector} of $A$ corresponding to $\lambda$. We call $(\lambda,\vec{v})$ an \textbf{eigenpair}. \\ 
Let $L$ be a linear operator on $\mathbb{R}^n$. If there is a non-zero vector $\vec{v}$ such that $L(\vec{v})=\lambda\vec{v}$ for some scalar $\vec{v}$, then $\lambda$ is called an \textbf{eigenvalue} of $L$ and $\vec{v}$ is called an \textbf{eigenvector} of $L$ corresponding to $\lambda$. \\ 
Example: Consider $A= \begin{bmatrix}3&6&7\\3&3&7\\5&6&5\end{bmatrix}$. Determine which of the following vectors are eigenvectors of $A$. \begin{enumerate}
    \item $\vec{v}=\begin{bmatrix}1\\-2\\1\end{bmatrix}$\\ Note we have $A\vec{v}=\begin{bmatrix}3&6&7\\3&3&7\\5&6&5\end{bmatrix}\begin{bmatrix}1\\-2\\1\end{bmatrix}=\begin{bmatrix}-2\\4\\-2\end{bmatrix}=-2\vec{v}$ thus, $\vec{v}$ is an eigenvector of $A$ with corresponding eigenvalue $\lambda=-2$. 
    \item $\vec{v}=\begin{bmatrix}1\\1\\1\end{bmatrix}$\\ $A\vec{v}=\begin{bmatrix}3&6&7\\3&3&7\\5&6&5\end{bmatrix}\begin{bmatrix}1\\1\\1\end{bmatrix}=\begin{bmatrix}16\\13\\16\end{bmatrix}=-2\vec{v}$, which is not a scalar multiple of $\vec{v}$, so it is not an eigenvector 
    \item $\vec{v}=\begin{bmatrix}0\\0\\0\end{bmatrix}$ \\ We have $A\vec{v}=\vec{0}=1\vec{v}$, so $\vec{v}$, but $\vec{v}$ is NOT an eigenvector because by definition the zero vector is not allowed to be an eigenvector! 
\end{enumerate}
Example: Consider $A=\begin{bmatrix}3&6&7\\3&3&7\\5&6&5\end{bmatrix}$ Is $\lambda=1$ an eigenvalue of $A$? \\ 
We need to determine if there is a non-zero vector $\vec{v}$ such that $A\vec{v}=1\vec{v}$. For this we need $$\begin{bmatrix}3&6&7\\3&3&7\\5&6&5\end{bmatrix}\begin{bmatrix}v_1\\v_2\\v_3\end{bmatrix}=\begin{bmatrix}v_1\\v_2\\v_3\end{bmatrix}$$ this gives $$\begin{bmatrix}3v_1&6v_2&7v_3\\3v_1&3v_2&7v_3\\5v_1&6v_2&5v_3\end{bmatrix}=\begin{bmatrix}v_1\\v_2\\v_3\end{bmatrix}$$ Moving the variables on the right hand side to the left gives the homogeneous system gives us the homogeneous system $$\begin{bmatrix}2v_1&6v_2&7v_3\\3v_1&2v_2&7v_3\\5v_1&6v_2&4v_3\end{bmatrix}$$ Solving this system we find that the only solution is the trivial solution. Hence, the only vector that satisfies $A\vec{v}=\vec{v}$ is the zero vector, and $\lambda$ is not an eigenvalue of $A$. Note: The coefficient matrix of the homogeneous system was $A-\lambda I$. 

\paragraph{How to find an eigenvalue and eigenvector} $\lambda$ is an eigenvalue of $A$ if and only if det$(A-\lambda I)=0$. Moreover, if $\lambda$ is an eigenvalue, then all corresponding eigenvectors are the non-trivial solutions of the homogeneous system. \\ 
Example: Find the eigenvalues and corresponding eigenvectors of $A=\begin{bmatrix}2&-12\\1&-5\end{bmatrix}$\\ 
We have $$det(A-\lambda I)=det\left(\begin{bmatrix}2&-12\\1&-5\end{bmatrix}-\lambda\begin{bmatrix}1&0\\0&1\end{bmatrix}\right)=det\left(\begin{bmatrix}2-\lambda&-12\\1&-5-\lambda\end{bmatrix}\right)$$ $$=(2-\lambda)(-5-\lambda)-(-12)(1)=\lambda^2+3\lambda+2=(\lambda+2)(\lambda+1)$$ So det$(A-\lambda I)=0$ if and only if $\lambda=-2$ or $\lambda=-1$. For $\lambda_1=-2$, we have $A-\lambda_1I=\begin{bmatrix}4&-12\\1&-3\end{bmatrix}\sim \begin{bmatrix}1&-3\\0&0\end{bmatrix}$. Hence, a vector equation for the solution set is $\vec{x}=t\begin{bmatrix}3\\1\end{bmatrix}$, $t\in\mathbb{R}$, therefore all eigenvectors corresponding to $\lambda_1=-2$ are $\vec{x}=t\begin{bmatrix}3\\1\end{bmatrix}$, $t\in\mathbb{R},t\neq0$. For $\lambda_2=-1$, we have $A-\lambda_2I=\begin{bmatrix}3&-12\\1&-4\end{bmatrix}\sim\begin{bmatrix}1&-4\\0&0\end{bmatrix}$. Hence a vector equation for the solution set is $\vec{x}=t\begin{bmatrix}4\\1\end{bmatrix}$, and so the corresponding eigenvectors corresponding to $\lambda_2=-1$ are $\vec{x}=t\begin{bmatrix}4\\1\end{bmatrix},0\neq t\in\mathbb{R}$\\ 
\textbf{Definition} If $A$ is an $n\times n$ matrix, then we call the $n$-th degree polynomial given by $C(\lambda)=det(A-\lambda I)$ the \textbf{characteristic polynomial} of $A$\\ 
\textbf{Theorem 6.2.1} A scalar $\lambda$ is an eigenvalue of a square matrix $A$ if and only if $C(\lambda)=0$. \\ 
\textbf{Definition} The \textbf{algebraic multiplicity}, $a_\lambda$, of an eigenvalue $\lambda$ is the number of times $\lambda$ appears as a root of the characteristic polynomial $C(\lambda)$. \\ 
Example: We saw that the characteristic polynomial of $B=\begin{bmatrix}1&2&-2\\-2&5&-2\\-6&6&-3\end{bmatrix}$ was $C(\lambda)=-(\lambda-3)^2(\lambda+3)$. Thus for $\lambda_1=3$, we have $a_{\lambda_1}=2$, and for $\lambda_2=-3$, we have $a_{\lambda_2}=1$ \\ 
\textbf{Definition} If $\lambda$ is an eigenvalue of $A$, then Null$(A-\lambda I)$ is called the \textbf{eigenspace} of $\lambda$ and is denoted $E_\lambda$. \\ 
\textbf{Definition} The \textbf{geometric multiplicity} $g_\lambda$ of an eigenvalue $\lambda$ is the dimension of its eigenspace. That is $g_\lambda=\text{dim}\,\text{Null}(A-\lambda I)$. Note that $g_\lambda$ must be at least 1. \\ 
Example: For $B=\begin{bmatrix}1&2&-2\\-2&5&-2\\-6&6&-3\end{bmatrix}$ we found that a basis for the eigenspace of $\lambda_1=3$ was $\left\{\begin{bmatrix}1\\1\\0\end{bmatrix},\begin{bmatrix}-1\\0\\1\end{bmatrix}\right\}$. Thus, $g_{\lambda_1}=dim\,E_{\lambda_1}=2$. We also found that a basis for the eigenspace of $\lambda_2=-3$ was $\left\{\begin{bmatrix}1\\1\\3\end{bmatrix}\right\}$. Hence, $g_{\lambda_2}=dim\,E_{\lambda_2}=1$. \\ 
Example: Find the algebraic and geometric multiplicity of all eigenvalues of $A=\begin{bmatrix}1&2&3\\-2&5&3\\-4&4&5\end{bmatrix}$\\ 
We have $$C(\lambda)=det(A-\lambda I)=\left|\begin{matrix}1-\lambda&2&3\\-2&5-\lambda&3\\-4&4&5-\lambda\end{matrix}\right|$$ $$=\left|\begin{matrix}3-\lambda&2&3\\3-\lambda&5-\lambda&3\\0&4&5-\lambda\end{matrix}\right|=\left|\begin{matrix}3-\lambda&2&3\\0&3-\lambda&0\\0&4&5-\lambda\end{matrix}\right|$$ $$=-(\lambda-5)(\lambda-3)^2$$ Hence, the eigenvalues of $A$ are $\lambda_1=5$ and $\lambda_2=3$. We have $a_{\lambda_1}=1$, and $a_{\lambda_2}=2$. Now we have to find the geometric multiplicities(find the basis for the eigenspace of each eigenvalue). For $\lambda_1=5$, we get $$A-\lambda_1I=\begin{bmatrix}-4&2&3\\-2&0&3\\-4&4&0\end{bmatrix}\sim\begin{bmatrix}1&0&-3/2\\0&1&-3/2\\0&0&0\end{bmatrix}$$ Thus, a basis for the eigenspace of $\lambda_1=5$ is $\left\{\begin{bmatrix}3\\3\\2\end{bmatrix}\right\}$, and hence $g_{\lambda_1}=1$ For $\lambda_2=3$, we get $$A-\lambda_2I=\begin{bmatrix}-2&2&3\\-2&2&3\\-4&4&2\end{bmatrix}\sim\begin{bmatrix}1&-1&0\\0&0&1\\0&0&0\end{bmatrix}$$ Thus, a basis for the eigenspace of $\lambda_2=3$ is $\left\{\begin{bmatrix}1\\1\\0\end{bmatrix}\right\}$ and hence $g_{\lambda_2}=1$. \\ 
\textbf{Theorem 6.2.3} If $\lambda$ is an eigenvalue of $A$, then $1\leq g_\lambda\leq a_\lambda$. Note than an eigenvalue $\lambda$ such that $g_\lambda<a_\lambda$ is said to be \textbf{deficient}\\ 
\textbf{Theorem 6.2.4} If $\lambda_1,\ldots,\lambda_n$ are all eigenvalues of an $n\times n$ matrix $A$, then $det A=\lambda_1\cdots\lambda_n$, and $trA=\lambda_1+\cdots+\lambda_n$

\paragraph{Diagonalization Theory} An $n\times n$ matrix $A$ is said to be \textbf{diagonalizable} if there exists an invertible matrix $P$ such that $P^{-1}AP=D$ is diagonal. We say that $P$ \textbf{diagonalizes} $A$.\\ 
\textbf{Lemma 6.3.1} Suppose that $A$ is an $n\times n$ matrix and that $\lambda_1,\ldots,\lambda_k$ are distinct eigenvalues with corresponding eigenvectors $\vec{v}_1,\ldots,\vec{v}_k$, then $\{\vec{v}_1,\ldots,\vec{v}_k\}$ is linearly independent. The proof of this includes induction and rearranging addition. \\ 
\textbf{Lemma 6.3.2} If $A$ is a matrix with distinct eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$ and $\mathcal{B}_i=\{\vec{v}_{i,1},\ldots,\vec{v}_{i,g_{\lambda_i}}\}$ is a basis for the eigenspace of $\lambda_i$ for $1\leq i\leq k$, then $\mathcal{B}_1\cup\mathcal{B}_2\cup\ldots\cup\mathcal{B}_k$ is a linearly independent set. \\ 
\textbf{Theorem 6.3.3 - The Diagonalization Theorem} Let $\lambda_1,\ldots,\lambda_k$ be the distinct eigenvalues of a matrix $A$. Then, $A$ is diagonalizable if and only if $g_{\lambda_i}=a_{\lambda_i}$ for $1\leq i\leq k$. Ie. none of its eigenvalues are deficient. \\ 
\textbf{Corollary 6.3.4} If an $n\times n$ matrix $A$ has $n$ distinct eigenvalues, then $A$ is diagonalizable. \\ 
Note: \begin{enumerate}
    \item If $A$ is diagonalizable, then there exists an invertible matrix $P$ and diagonal matrix $D$ such that $P^{-1}AP=D$. Notice that we can multiply by $P$ on the left and $P^{-1}$ on the right to get a matrix factorization of $A$: $A=PDP^{-1}$. 
    \item Since we are currently only concerned with real numbers, we should modify the Diagonlization Theorem to say that a real matrix $A$ is diagonalizable over the real numbers iff all of the eigenvalues of $A$ are real and $g_{\lambda_i}=a_{\lambda_i}$. If $A$ has at least one non-real eigenvalue, and hence one non-real eigenvector, then we say that $A$ is not diagonalizable over $\mathbb{R}$. 
\end{enumerate}
\textbf{Algorithm to Diagonalize} Let $L:\mathbb{R}^n\rightarrow\mathbb{R}^n$ be a linear mapping with standard matrix $A=[L]$. To diagonalize the $n\times n$ matrix $A$, or show that $A$ is not diagonalizable: \begin{enumerate}
    \item Find and factor the characteristic polynomial $C(\lambda)=det(A-\lambda I)$ 
    \item Let $\lambda_1,\ldots,\lambda_n$ denote the $n$-roots of $C(\lambda)$ (repeated according to multiplicity). If any of the eigenvalues $\lambda_i$ are not real, then $A$ is not diagonalizable over $\mathbb{R}$. 
    \item Find a basis for the eigenspace of each $\lambda_i$ by finding a basis for the nullspace of $A-\lambda_iI$. 
    \item If $g_{\lambda_i}<a_{\lambda_i}$ for any $\lambda_i$, then $A$ is not diagonalizable. Otherwise, form a basis $\{\vec{v}_1,\ldots,\vec{v}_n\}$ for $\mathbb{R}^n$ of eigenvectors of $A$ by combining the eigenvectors in the bases for each eigenspace of $A$. Let $P=[\vec{v}_1\ldots\vec{v}_n]$. Then $P^{-1}AP=diag(\lambda_1,\ldots,\lambda_n)$, where $\lambda_i$ is an eigenvalue corresponding to the eigenvector $\vec{v}_i$ for $1\leq i\leq n$. That is, if we take $\mathcal{B}=\{\vec{v}_1,\ldots,\vec{v}_n\}$, then $[L]_\mathcal{B}=diag(\lambda_1,\ldots,\lambda_n)$.  
\end{enumerate}

\paragraph{Diagonalization Examples}  
Example: Let $A=\begin{bmatrix}5&8&16\\4&1&8\\-4&-4&-11\end{bmatrix}$ Diagonalize $A$ or show that $A$ is not diagonalizable. \\ 
Solution: The first step is to find and factor the characterization polynomial. We have $$C(\lambda)=det(A-\lambda I)=\left|\begin{matrix}5-\lambda&8&16\\4&1-\lambda&8\\-4&-4&-11-\lambda\end{matrix}\right|=\left|\begin{matrix}5-\lambda&8&8\\0&-3-\lambda&0\\-4&-4&-7-\lambda\end{matrix}\right|=-(\lambda+3)[(\lambda+7)(\lambda-5)+32]=-(\lambda-1)(\lambda+3)^2$$ Hence, the eigenvalues are $\lambda_1=1$ with algebraic multiplicity 1, and $\lambda_2=-3$ with algebraic multiplicity 2. By theorem 6.2.3, we know that $1\leq g_{\lambda_1}\leq a_{\lambda_1}=1$. Also, we know that $1\leq g_{\lambda_2}\leq a_{\lambda_2}=2$. Therefore, by the diagonalization theorem, the only way that the matrix $A$ is not diagonliazble is if $g_{\lambda_2}=1$. Therefore, to minimize work, we should test it first. \\ 
For $\lambda_2=-3$, we get $$A-\lambda_2 I=\begin{bmatrix}8&8&16\\4&4&8\\-4&-4&8\end{bmatrix}\sim\begin{bmatrix}1&1&2\\0&0&0\\0&0&0\end{bmatrix}$$ Thus a vector equation of the solution space for $(A-\lambda_2I)\vec{x}=\vec{0}$ is $\vec{x}=x_2\begin{bmatrix}-1\\1\\0\end{bmatrix}+x_3\begin{bmatrix}-2\\0\\1\end{bmatrix}$, so a basis for the eigenspace $E_{\lambda_2}$ of $\lambda_2$ is $\left\{\begin{bmatrix}-1\\1\\0\end{bmatrix},\begin{bmatrix}-2\\0\\1\end{bmatrix}\right\}$. Hence the geometric multiplicity of $\lambda_2$ is also 2, and so $A$ is diagonlizable. \\ 
For $\lambda_1=1$, we get $$A-\lambda_1I=\begin{bmatrix}4&8&16\\4&0&8\\-4&-4&-12\end{bmatrix}\sim\begin{bmatrix}1&0&2\\0&1&1\\0&0&0\end{bmatrix}$$ thus a vector equation for the solution space of $(A-\lambda_1I)\vec{x}=\vec{0}$ is $\vec{x}=x_3\begin{bmatrix}-2\\-1\\1\end{bmatrix}$. Thus a basis for the eigenspace $E_{\lambda_1}$ of $\lambda_1$ is $\left\{\begin{bmatrix}-2\\-1\\1\end{bmatrix}\right\}$. We form the basis $\mathcal{B}=\left\{\begin{bmatrix}-1\\1\\0\end{bmatrix},\begin{bmatrix}-2\\0\\1\end{bmatrix}\begin{bmatrix}-2\\-1\\1\end{bmatrix}\right\}$ Consequently, if we take $P=\begin{bmatrix}-1&-2&-2\\1&0&-1\\0&1&1\end{bmatrix}$, then we get $P^{-1}AP=diag(\lambda_2,\lambda_2,\lambda_1)=\begin{bmatrix}-3&0&0\\0&-3&0\\0&0&1\end{bmatrix}$. Note we also could have taken $P=\begin{bmatrix}-2&-1&-2\\-1&1&0\\1&0&1\end{bmatrix}$, which would have given us $P^{-1}AP=diag(\lambda_1,\lambda_2,\lambda_2)=\begin{bmatrix}1&0&0\\0&-3&0\\0&0&-3\end{bmatrix}$
\\ \\ 
Example 2: Let $B=\begin{bmatrix}2&1&1\\2&1&-2\\-1&0&-2\end{bmatrix}$ diagonalize $B$ or show that it is not diagonalizable. \\ 
Solution: The characteristic polynomial is $$C(\lambda)=\left|\begin{matrix}2-\lambda&1&1\\2&1-\lambda&2\\-1&0&-2-\lambda\end{matrix}\right|=-(\lambda+1)^2(\lambda-3)$$. Hence the eigenvalues are $\lambda_1=-1$ and $\lambda_2=3$, with $a_{\lambda_1}=2$, and $a_{\lambda_2}=1$.\\ 
For $\lambda=-1$, we have $$B-\lambda_1I=\begin{bmatrix}3&1&1\\2&2&-2\\-1&0&-1\end{bmatrix}\sim\begin{bmatrix}1&0&1\\0&1&-2\\0&0&0\end{bmatrix}$$ Hence, a basis for the eigenspace of $\lambda_1$ is $\left\{\begin{bmatrix}-1\\2\\1\end{bmatrix}\right\}$. Thus, we get that $g_{\lambda_1}=1<2=a_{\lambda_1}$ Consequently, $B$ is not diagonalizable by the Diagonalization Theorem. \\ \\ 
Example 3: Let $C=\begin{bmatrix}0&1\\-1&0\end{bmatrix}$ Diagonalize $C$ or show that it is not diagonalizable. \\ 
Solution: $C(\lambda)=\left|\begin{matrix}-\lambda&1\\-1&-\lambda\end{matrix}\right|=\lambda^2+1$ Thus, $\lambda=i$, and so the eigenvalues are not real, and $C$ is not diagonalizable over $\mathbb{R}$ \\ \\ 
Example 4: Let $A=\begin{bmatrix}3&3&-1\\-2&-1&0\\-4&-5&2\end{bmatrix}$ Diagonalize or show that it can't be diagonalized. \\ 
Solution: $$C(\lambda)=\left|\begin{matrix}3-\lambda&3&-1\\-2&-1-\lambda&0\\-4&-5&2-\lambda\end{matrix}\right|=-\lambda(\lambda-3)(\lambda-1)$$ Thus the eigenvalues of $A$ are $\lambda_1=0$, $\lambda_2=3$, and $\lambda_3=1$. Note that $a_{\lambda_i}=1$ for $1\leq i\leq 3$, so $A$ is diagonalizable. \\ 
For $\lambda_1=0$, we have $$A-\lambda_1I=\begin{bmatrix}3&3&-1\\-2&-1&0\\-4&-5&2\end{bmatrix}\sim\begin{bmatrix}1&0&1/3\\0&1&-2/3\\0&0&0\end{bmatrix}$$ Hence a basis for $E_{\lambda_1}$ is $\left\{\begin{bmatrix}-1\\2\\3\end{bmatrix}\right\}$ \\ 
For $\lambda_2=3$, we have $$A-\lambda_2I=\begin{bmatrix}0&3&-1\\-2&-4&0\\-4&-5&-1\end{bmatrix}\sim\begin{bmatrix}1&0&2/3\\0&1&-1/3\\0&0&0\end{bmatrix}$$ Hence a basis for $E_{\lambda_2}$ is $\left\{\begin{bmatrix}-2\\1\\3\end{bmatrix}\right\}$ \\ 
For $\lambda_3=1$, we have $$A-\lambda_3I=\begin{bmatrix}0&3&-1\\-2&-4&0\\-4&-5&-1\end{bmatrix}\sim\begin{bmatrix}1&0&1\\0&1&-1\\0&0&0\end{bmatrix}$$ Hence a basis for $E_{\lambda_3}$ is $\left\{\begin{bmatrix}-1\\1\\1\end{bmatrix}\right\}$. \\ 
Thus the matrix $P=\begin{bmatrix}-1&-2&-1\\2&1&1\\3&3&1\end{bmatrix}$ diagonalizes $A$ to $$P^{-1}AP=diag(0,3,1)=\begin{bmatrix}0&0&0\\0&3&0\\0&0&1\end{bmatrix}$$ \\ \\ 
Example 5: Let $B=\begin{bmatrix}1&2&3\\0&1&1\\0&0&1\end{bmatrix}$ Diagonalize $B$ or show that $B$ is not diagonalizable. \\ 
Solution: Notice that since $B$ is upper triangular, we get that $$C(\lambda)=\left|\begin{matrix}1-\lambda&2&3\\0&1-\lambda&1\\0&0&1-\lambda\end{matrix}\right|=(1-\lambda)^3=-(\lambda-1)^3$$ Hence $\lambda_1=1$ is an eigenvalue with $a_{\lambda_1}=3$. Notice that to be diagonalizable, $g_{\lambda_1}$ has to be 3. To get a 3-dimensional nullspace, the RREF of $(B-\lambda_1I)$ must have 3 columns of zeros. But then, it would be the 0 matrix. Since it is clearly not, it is not diagonalizable. Indeed, $$B-\lambda_1I=\begin{bmatrix}0&2&3\\0&0&1\\0&0&0\end{bmatrix}\sim\begin{bmatrix}0&1&0\\0&0&1\\0&0&0\end{bmatrix}$$ which translates to a basis for $E_{\lambda_1}$ is $\left\{\begin{bmatrix}1\\0\\0\end{bmatrix}\right\}$, and since $g_{\lambda_1}=1<a_{\lambda_1}$, $B$ is not diagonalizable. 

\paragraph{Powers of Matrices} Let $A$ be an $n\times n$ matrix. We define $A^2=A(A)$, etc. \\ 
\textbf{Lemma 6.4.1} If $D=diag(d_1,\ldots,d_n)$, then $D^m=diag(d_1^m,\ldots,d_n^m)$ for any positive integer $m$. Proof is just induction on $m$. \\ 
\textbf{Theorem 6.4.2} If there exists an invertible matrix $P$ such that $P^{-1}AP=D$ is diagonal, then $A^m=PD^mP^{-1}$. This proof also includes induction, and rearranging the above to get $A=PDP^{-1}$. \\ \\ 
Example: Let $A=\begin{bmatrix}1&2\\-1&4\end{bmatrix}$ Calculate $A^{1000}$. \\ 
Solution: We first diagonalize $A$. We have $$C(\lambda)=det(A-\lambda I)=(\lambda-2)(\lambda-3)$$ Thus we have the eigenvalues $\lambda_1=2$ and $\lambda_2=3$. \\ 
For $\lambda_1=2$, we get $$A-\lambda_1I=\begin{bmatrix}-1&2\\-1&2\end{bmatrix}\sim\begin{bmatrix}1&-2\\0&0\end{bmatrix}$$ Hence a basis for $E_{\lambda_1}$ is $\left\{\begin{bmatrix}2\\1\end{bmatrix}\right\}$ \\ 
For $\lambda_2=3$, we get $$A-\lambda_2I=\begin{bmatrix}-2&2\\-1&1\end{bmatrix}\sim\begin{bmatrix}1&-1\\0&0\end{bmatrix}$$ Hence a basis for $E_{\lambda_2}$ is $\left\{\begin{bmatrix}1\\1\end{bmatrix}\right\}$ \\ 
Therefore, taking $P=\begin{bmatrix}2&1\\1&1\end{bmatrix}$ gives $P^{-1}AP=D$ where $D=\begin{bmatrix}2&0\\0&3\end{bmatrix}$. Thus by theorem 6.4.2 we get that $$A^{1000}=PD^{1000}P^{-1}$$ $$=\begin{bmatrix}2&1\\1&1\end{bmatrix}\begin{bmatrix}2^{1000}&0\\0&3^{1000}\end{bmatrix}\begin{bmatrix}1&-1\\-1&2\end{bmatrix} = \begin{bmatrix}2^{1001}-3^{1000}&-2^{1001}+2\cdot3^{1000}\\2^{1000}-3^{1000}&-2^{1000}+2\cdot3^{1000}\end{bmatrix}$$\\ \\ 
Example: Let $B=\begin{bmatrix}2&2&2\\2&2&2\\2&2&2\end{bmatrix}$. Calculate $B^{100}$. 









\end{document}