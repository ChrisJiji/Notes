\documentclass[10pt,letter]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{setspace}
\onehalfspacing
\usepackage{fullpage}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newtheorem*{remark}{Remark}
\begin{document}

\subsection*{General Info}
Name: Jeff Wong \quad Email: Tywong@uwaterloo.ca \quad Office hour: Friday 12-1 
\section*{Lecture 1}

\subsection*{Definitions of Probability}
\begin{itemize}
    \item Classical: Assuming all outcomes are equally likely, then the probability of some event is the number of ways the event occurs divided by the total number of possible outcomes. THE PROBLEM: what is "equally likely"? Outcomes are often NOT "equally likely".
    \item Relative frequency: Portion of times the event occurs in a very long series of repetition of experiments. THE PROBLEM: NOT realistic.
    \item Subjective: A measure of how sure the person making the statement is that the event will happen. THE PROBLEM: NOT objective. 
\end{itemize}

\paragraph{Some Definitions for our Probability Model}
\begin{itemize}
    \item Sample Space: Collection of all possible outcomes. E.g., in a coin flip, the sample space contains heads and tails.
    \item Events: Subsets of sample space
    \item Assigning probabilities: mapping between events to numbers $\in[0,1]$
\end{itemize}

\section*{Lecture 2}
\subsection*{Sample Spaces and Probability} 
\paragraph{More definitions}
\begin{itemize}
    \item Sample Space: A set of distinct outcomes with the property that a single, and only one of these outcomes occurs. 
    \item Discrete Sample Space: A sample space with a countable amount of elements. 
    \item Non-discrete sample space: A sample space with an uncountable amount of elements. 
    \item Event: A subset of a sample space. A \textbf{simple} event is an event that only contains 1 element. A \textbf{compound} event contains more than 1 element.
\end{itemize}

\paragraph{Assigning Probabilities} 
We want to assign a number (called probability) to event in $S$. The assignment needs to satisfy the axioms of probability. \begin{itemize}
    \item Probability of an event is non-negative.
    \item Probability of the entire sample space is 1.
    \item Any countable sequence of mutually exclusive events should satisfy $P(\cup E_i)=\sum_i P(E_i)$
\end{itemize}

\section*{Lecture 3}
\subsection*{Addition and Multiplication Rules} 
\paragraph{Addition Rule} If we have $A$ ways doing something and $B$ ways of doing another thing, and we \textbf{can not do both} at the same time, then there are $A+B$ ways to do one of these actions. 
\paragraph{Multiplication Rule} If there are $A$ ways of doing something and $B$ ways of doing another thing, then there are $A\times B$ ways of \textbf{performing both outcomes}.

\subsection*{Permutation} \textbf{An arrangement} of $r$ objects taken from a set of $n$ \textbf{distinguishable objects} is called a permutation of $n$ objects taken $r$ at a time. The total number of such ordered subsets are symbolized $nPr$($r\leq n$) and can be calculated by $nPr=n(n-1)\ldots(n-r+1)=\frac{n!}{(n-r)!}$. \textbf{Note: Order matters in permutations}. 

\subsection*{Combination} A subset of $r$ objects selected \textbf{without regard to order} form a set of $n$ \textbf{distinguishable objects} is called a combination of $n$ objects taken $r$ at a time. The total number of such combination is symbolized by $nCr$ ($r\leq n$) and can be calculated by: $nCr=\frac{n!}{r!(n-r)!}=\frac{nPr}{r!}$. 

\section*{Lecture 4}
\subsection*{Permutation for Indistinguishable Objects} Consider the following: How many different arrangements can be formed using the word LETTERS? Suppose the 2 T's and 2 E's are distinguishable, then the answer is simply $_7P_7=7!=5040$. But since the E's and T's are not distinguishable, we must remove some permutations. Observe that $LE_1T_1T_2E_2RS$, $LE_1T_2T_1E_2RS$, $LE_2T_1T_2E_1RS$, and $LE_2T_2T_1E_1RS$ are all counted in the permutations, but should not be. There are $2!*2!$ ways to arrange the E's and T's, so we must divide our original answer by $2!*2!=4$ to get the actual amount of distinct permutations. \\ 
In general, if $n_1$ objects are of type 1, and $n_r$ objects are of type $r$, then the number of permutation of true $n$ objects $(n=n_1+n_2+\ldots+n_r)$ is $\frac{n!}{n_1n_2!\ldots n_r!}$. \\ 
Neat trick: You have \$25, and imagine there are 4 investment opportunities, with the minimum investments being 3,4, 4, and 5 respectively. How many different investment strategies are available if investment must be made in each opportunity? there are $25-(3+4+4+5)=9$ leftover to invest among 4 opportunities. This cannot be solved using nCr or nPr, however, if we simplify this problem into binary strings of 12, with each partition of 0's by the 1's representing the money invested in that respective opportunity, we can use the possible permutations of these symbols to calculate the solution. There are 12 total symbols, with 9 0's and 3 1's, and so the answer is $\frac{12!}{3!9!}=220$. 

\section*{Lecture 5}
Examples (3.5) and definition of set operations (union, intersect, complement, difference), and examples of using venn diagrams, up to the end of 4.1.

\section*{Lecture 6}
Set relationships (disjoint, mutually exclusive, complement) 
\paragraph{Axioms of Probability}
A probability on the sample space $S$ is an assinment of a value, $P(E)$, to each event $E$ such that \begin{itemize}
    \item $P(E)\geq0$ for any event $E$ in $S$ 
    \item $P(S)=1$ 
    \item for any sequence of mutually exclusive events $E_1,E_2,\ldots$, $P(\cup_{i=1}E_i)=\sum_{i=1}P(E_i)$ 
\end{itemize} 
Using the above axioms, we can derive the following useful properties: 
\begin{itemize}
    \item $P(\phi)=0$ 
    \item $P(A^C)=1-P(A)$
    \item The following are all the Inclusion-Exclusion Principle
    \begin{itemize}
        \item for $n=2$, $P(A\cup B)=P(A)+P(B)-P(A\cap B)$ 
        \item for $n=3$, $P(A\cup B\cup C)=P(A)+P(B)+P(C)-P(A\cap B)-P(A\cap C)-P(B\cap C)+P(A\cap B\cap C)$ 
        \item for $n=n$, $P(E_1\cup\ldots\cup E_n)=\sum_{i=1}^nP(E_i)-\sum_{1\leq i<j\leq n}(E_{i}\cap E_{j})+\ldots+\sum_{1\leq i<j<k\leq n}P(A_i\cap A_j\cap A_k)-\ldots+(-1)^{n-1}P(E_1\cap\ldots\cap E_n)$
    \end{itemize}
\end{itemize}

\section*{Lecture 7}
\paragraph{Independence for 2 Events}
Two events $A$ and $B$ are called \textbf{independent} if $P(A\cap B)=P(A)P(B)$. \pagebreak\\ 
EXAMPLE: \begin{tabular}{ |c|c|c| }\hline&Support(S)&Oppose(O)\\\hline White(W)&0.459&0.441\\Non-white(N)&0.051&0.049\\\hline\end{tabular}\\ 
Determine whether race and opinion on abortion are independent. Check that $P(W)=0.459+0.441=0.9$, and $P(S)=0.459+0.051=0.51$. Then $P(W\cap S)=0.459=0.9\times0.51=P(W)P(S)$. Similarly, it can be shown that $P(W\cap O)=P(W)P(O)$, $P(N\cap S)=P(N)P(S)$ and $P(N\cap O)=P(N)P(O)$, so therefore race and opinion on abortion are independent.  

\paragraph{Independence for Multiple Events}
Events $E_1,\ldots,E_n$ are mutually independent iff $P(E_{i_1}\cap\ldots\cap E_{i_k})=P(E_{i_1})\ldots P(E_{i_k})$ for all sets $(i_1,\ldots,i_k)$ of distinct $i$'s chosen from $(1,2,\ldots,n)$. 

\paragraph{Conditional Probability} 
The conditional probability of $A$ given the occurrence of $B$ is written as $P(A|B)$ and is defined as $P(A|B)=\frac{P(A\cap B)}{P(B)}$.  \\ 
EXAMPLE: A mother has two kids. You asked, "is any one of them a boy?" The mother says yes. What is the probability that they are both boys? \\ 
Denote $A=\{\geq1 \text{ boy}\}$ and $B=\{2 \text{ boys}\}$. Then $A\cap B=\{2 \text{ boys}\}$. Then $P(A\cap B)=\{2 \text{ boys}\}$. Note that $P(A\cap B)=P(2\text{ boys})=\frac{1}{2^2}=\frac{1}{4}$, and $P(A)=P(\geq1\text{ boy})=1-P(0\text{ boys})=1-\frac{1}{2^2}=\frac{3}{4}$. Then $P(B|A)=\frac{P(B\cap A)}{P(A)}=\frac{\frac{1}{4}}{\frac{3}{4}}=\frac{1}{3}$. \\ 

\paragraph{Independence and Conditional Probability} 
If $P(B)>0$, then two events $A$ and $B$ are independent iff $P(A|B)=P(A)$. 

\section*{Lecture 8}
\paragraph{Product Rule (aka multiplication theorem)}
For any two events $A,B$ with $P(B)\geq0$, $P(A\cap B)=P(B)P(A|B)$. For any three events $A,B,C$ with $P(B\cap C)\geq0$, $P(A\cap B\cap C)=P(C)P(B|C)P(A|B\cap C)$

\paragraph{Law of Total Probability}
If $A_1,\ldots,A_k$ are mutually exclusive events, and is a partition of sample space $S$, then for every event $B$, $P(B)=\sum_{i=1}^kP(B|A_i)P(A_i)$. From this we can see $P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)$ 

\paragraph{Bayes' Theorem}
If $B_1,\ldots,B_k$ are mutually exclusive events and is a partition of sample space $S$, and $A$ is any event with $P(A)>0$, then for any $B$, $P(B_j|A)=\frac{P(A|B_j)P(B_j)}{P(A)}=\frac{P(A|B_j)P(B_j)}{\sum_{i=1}^kP(A|B_i)P(B_i)}$

\section*{Lecture 9}
Examples of using the theorems used in previous lectures

\section*{Lecture 10}
\paragraph{Random Variable}
A random variable $X:S\rightarrow\mathbb{R}$ is a \textbf{numerical valued function} defined on a sample space $S$. In other words, a number $X(w)$ is assigned to each outcome $w$ in the sample space. We may interpret $X(w)$ as a measure of characteristic of interest. To find the probability that $P(X=x)$ is equivalent to find the probability $P(\{w\in S|X(w)=x\})$. To find the probability that $P(X\leq x)$ is equivalent to find the probability that $\{w\in S|X(w)\leq x\}$. 
\paragraph{Discrete Random Variable}
A random variable $X$ defined on $S$ is called a \textbf{discrete random variable} if $X(S)=\{X(w)|x\in S\}$ is countable (e.g. $S\rightarrow \mathbb{N}_0$). 
\paragraph{Probability Mass Function}
A probability mass function, or pmf or pf, of a discrete random variable $X$ is defined as $f(x)=P(X=x)\quad\forall x\in X(S)$, where $X(S)$ is the countable set of possible values of $X$. It should have the following conditions: \begin{itemize}
    \item $f(x)\geq0$ for all $x\in X(S)$
    \item $f(x)=0$ for all $x\notin X(S)$ 
    \item $\sum_{x\in X(S)}f(x)=1$ 
    \item $P(X\in A)=\sum_{x\in A}f(x)$, where $A\subset X(S)$
\end{itemize}
 
\section*{Lecture 11}
\paragraph{Cumulative Distribution Function}
The cumulative distribution function (cdf) of a discrete random variable $X$ is defined as $F(x)=\mathbb{P}(X\leq x), \forall x\in\mathbb{R}$. It has the following properties: \begin{enumerate}
    \item $F(x)$ is non-decreasing
    \item $\lim_{n\rightarrow-\infty}F(x)=0$ and $\lim_{n\rightarrow\infty}F(x)=1$ 
    \item $F$ is right continuous (note the $\leq$ sign)
\end{enumerate}

\paragraph{PMf and CDF}
$f(x)=P(X=x)=P(X\leq x)-P(X\leq x-1)=F(x)-F(x-1)$ 

\paragraph{Discrete Uniform Distribution}
Let $X$ be the value randomly dranw from $a,a+1,\ldots,b$ numbers with all values being equally likely. Then $X$ is said to have a discrete uniform distribution and is denoted $X\sim\text{uni}(a,b)$. Note that $$f(x)=\begin{cases}\frac{1}{b-a+1}\quad x=a,a+1,\ldots,b\\0\quad\quad\quad\,\,\,\,\quad \text{otherwise}\end{cases}$$ $$F(x)=\begin{cases}0\quad\quad\quad\quad\quad\,\,\,\, x<a\\\frac{\lfloor x\rfloor-a+1}{b-a+1}\quad a\leq x<b\\1\quad\quad\quad\quad\quad\,\,\,\, b\leq x\end{cases} $$

\paragraph{Bernoulli Distribution}
Let $X$ be the random variable denoting the number of successes in a Bernoulli trial. Then $X$ is said to have a Bernoulli distribution and is denoted $X\sim\text{Bernoulli}(p)$, where $p$ is the probability of success. $$f(x)=\begin{cases}p^x(1-p)^{1-x}\quad x=0,1\\0\quad\quad\quad\quad\,\,\,\,\,\quad\text{otherwise}\end{cases}$$ 

\paragraph{Binomial Distribution}
Let $X$ be the random variable denoting the number of successes in \textbf{$n$ independent Bernoulli trials}. Then $X$ is said to have a Binomial distribution and is denoted as $X\sim\text{Binomial}(n,p)$. $$f(x)=\begin{cases}{n\choose x}(p)^x(1-p)^{n-x}\quad x=0,1,\ldots,n\\0\quad\quad\quad\quad\quad\quad\quad\quad\,\,\,\,\text{otherwise}\end{cases}$$ Note that independence is a key assumption in the construction of binomial distribution (ie. the success of any trial is NOT affected by the outcome of other trials). 


\section*{Lecture 12}
\paragraph{Hypergeometric Distribution}
Suppose we have $N$ objects with $m$ objects as type $I$ and $(N-m)$ objects as type $II$. A sample of $n$ objects is randomly drawn without replacement from the $N$ objects. Let $X$ be the number of type $I$ objects in the sample. Then $X$ is said to have a hypergeometric distribution and is denoted by $X\sim \text{Hypergeometric}(N,m,n)$. $f(x)=\frac{{m\choose x}{N-m\choose n-x}}{{N\choose n}}$ Note that $\text{max}(0,n-(N-m))\leq x\leq \text{min}(n,m)$

\section*{Lecture 13}
\paragraph{Geometric Distribution}
Suppose we perform a sequence of independent Bernoulli trials with success probability $P$. Let $X$ be the number of failures obtained before the $1^{st}$ success is obtained. Then $X$ is said to have a geometric distribution and is denoted by $X\sim\text{Geometric}(p)$. $f(x)=(1-p)^xp$, $F(x)=1-(1-p)^{x+1}$

\paragraph{Negative Binomial Distribution}
Suppose we perform a sequence of independent Bernoulli trials with success probability $P$. Let $X$ be the number of failures obtained before the $k^{th}$ success is obtained. Then $X$ is said to have a negative binomial distribution and is denoted by $X\sim NB(k,p)$. $f(x)={x+k-1\choose x}p^k(1-p)^x$. Note that geometric distribution is a special case of negative binomial distribution by taking $k=1$. 

\section*{Lecture 14}
\paragraph{Poisson Process}
Let an event $E$ with occurrence in time obeying the following postulates: 
\begin{itemize}
    \item independence- The number of times $E$ occurs in \textbf{non-overlapping time intervals} are independent
    \item Individuality/Lack of clustering- probability of \textbf{two or more occurrences} in a sufficiently short interval is essentially zero 
    \item Homogeneity- probability of \textbf{exactly one occurrence} in a sufficiently short time interval of length $h$ is approximately $\lambda h$ (proportional to $h$) 
\end{itemize}
Denote $N(t)$ as the \textbf{number of occurrences of $E$ with time interval [0,t]}. Then $\{N(t),t\geq0\}$ is said to be a Poisson Process with rate $\lambda$. Essentially, the Poisson Process is used to model occurrences of events that appear to happen at a constant rate $(\lambda)$, but completely at random. \\ 
The pmf of $N(t)$ is $f(x)=\mathbb{P}(N(t)=x)=\frac{(\lambda t)^xe^{-\lambda t}}{x!}$ \\ 
A random variable $X$ is said to have a Poisson distribution with parameter $\mu$ ($\mu>0$) if $f(x)=\frac{\mu^xe^{-\mu}}{x!}$. It is denoted as $X\sim Poisson(\mu)$ 

\paragraph{Poisson Approximation to Binomial} 
When $n$ is large and $p$ is small such that $np$ is bounded, then the binomial distribution Binomial$(n,p)$ can be approximated by Poisson$(np)$, ie. $f(x)={n\choose x}p^x(1-p)^{n-x}\approx \frac{(np)^xe^{-np}}{x!}$. It works well when $n\geq100$ and $np\leq10$. 

\section*{Lecture 15}
examples of using the previous distributions, but together. More precisely, using the Poisson Distribution to find the probability of something, then using that probability in other distribution's calculations.

\section*{Lecture 16}
Examples as above. 
\paragraph{Summarizing Data on Random Variables}
A \textbf{sample} refers to a set of observed outcomes $x_1,\ldots,x_n$ for a random variable $X$. \\ 
We can measure a data's \textbf{central tendency} using three different methods: 
\begin{itemize}
    \item The \textbf{sample mean} is defined by $\bar{x}=\frac{1}{n}\sum_{i=1}^nx_i$. 
    \item The \textbf{median} is a value such that half the results are below it and half above it when the results are arranged in order. When there are an even number of sample points, we take the average of the middle two values. 
    \item The \textbf{mode} is the value that occurs most often. There could be multiple modes. 
\end{itemize}

\section*{Lecture 17}
\paragraph{Expectation of a Random Variable}
Let $X$ be a random variable with probability function $f(x)$. The expected value of $X$ is defined by $\mu=E(X)=\sum_{x\in X(\Omega)}xf(x)$, provided that the sum exists. In general, for any function $g$, the expected value of $g(X)$ is defined by $E(g(X))=\sum_{x\in X(\Omega)}g(x)f(x)$. As a direct consequence, there are the following properties: \begin{itemize}
    \item If $c$ is a constant, then $E(c)=c$ 
    \item If $c$ is a constant, then $E(cg(X))=cE(g(X))$ 
    \item If $c_1,\ldots,c_n$ are constants, then $E\left(\sum_{i=1}^nc_ig_i(X)\right)=\sum_{i=1}^nc_iE(g_i(X))$ 
    \item $E(g(X))\neq g(E(X))$
\end{itemize}
Expectation of random variables is usually applied in problems with a concept of profit and loss.

\section*{Lecture 18}
\paragraph{Means and Variance of Distributions}
Let $X$ be a random variable. \\ 
$E(X)$ is called the \textbf{mean} of $X$, and is usually denoted by $\mu$. It measures the central location of the random variable $X$. \\ 
$\text{Var}(X)=E((X-\mu)^2)$ is called the \textbf{variance} of $X$, and is usually denoted by $\sigma^2$. It measures the spread of the random variable $X$. \\ 
The positive square root $\sigma=\sqrt{\text{Var}(X)}$ is called the \textbf{standard deviation} of $X$. Because of this, we have the following properties: \begin{itemize}
    \item $\text{Var}(X)=E(X^2)-\mu^2=E(X^2)-[E(X)]^2$ 
    \item $\text{Var}(aX+b)=a^2Var(X)$ for some constants $a,b$
\end{itemize}

\begin{tabular}{|c|c|c|}
\hline
Distribution & Mean & Variance \\\hline
Binomial     & $np$   & $np(1-p)$  \\\hline
Poisson      & $\mu$   & $\mu$      \\\hline
\end{tabular}

\section*{Lecture 19}
Mean and variance visualized. Examples of finding means and variance for distributions.

\section*{Lecture 20}
\paragraph{Continuous Random Variable}
A random variable $X$ on $S$ is called a continuous random variable if $X(S)$ takes values in a continuous set. Note that an equivalent definition is $X$ is continuous if its cdf $F(x)=P(X=x)$ has the form $F(x)=\int_{-\infty}^xf(t)dt$, for some function $f:\mathbb{R}\rightarrow [0,\infty)$. The function $f$ is called the \textbf{probability density function}. Note that from above, we immediately have $f(x)=\frac{d}{dx}F(x)$

\paragraph{Properties and Conditions for Probability Density Functions}
CONDITIONS: \begin{enumerate}
    \item $f(x)\geq0\quad\forall x$
    \item $\int_{-\infty}^\infty f(x)dx=1$
    \item $P(X\in B)=\int_B f(x)dx$, where $B$ is any subset of $\mathbb{R}$
\end{enumerate}
PROPERITES: \begin{enumerate}
    \item $P(X=a)=\int_a^af(x)dx=0$ for all $a$ 
    \item As a direct consequence of (1), we have $P(a\leq X\leq b)=P(a<x<b)=\int_b^af(x)dx=F(b)-F(a)$
    \item $F$ is continuous everywhere 
\end{enumerate}

\paragraph{Quantities and Percentiles}
For a continuous random variable $X$, the $p-$th quantile (or 100p-th percentile) of $X$ is the value $q(p)$ so that $F(q(p))=p$. For a continuous random variable, the median of $X$ is $q(0.5)$. 

\section*{Lecture 21}
\paragraph{Expectation, Mean, and Variance for a Continuous Random Variable}
Let $X$ be a continuous random variable with pdf $f(x)$. The expectation of $X$ is defined by $E(X)=\int_{-\infty}^\infty xf(x)dx$, provided that the integral exists. In general, for any function $g$, the expected value of $g(X)$ is defined by $E(g(X))=\int_{-\infty}^\infty g(x)f(x)dx$. Then $\mu=E(x)$ and $\sigma^2=\text{Var}(X)=E((X-\mu)^2)=E(X^2)-[E(X)]^2$
\paragraph{Transformation of Random Variables}
Let $X$ be a continuous random variable distributed on space $S$ with pdf $f(x)$. Let $Y$ be another random variable given by $Y=g(X)$. The following procedure outlines the steps in obtaining the pdf of $Y$ denoted as $f_Y(y)$. \begin{enumerate}
    \item Find the cdf of $Y$ $(F_Y(y))$ by using the cdf of $X$ ($P(Y\leq y)=P(g(X)\leq \int f(x)dx)$, solve for $y$ in $g(X)$)
    \item Differentiate $F_Y(y)$ (the y you solved for previously) to obtain $f_Y(y)$ 
    \item Find the domain of $f_Y(y)$ 
\end{enumerate}


\section*{Lecture 22}
\paragraph{Continuous Uniform Distribution}
For an interval $(a,b)$, let $X$ be the point randomly drawn from this interval. Then $X$ is said to have a continuous uniform distribution and is denoted as $X\sim U(a,b)$. Note that then\\ 
$f(x)= \left.
\begin{cases}
\frac{1}{b-a}, & \text{for } x\in[a,b] \\ 
0, & \text{otherwise}
\end{cases}\right\}\quad\quad$
$F(x)=\left.
\begin{cases}
0, & \text{for }x<a\\ 
\frac{x-a}{b-a}, & \text{for }a\leq x<b\\
1, & \text{for }b\leq x
\end{cases}\right\}$

\paragraph{Theorem}
For $X\sim U(a,b)$, we have $E(X)=\frac{a+b}{2}$ and $\text{Var}(X)=\frac{(b-a)^2}{12}$. The proof is on page 161.

\paragraph{Properties of Continuous Uniform Distribution}
Let $X\sim U(a,b)$, $Y=cX+d$. Then \begin{itemize}
    \item $Y\sim U(ac+d,bc+d)$ if $c$ is positive 
    \item $Y\sim U(bc+d,ac+d)$ if $c$ is negative
\end{itemize} 

\section*{Lecture 23}
\paragraph{Exponential Distribution}
Let $X$ be a positive random variable with pdf $(\lambda>0)$. 
$f(x)=\left.
\begin{cases}
\lambda e^{-\lambda x}, & \text{for }x>0\\ 
0, & \text{otherwise}
\end{cases}\right\}$
Then $X$ is said to have an exponential distribution and is denoted as $X\sim \text{Exponential}(\lambda)$. Note that the CDF, $F(x)=e^{-\lambda x}$

\paragraph{Theorem}
For $X\sim\text{Exponential}(\lambda)$, $E(X)=\frac{1}{\lambda}$ and $\text{Var}(X)=\frac{1}{\lambda^2}$. 

\paragraph{Gamma Function}
The gamma function is defined as $\Gamma(\alpha)=\int_0^\infty y^{\alpha-1}e^{-y}dy$, for $\alpha>0$. This is frequently encountered across different subject areas. It can be shown that \begin{enumerate}
    \item $\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)$ for $\alpha>1$ by integrating by parts.
    \item $\Gamma(\alpha)=(\alpha-1)!$ for $\alpha\geq1$ by induction. 
    \item $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$
\end{enumerate}
\paragraph{Memoryless Property of the Exponential Distribution}
For $X\sim\text{Exponential}(\lambda)$, we have $P(X>a+b|X>a)=P(X>b)$. Note that the geometric distribution is the only discrete distribution with this property, and the exponential distribution is the only continuous distribution with this property.

\paragraph{Erlang distribution}
Let $X$ be a positive random variable with pdf $(n\in\mathbb{N},\lambda>0)$ \\ $f(x)=\left.\begin{cases}\frac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x}, & x>0\\ 0, & \text{otherwise}\end{cases}\right\}$ Then $X$ is said to have Erlang distribution and is denoted as $X\sim \text{Erlang}(n,\lambda)$. Note $E(X)=\frac{n}{\lambda}$, $\text{Var}(X)=\frac{n}{\lambda^2}$


\section*{Lecture 24}
\paragraph{Normal Distribution}
The random variable $X$ is said to have a normal distribution if its pdf is defined by $f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$, for $x\in\mathbb{R}$, where $-\infty<\mu<\infty$, and $\sigma^2>0$, and this is denoted as $X\sim N(\mu,\sigma^2)$. For $X\sim N(\mu,\sigma^2)$, we have $E(X)=\mu$, and $\text{Var}(X)=\sigma^2$. 
\paragraph{Properties of the Normal Distribution}
\begin{enumerate}
    \item Normal distribution is symmetric about its mean. In particular, if $X\sim N(\mu,\sigma^2)$, then we have $P(X\leq \mu-x)=P(X\geq \mu+x)$. 
    \item If $X\sim N(\mu,\sigma^2)$, then $aX+b=N(a\mu+b,a^2\sigma^2)$
\end{enumerate}
\paragraph{Standardization}
As a direct consequence of property 2, we have the following theorem: Let $X\sim N(\mu,\sigma^2)$ and define $Z=\frac{x-\mu}{\sigma}$. Then $Z\sim N(0,1)$ and $P(X\leq x)=P(Z\leq \frac{x-\mu}{\sigma})$. Note $Z\sim N(0,1)$ is called the standard normal distribution. Because of this, any probability calculations related to a general normal distribution can be translated to probability calculations related to a standard normal distribution. 
\paragraph{Standard Normal Table}
Due to the above discussion, we only need the cdf for a standard normal distribution in order to calculate probabilities. Unfortunately, the integral $P(Z\leq x)=\int_{-\infty}^x\frac{1}{\sqrt{2\pi}}e^\frac{-z^2}{2}dz$ does not have a close form, but we can use numerical integration to tablelize standard normal cdf values, and we can use those. 

\section*{Lecture 25}
Examples.  \\ 
To find $X\sim N(\mu,\sigma^2)$ probabilities in general, use an above property that implies $P(a\leq X\leq b)=P\left(\frac{a-\mu}{\sigma}\leq Z\leq\frac{b-\mu}{\sigma}\right)=P\left(Z\leq\frac{b-\mu}{\sigma}\right)-P\left(Z\leq\frac{a-\mu}{\sigma}\right)$

\section*{Lecture 26}
\paragraph{Joint Probability (Mass) Function}
The joint probability (mass) function (joint pmf or joint pf) of the discrete random variables $X$ and $Y$ is defined as $f(x,y)=P(X=x,Y=y)$. 
\paragraph{Conditions for a valid joint pmf}
\begin{enumerate}
    \item $f(x,y)\geq0$ for all $x\in X(S),y\in Y(S)$, and $f(x,y)=0$ otherwise 
    \item $\sum_{x\in X(S)}\sum_{y\in Y(S)}f(x,y)=1$ 
    \item $P((X,Y)\in A)=\sum_{(x,y)\in A}f(x,y)$ where $A\subset X(S)\times Y(S)$
\end{enumerate}

\paragraph{Marginal Probability (Mass) Function}
Let $X$ and $Y$ be discrete random variables with joint pmf $f(x,y)$. The marginal probability (mass) function (marginal pmf/marginal pf) of $X$ and $Y$ are respectively defined as $f_X(x)=P(X=x)=\sum_{y\in Y(S)}f(x,y)$, $\forall x\in X(S)$, and $f_Y(y)=P(Y=y)=\sum_{x\in X(S)}f(x,y)$, $\forall y\in Y(S)$.

\paragraph{Independence of Random Variables}
Two random variables $X$ and $Y$ are said to be independent if and only if their join pmf is equal to the product of their marginal pmfs, ie. $f(x,y)=f_X(x)f_Y(y)$ for all $x,y$. $X$ and $Y$ are independent if and only if:
\begin{itemize}
    \item range of $X$ and $Y$ do NOT depend on each other 
    \item $f(x,y)$ can be factorized as $g(x)h(y)$ 
\end{itemize}
We write $X\independent Y$ in short to say $X$ and $Y$ are independent.


\section*{Lecture 27}
\paragraph{Conditional Probability Functions}
Let $X,Y$ be discrete random variables with joint pmf $f(x,y)$ and marginal pmfs $f_X(x)$ and $f_Y(y)$. For any $x$ such that $f_X(x)>0$, the \textbf{conditional probability function of $Y$ given $X=x$} is the function of $y$ denoted by $f_{Y|X}(y|x)$ and defined by $f_{Y|X}(y|x)=P(Y=y|X=x)=\frac{f(x,y)}{f_X(x)}$. For any $y$ such that $f_Y(y)>0$, the \textbf{conditional probability function of $X$ given $Y=y$} is the function of $x$ denoted by $f_{X|Y}(x|y)$ and defined by $f_{X|Y}(x|y)=P(X=x|Y=y)=\frac{f(x,y)}{f_Y(y)}$. 
\paragraph{Transformation of Random Variables}
Let $X,Y$ be discrete random variables with joint pmf $f(x,y)$. For the random variables $U=g(X,Y)$ such that $U$ is univariate, the probability function of $U$ is $f_U(u)=P(U=u)=\sum_{(x,y):g(x,y)=u}f(x,y)$. \\ 
PROPERTIES: 
\begin{itemize}
    \item For $Z=X+Y$, we have $f_Z(z)=P(X+Y=z)=\sum_{x}f(x,z-x)=\sum_{y}f(z-y,y)$
    \item For $Z=X-Y$, we have $f_Z(z)=P(X-Y=z)=\sum_{x}f(x,x-z)=\sum_{y}f(y+z,y)$
\end{itemize}

\section*{Lecture 28}
\paragraph{Multinomial Distribution}
Note that this is an extension of the binomial distribution. Let $X_1,\ldots,X_k$ be the random variables with $X_i$ denoting the number of outcomes of type $i$ (with probabililty $p_i$) in $n$ independent trials. Then $X_1,\ldots,X_k$ is said to have a multinomial distribution with $f(x_1,\ldots,x_k)=\frac{n!}{x_1!x_2!\ldots x_k!}p_1^{x_1}p_2^{x_2}\ldots p_k^{x_k}$ and is denoted as $(X_1,\ldots,X_k)\sim \text{Multinomial}(n;p_1,\ldots,p_k)$. Note that:
\begin{itemize}
    \item $p_1+p_2+\cdots+p_k=1$ 
    \item $f(x_1,\ldots,x_{k-1})=\frac{n!}{x_1!\ldots x_{k-1}!(n-\sum_{i=1}^{k-1}x_i)!}p_1^{x_1}\ldots p_{k-1}^{x_{k-1}}(1-\sum_{i=1}^{k-1}p_i)$, for $x_i=0,1,\ldots,n;0\leq\sum_{i=1}^{k-1}x_i\leq n$. Then it is clear that the case for $k=2$ reduces to a binomial distribution. 
\end{itemize}

\paragraph{Marginal Distribution}
For $(X_1,\ldots,X_k)\sim \text{Multinomial}(n;p_1,\ldots,p_k)$, the marginal distribution of $X_i\quad(i=1,\ldots,n)$ is given by $X_i\sim\text{Binomial}(n,p_i);i=1,\ldots,n$ 
\paragraph{Marginal Joint Distribution}
For $(X_1,\ldots,X_k)\sim\text{Multinomial}(n;p_1,\ldots,p_k)$, suppose $(A_1,\ldots,A_m)$ $(m\geq2)$ is a partition of the index $\{1,\ldots,k\}$. Let $z_j=\sum_{i\in A_j}X_i$ and $q_j=\sum_{i\in A_j}p_i$ for $j=1,\ldots,m$. Then the joint distribution of $(Z_1,\ldots,Z_m)$ is given by $(Z_1,\ldots,Z_m)\sim \text{Multinomial}(n;q_1,\ldots,q_m)$

\paragraph{Conditional Distribution}
For $(X_1,\ldots,X_k)\sim\text{Multinomial}(n;p_1,\ldots,p_k)$, the conditional distribution of $(X_1,\ldots,X_{k-1})$ conditional on $X_k=x_k$ is given by $(X_1,\ldots,X_{k-1})|X_k=x_k\sim\text{Multinomial}(n-x_k;q_1,\ldots,q_{k-1})$, where $q_i=\frac{p_i}{1-p_k},i=1,\ldots,k-1$. 


\section*{Lecture 29}
\paragraph{Expectation for Multivariate Distributions}
Let $X_1,\ldots,X_n$ be random variables with joint probability function $f(x_1,\ldots,x_n)$. For any function $g$, the \textbf{expected value of $g(X_1,\ldots,X_n)$} is defined by $E(g(X_1,\ldots,X_n))=\sum_{x_1}\ldots\sum_{x_n}g(x_1,\ldots,x_n)f(x_1,\ldots,x_n)$. It has the following properties: 
\begin{itemize}
    \item If $c_1,\ldots,c_n$ are constants, then $E\left(\sum_{i=1}^n c_ig_i(X,Y)\right)=\sum_{i=1}^nc_iE(g_i(X,Y))$ 
    \item If $X\independent Y$, then $E(g_1(X)g_2(Y))=E(g_1(X))E(g_2(Y))$. Note that $E(g_1(X)g_2(Y))=E(g_1(X))E(g_2(Y))$ does not imply $X\independent Y$.  
    \item $E(g(X))=\sum_{x}g(x)f_X(x)$ and $E(g(Y))=\sum_{y}g(y)f_Y(y)$
\end{itemize}
\paragraph{Covariance}
To measure the linear relationship between two random variables, we make use of the following concept: the \textbf{covariance} between two random variables $X,Y$ is defined as $\text{Cov}(X,Y)=E((X-E(X))(Y-E(Y)))$. Note that $\text{Cov}(X,Y)=E(XY)-E(X)E(Y)$, and $\text{Cov}(X,X)=\text{Var}(X)$. Note that the sign of $\text{Cov}(X,Y)$ reveals the direction of the linear relationship between $X$ and $Y$. If $\text{Cov}(X,Y)$ is positive, then $X$ and $Y$ have a tendency moving in the same direction. If it is negative, then they have a tendency moving in the opposite direction. If $X\independent Y$, then $\text{Cov}(X,Y)=0$. Note that if $\text{Cov}(X,Y)=0$, it does not imply that $X\independent Y$. 
\paragraph{Correlation Coefficient}
The \textbf{correlation coefficient} between two random variables $X,Y$ is defined as $\rho=\text{Corr}(X,Y)=\frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}$. The sign and the magnitude of $\text{Corr}(X,Y)$ reveals the direction and the strength of the linear relationship between $X$ and $Y$. It has the following properties: 
\begin{itemize}
    \item $\text{Corr}(X,Y)$ is invariant under linear transformation. In particular, let $X'=aX+b$ and $Y'=cY+d$. Then $\text{Corr}(X',Y')=\text{sign}(ac)\text{Corr}(X,Y)$ 
    \item $-1\leq\text{Corr}(X,Y)\leq1$ 
\end{itemize}

\section*{Lecture 30}
\paragraph{Mean and Variance of Linear Combination of Random Variables}
Mean: $\mathbb{E}\left(\sum_{i=1}^na_iX_i\right)=\sum_{i=1}^na_i\mathbb{E}(X_i)$\\ 
Covariance: $\text{Cov}\left(\sum_{i=1}^na_iX_i,\sum_{j=1}^mb_jY_j\right)=\sum_{i=1}^n\sum_{j=1}^ma_ib_j\text{Cov}(X_i,Y_j)$ \\ 
Variance: $\text{Var}(\sum_{i=1}^na_iX_i)=\sum_{i=1}^na_i^2\text{Var}(X_i)+2\sum_{i<j}a_ia_j\text{Cov}(X_i,X_j)$ \\ 
Let $X_1,\ldots,X_n$ be independent random variables with same mean $\mu$ and same variance $\sigma^2$. Let $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$. Then $E(\bar{X})=\mu$ and $\text{Var}(\bar{X})=\frac{\sigma^2}{n}$. 

\paragraph{Linear Combination of Independent Normal Random Variables}
Let $X_i$ be independent $N(\mu_i,\sigma^2_i)$ random variables. Then $\sum_{i=1}^nX_i\sim N(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2)$. Let $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$. Then $\bar{X}\sim N(\mu,\frac{\sigma^2}{n})$ 

\section*{Lecture 31}
\paragraph{Indicator Random Variables}
An indicator random variable $I$ is a variable that takes 1 if the desired event occurs, and 0 otherwise. $E(I_A^k)=P(A)$, and $\text{Var}(I_A)=P(A)(1-P(A))$. 

\section*{Lecture 32}
\paragraph{Central Limit Theorem}
Let $X_1,\ldots,X_n$ be a sequence of independent and identically distributed random variables with $E(X_i)=\mu$ and $\text{Var}(X_i)=\sigma^2$. Then, the random variable $Z_n=\frac{\frac{1}{n}\sum X_i-\mu}{\frac{\sigma}{\sqrt{n}}}$ converge in distribution to the standard normal distribution, ie. $\lim_{n\rightarrow\infty}\mathbb{P}(Z_n=\frac{\frac{1}{n}\sum X_i-\mu}{\frac{\sigma}{\sqrt{n}}}\leq x)=\mathbb{P}(Z\leq x)$, where $Z$ represents a $N(0,1)$ random variable. We can use this to approximate probabilities. 
\paragraph{Normal Approximation to Binomial Distribution}
Let $X\sim\text{Binomial}(n,p)$. Then, the random variable $\frac{x-np}{\sqrt{np(1-p)}}$ converge in distribution to $N(0,1)$. 

https://www.statisticshowto.datasciencecentral.com/what-is-the-continuity-correction-factor/

\section*{Lecture 33}
\paragraph{Moment Generating Function}
Let $X$ be a random variable. Then $M(t)=\mathbb{E}(e^{tx})$ is called the moment generating function of $X$ if it exists. The domain of $M(t)$ are all real numbers $t$ such that it has finite expected value. 

\paragraph{Generating Moments}
Let $k$ be a positive integer. $\mathbb{E}(X^k)$ is called the $k-$th moment of $X$. Note that the calculation of $E(X)$ and $\text{Var}(X)$ is equivalent to computing the first and second moment. 

\paragraph{Determining Distributions}
If $X$ and $Y$ have the same moment generating function, they must have the same distribution. 


\section*{Lecture 34}
examples




\end{document}